{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 768  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 404 Not Found\"\n",
      "INFO:__main__:Creating collection 'advanced_readme_sections'.\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/advanced_readme_sections/points/scroll \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 330\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muvicorn\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     \u001b[43mbuild_knn_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Initial build of KNN index\u001b[39;00m\n\u001b[0;32m    331\u001b[0m     uvicorn\u001b[38;5;241m.\u001b[39mrun(app, host\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 153\u001b[0m, in \u001b[0;36mbuild_knn_index\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([point\u001b[38;5;241m.\u001b[39mvector \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m all_points[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m    152\u001b[0m knn_model \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, algorithm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m \u001b[43mknn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m point_id_mapping \u001b[38;5;241m=\u001b[39m {i: point\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m i, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_points[\u001b[38;5;241m0\u001b[39m])}\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\neighbors\\_unsupervised.py:175\u001b[0m, in \u001b[0;36mNearestNeighbors.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# NearestNeighbors.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the nearest neighbors estimator from the training dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m        The fitted nearest neighbors estimator.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\neighbors\\_base.py:518\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 518\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_algorithm_metric()\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1029\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m             )\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    try:\n",
    "        vector = get_embedding(section.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for section '{section.heading}': {e}\")\n",
    "        return\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "    logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "\n",
    "knn_model: Optional[NearestNeighbors] = None\n",
    "point_id_mapping: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    logger.info(\"Building KNN index...\")\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    \n",
    "    if not all_points or not all_points[0]:\n",
    "        logger.warning(\"No points found in the collection. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"Embeddings array is empty. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "    logger.info(f\"KNN index built successfully with {len(point_id_mapping)} points.\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata.get('tfidf_similarity', 0.0),\n",
    "            section.metadata.get('semantic_similarity', 0.0),\n",
    "            section.metadata.get('centrality', 0.0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model is None:\n",
    "        logger.warning(\"KNN model is not built. No search can be performed.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "    logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    build_knn_index()  # This will now handle empty collections gracefully\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Using cached xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
