{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 768  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    vector = get_embedding(section.content)\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "\n",
    "knn_model = None\n",
    "point_id_mapping = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata['tfidf_similarity'],\n",
    "            section.metadata['semantic_similarity'],\n",
    "            section.metadata.get('centrality', 0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    build_knn_index()  # Initial build of KNN index\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
