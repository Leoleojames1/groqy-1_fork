{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, SearchRequest\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"readme_sections\"\n",
    "VECTOR_SIZE = 384  # Size of nomic-embed-text embeddings\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class READMEProcessor:\n",
    "    def __init__(self):\n",
    "        self.qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        self._setup_collection()\n",
    "        self.tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        if not self.qdrant_client.get_collection(COLLECTION_NAME):\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return response.json()['embedding']\n",
    "\n",
    "    def parse_readme(self, content: str) -> List[ReadmeSection]:\n",
    "        html = markdown.markdown(content)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        sections = []\n",
    "        section_stack = []\n",
    "        current_section = None\n",
    "\n",
    "        for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "            if elem.name.startswith('h'):\n",
    "                level = int(elem.name[1])\n",
    "                while section_stack and section_stack[-1].level >= level:\n",
    "                    section_stack.pop()\n",
    "\n",
    "                parent = section_stack[-1] if section_stack else None\n",
    "                current_section = ReadmeSection(\n",
    "                    content=elem.text,\n",
    "                    heading=elem.text,\n",
    "                    level=level,\n",
    "                    parent=parent.heading if parent else None,\n",
    "                    children=[],\n",
    "                    metadata={}\n",
    "                )\n",
    "                if parent:\n",
    "                    parent.children.append(current_section.heading)\n",
    "                sections.append(current_section)\n",
    "                section_stack.append(current_section)\n",
    "            else:\n",
    "                if current_section:\n",
    "                    current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "        return sections\n",
    "\n",
    "    def process_readme(self, content: str):\n",
    "        sections = self.parse_readme(content)\n",
    "        section_graph = self._build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            self._add_section_to_qdrant(section, section_graph)\n",
    "\n",
    "    def _build_section_graph(self, sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "        G = nx.DiGraph()\n",
    "        for section in sections:\n",
    "            G.add_node(section.heading, level=section.level)\n",
    "            if section.parent:\n",
    "                G.add_edge(section.parent, section.heading)\n",
    "        return G\n",
    "\n",
    "    def _add_section_to_qdrant(self, section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "        vector = self._get_embedding(section.content)\n",
    "        point_id = str(uuid.uuid4())\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # Calculate centrality and other graph-based features\n",
    "        centrality = nx.degree_centrality(section_graph)[section.heading]\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "\n",
    "        payload = {\n",
    "            \"content\": section.content,\n",
    "            \"heading\": section.heading,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"centrality\": centrality,\n",
    "                \"depth\": depth,\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "\n",
    "    def search_sections(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        query_vector = self._get_embedding(query)\n",
    "\n",
    "        # Perform semantic search\n",
    "        search_result = self.qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=top_k * 2  # Retrieve more results for re-ranking\n",
    "        )\n",
    "\n",
    "        # Extract contents for TF-IDF re-ranking\n",
    "        contents = [hit.payload['content'] for hit in search_result]\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform([query] + contents)\n",
    "        \n",
    "        # Calculate TF-IDF similarities\n",
    "        tfidf_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "        \n",
    "        # Combine semantic and TF-IDF scores\n",
    "        combined_scores = [(hit, 0.7 * hit.score + 0.3 * tfidf_sim) \n",
    "                           for hit, tfidf_sim in zip(search_result, tfidf_similarities)]\n",
    "        \n",
    "        # Sort by combined score and take top_k\n",
    "        combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = combined_scores[:top_k]\n",
    "\n",
    "        results = []\n",
    "        for hit, score in top_results:\n",
    "            section = hit.payload\n",
    "            section['score'] = score\n",
    "            self._update_section_relevance(hit.id, score)\n",
    "            results.append(section)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _update_section_relevance(self, point_id: str, score: float):\n",
    "        current_payload = self.qdrant_client.retrieve(COLLECTION_NAME, [point_id])[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (current_payload['metadata']['relevance_score'] + score) / 2\n",
    "\n",
    "        self.qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "        )\n",
    "\n",
    "    def get_context(self, section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = self.qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section['parent']:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = self.qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = self.get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "            if context[\"parent\"]:\n",
    "                for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                    if sibling_heading != section_heading:\n",
    "                        sibling_context = self.get_context(sibling_heading, 0)\n",
    "                        if sibling_context:\n",
    "                            context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "\n",
    "    def prune_sections(self, threshold: float = 0.5, max_age_days: int = 30):\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points_selector=filter_condition\n",
    "        )\n",
    "\n",
    "# FastAPI app\n",
    "app = FastAPI()\n",
    "readme_processor = READMEProcessor()\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    readme_processor.process_readme(content.decode())\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search(query: str, top_k: int = 5):\n",
    "    results = readme_processor.search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context(section_heading: str, depth: int = 1):\n",
    "    context = readme_processor.get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    readme_processor.prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 384\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: List[float] = None\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = \"http://localhost:11434/api/embeddings\"\n",
    "    response = requests.post(OLLAMA_API_URL, json={\n",
    "        \"model\": \"nomic-embed-text\",\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    response.raise_for_status()\n",
    "    return np.array(response.json()['embedding'])\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text,\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.text\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip(sections, cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    try:\n",
    "        vector = get_embedding(section.content)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for section '{section.heading}': {e}\")\n",
    "        return\n",
    "    \n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, vector=vector.tolist(), payload=payload)]\n",
    "    )\n",
    "    logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "\n",
    "knn_model: Optional[NearestNeighbors] = None\n",
    "point_id_mapping: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index():\n",
    "    global knn_model, point_id_mapping\n",
    "    logger.info(\"Building KNN index...\")\n",
    "    all_points = qdrant_client.scroll(collection_name=COLLECTION_NAME, limit=10000)\n",
    "    \n",
    "    if not all_points or not all_points[0]:\n",
    "        logger.warning(\"No points found in the collection. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    embeddings = np.array([point.vector for point in all_points[0]])\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"Embeddings array is empty. KNN index not built.\")\n",
    "        knn_model = None\n",
    "        point_id_mapping = {}\n",
    "        return\n",
    "    \n",
    "    knn_model = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "    knn_model.fit(embeddings)\n",
    "    point_id_mapping = {i: point.id for i, point in enumerate(all_points[0])}\n",
    "    logger.info(f\"KNN index built successfully with {len(point_id_mapping)} points.\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[ReadmeSection]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section.metadata.get('tfidf_similarity', 0.0),\n",
    "            section.metadata.get('semantic_similarity', 0.0),\n",
    "            section.metadata.get('centrality', 0.0),\n",
    "            section.level,\n",
    "            section.metadata.get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section.metadata.get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model is None:\n",
    "        logger.warning(\"KNN model is not built. No search can be performed.\")\n",
    "        return []\n",
    "    \n",
    "    query_vector = get_embedding(query).reshape(1, -1)\n",
    "    distances, indices = knn_model.kneighbors(query_vector)\n",
    "    nearest_points = [point_id_mapping[idx] for idx in indices[0]]\n",
    "    \n",
    "    sections = []\n",
    "    for point_id in nearest_points:\n",
    "        point = qdrant_client.retrieve(collection_name=COLLECTION_NAME, ids=[point_id])[0]\n",
    "        section = point.payload\n",
    "        section['vector'] = point.vector\n",
    "        tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "        section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "        semantic_sim = 1 / (1 + distances[0][indices[0].tolist().index(point_id_mapping.index(point_id))])\n",
    "        section['metadata']['semantic_similarity'] = semantic_sim\n",
    "        sections.append(section)\n",
    "    \n",
    "    if not sections:\n",
    "        return []\n",
    "    \n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    relevance_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    current_payload = qdrant_client.retrieve(\n",
    "        collection_name=COLLECTION_NAME, ids=[point_id]\n",
    "    )[0].payload\n",
    "    current_payload['metadata']['access_count'] += 1\n",
    "    current_payload['metadata']['relevance_score'] = (\n",
    "        current_payload['metadata']['relevance_score'] + score\n",
    "    ) / 2\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[PointStruct(id=point_id, payload=current_payload)]\n",
    "    )\n",
    "    logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    filter_condition = Filter(\n",
    "        must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "    )\n",
    "    results = qdrant_client.scroll(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        scroll_filter=filter_condition,\n",
    "        limit=1\n",
    "    )\n",
    "    if not results.points:\n",
    "        return {}\n",
    "\n",
    "    section = results.points[0].payload\n",
    "    context = {\n",
    "        \"current\": section,\n",
    "        \"parent\": None,\n",
    "        \"children\": [],\n",
    "        \"siblings\": []\n",
    "    }\n",
    "\n",
    "    if section['parent']:\n",
    "        parent_filter = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "        )\n",
    "        parent_results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=parent_filter,\n",
    "            limit=1\n",
    "        )\n",
    "        if parent_results.points:\n",
    "            context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "    if depth > 0 and 'children' in section:\n",
    "        for child_heading in section['children']:\n",
    "            child_context = get_context(child_heading, depth - 1)\n",
    "            if child_context:\n",
    "                context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "    if context[\"parent\"] and 'children' in context[\"parent\"]:\n",
    "        for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "            if sibling_heading != section_heading:\n",
    "                sibling_context = get_context(sibling_heading, 0)\n",
    "                if sibling_context:\n",
    "                    context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "    return context\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    current_time = time.time()\n",
    "    max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "    filter_condition = Filter(\n",
    "        must=[\n",
    "            FieldCondition(\n",
    "                key=\"metadata.relevance_score\",\n",
    "                range=Range(lt=threshold)\n",
    "            ),\n",
    "            FieldCondition(\n",
    "                key=\"metadata.timestamp\",\n",
    "                range=Range(lt=current_time - max_age_seconds)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    qdrant_client.delete(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points_selector=filter_condition\n",
    "    )\n",
    "    logger.info(\"Pruned low-relevance and old sections.\")\n",
    "\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    sections = parse_readme(content.decode())\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        section.vector = get_embedding(section.content).tolist()\n",
    "    cluster_sections(sections)\n",
    "    for section in sections:\n",
    "        add_section_to_qdrant(section, section_graph)\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"README processed successfully\"}\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    results = search_sections(query, top_k)\n",
    "    return {\"results\": results}\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    context = get_context(section_heading, depth)\n",
    "    return {\"context\": context}\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    prune_sections(threshold, max_age_days)\n",
    "    return {\"message\": \"Pruning completed successfully\"}\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index():\n",
    "    build_knn_index()\n",
    "    return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "#     build_knn_index()  # This will now handle empty collections gracefully\n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-2.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Using cached xgboost-2.1.1-py3-none-win_amd64.whl (124.9 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: numpy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: xgboost in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (1.6.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: sentence-transformers in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (2.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from fastapi) (4.11.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (1.66.2)\n",
      "Requirement already satisfied: httpx>=0.20.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.26.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from grpcio-tools>=1.41.0->qdrant-client) (69.5.1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.3)\n",
      "Requirement already satisfied: h2<5,>=3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached protobuf-5.28.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Rolling back uninstall of protobuf\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\any_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\api_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\descriptor_pool.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\duration_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\empty_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\field_mask_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\json_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\json_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\message_factory.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\message_factory.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\proto_builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\service_reflection.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\source_context_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\struct_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\symbol_database.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_encoding.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\text_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\text_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\timestamp_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\type_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\unknown_fields.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\__pycache__\\wrappers_pb2.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\any_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\any_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\api_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\api_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\compiler\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~ompiler\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\descriptor_pool.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\descriptor_pool.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\duration_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\duration_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\empty_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\empty_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\field_mask_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\field_mask_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__init__.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\__init__.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\_parameterized.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\api_implementation.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\builder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\containers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\decoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\encoder.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\enum_type_wrapper.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\extension_dict.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\field_mask.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\message_listener.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\python_message.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\type_checkers.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\well_known_types.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\__pycache__\\wire_format.cpython-310.pyc\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\_parameterized.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\_parameterized.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\api_implementation.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\api_implementation.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\containers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\decoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\decoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\encoder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\enum_type_wrapper.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\enum_type_wrapper.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\extension_dict.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\extension_dict.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\field_mask.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\field_mask.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\message_listener.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\message_listener.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\numpy\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\internal\\~umpy\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\python_message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\type_checkers.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\well_known_types.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\well_known_types.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\internal\\wire_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\internal\\wire_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\json_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\json_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\message_factory.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\message_factory.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\proto_builder.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\proto_builder.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\pyext\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~yext\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\service_reflection.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\service_reflection.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\source_context_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\source_context_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\struct_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\struct_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\symbol_database.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\symbol_database.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_encoding.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_encoding.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\text_format.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\text_format.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\timestamp_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\timestamp_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\type_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\type_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\unknown_fields.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\unknown_fields.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\util\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\google\\protobuf\\~til\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\google\\protobuf\\wrappers_pb2.py\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-1we76qm2\\wrappers_pb2.py\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "   from C:\\Users\\nasan\\AppData\\Local\\Temp\\pip-uninstall-u_83ebsj\\protobuf-4.25.3-py3.10-nspkg.pth\n",
      "  Moving to d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\protobuf-4.25.3.dist-info\\\n",
      "   from D:\\Users\\nasan\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~rotobuf-4.25.3.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\Users\\\\nasan\\\\anaconda3\\\\envs\\\\myenv\\\\Lib\\\\site-packages\\\\google\\\\_upb\\\\_message.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' already exists.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/Mind \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'Mind' exists.\n",
      "INFO:__main__:Training XGBRanker is not implemented. Using default model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [35356]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; adjust based on SentenceTransformer model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000, offset=scroll_response.next_page_offset)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence_transformers) (0.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nasan\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Installing collected packages: sentence_transformers\n",
      "Successfully installed sentence_transformers-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This could be it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Dependencies\n",
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation in One Code Block\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant clients for both collections\n",
    "qdrant_client_readme = QdrantClient(host=\"localhost\", port=6333)\n",
    "qdrant_client_mind = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants for Readme Sections\n",
    "COLLECTION_NAME_README = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE_README = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Constants for Memory Manager\n",
    "COLLECTION_NAME_MIND = \"Mind\"\n",
    "VECTOR_SIZE_MIND = 384  # Example size; will be updated based on model\n",
    "\n",
    "# Create Readme Sections Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_readme.get_collection(COLLECTION_NAME_README)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_README}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_README}'.\")\n",
    "    qdrant_client_readme.create_collection(\n",
    "        collection_name=COLLECTION_NAME_README,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_README, distance=Distance.EUCLID)\n",
    "    )\n",
    "\n",
    "# Create Mind Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client_mind.get_collection(COLLECTION_NAME_MIND)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME_MIND}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME_MIND}'.\")\n",
    "    # Initialize SentenceTransformer for MemoryManager\n",
    "    memory_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    VECTOR_SIZE_MIND = memory_model.get_sentence_embedding_dimension()\n",
    "    qdrant_client_mind.create_collection(\n",
    "        collection_name=COLLECTION_NAME_MIND,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE_MIND, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class MemoryPacket(BaseModel):\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Define MemoryManager Class\n",
    "class MemoryManager:\n",
    "    def __init__(self, qdrant_client: QdrantClient, collection_name: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.qdrant_client = qdrant_client\n",
    "        self.collection_name = collection_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self._setup_collection()\n",
    "\n",
    "    def _setup_collection(self):\n",
    "        try:\n",
    "            self.qdrant_client.get_collection(self.collection_name)\n",
    "            logger.info(f\"Collection '{self.collection_name}' exists.\")\n",
    "        except Exception:\n",
    "            logger.info(f\"Creating collection '{self.collection_name}'.\")\n",
    "            self.qdrant_client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "            )\n",
    "\n",
    "    async def create_memory(self, content: str, metadata: Dict[str, Any]):\n",
    "        vector = self.model.encode(content).tolist()\n",
    "        memory_packet = MemoryPacket(vector=vector, content=content, metadata=metadata)\n",
    "        point_id = str(uuid.uuid4())\n",
    "\n",
    "        try:\n",
    "            self.qdrant_client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[PointStruct(id=point_id, vector=vector, payload=memory_packet.dict())]\n",
    "            )\n",
    "            logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating memory: {e}\")\n",
    "\n",
    "    async def recall_memory(self, query_content: str, top_k: int = 5):\n",
    "        query_vector = self.model.encode(query_content).tolist()\n",
    "\n",
    "        try:\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            for memory in memories:\n",
    "                self._update_relevance(memory, query_vector)\n",
    "\n",
    "            ranked_memories = sorted(\n",
    "                memories,\n",
    "                key=lambda mem: (\n",
    "                    mem.metadata['semantic_relativity'] * mem.metadata['memetic_similarity'] * mem.metadata['gravitational_pull']\n",
    "                ),\n",
    "                reverse=True\n",
    "            )\n",
    "\n",
    "            return [{\n",
    "                \"content\": memory.content,\n",
    "                \"metadata\": memory.metadata\n",
    "            } for memory in ranked_memories[:top_k]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memory: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _update_relevance(self, memory: MemoryPacket, query_vector: List[float]):\n",
    "        memory.metadata[\"semantic_relativity\"] = self._calculate_cosine_similarity(memory.vector, query_vector)\n",
    "        memory.metadata[\"memetic_similarity\"] = self._calculate_memetic_similarity(memory.metadata)\n",
    "        memory.metadata[\"gravitational_pull\"] = self._calculate_gravitational_pull(memory)\n",
    "        memory.metadata[\"spacetime_coordinate\"] = self._calculate_spacetime_coordinate(memory)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_cosine_similarity(vector_a: List[float], vector_b: List[float]) -> float:\n",
    "        dot_product = sum(a * b for a, b in zip(vector_a, vector_b))\n",
    "        magnitude_a = math.sqrt(sum(a ** 2 for a in vector_a))\n",
    "        magnitude_b = math.sqrt(sum(b ** 2 for b in vector_b))\n",
    "\n",
    "        if magnitude_a == 0 or magnitude_b == 0:\n",
    "            return 0.0\n",
    "\n",
    "        return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_memetic_similarity(metadata: Dict[str, Any]) -> float:\n",
    "        tags = set(metadata.get(\"tags\", []))\n",
    "        reference_tags = set(metadata.get(\"reference_tags\", []))\n",
    "\n",
    "        if not tags or not reference_tags:\n",
    "            return 1.0\n",
    "\n",
    "        intersection = len(tags.intersection(reference_tags))\n",
    "        union = len(tags.union(reference_tags))\n",
    "\n",
    "        return intersection / union if union > 0 else 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_gravitational_pull(memory: MemoryPacket) -> float:\n",
    "        vector_magnitude = math.sqrt(sum(x ** 2 for x in memory.vector))\n",
    "        recall_count = memory.metadata.get(\"recall_count\", 0)\n",
    "        memetic_similarity = memory.metadata.get(\"memetic_similarity\", 1.0)\n",
    "        semantic_relativity = memory.metadata.get(\"semantic_relativity\", 1.0)\n",
    "\n",
    "        return vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_spacetime_coordinate(memory: MemoryPacket) -> float:\n",
    "        time_decay_factor = 1 + (time.time() - memory.metadata.get(\"timestamp\", time.time()))\n",
    "        return memory.metadata[\"gravitational_pull\"] / time_decay_factor\n",
    "\n",
    "    async def prune_memories(self, threshold: float = 1e-5, max_age_days: int = 30):\n",
    "        try:\n",
    "            current_time = time.time()\n",
    "            max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "            filter_condition = Filter(\n",
    "                must=[\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.relevance_score\",\n",
    "                        range=Range(lt=threshold)\n",
    "                    ),\n",
    "                    FieldCondition(\n",
    "                        key=\"metadata.timestamp\",\n",
    "                        range=Range(lt=current_time - max_age_seconds)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.qdrant_client.delete(\n",
    "                collection_name=self.collection_name,\n",
    "                filter=filter_condition\n",
    "            )\n",
    "            logger.info(\"Pruned low-relevance and old memories.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error pruning memories: {e}\")\n",
    "\n",
    "    async def purge_all_memories(self):\n",
    "        try:\n",
    "            self.qdrant_client.delete_collection(self.collection_name)\n",
    "            self._setup_collection()\n",
    "            logger.info(f\"Purged all memories in the collection '{self.collection_name}'.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error purging all memories: {e}\")\n",
    "            raise e\n",
    "\n",
    "    async def recall_memory_with_metadata(self, query_content: str, search_metadata: Dict[str, Any], top_k: int = 10):\n",
    "        try:\n",
    "            query_vector = self.model.encode(query_content).tolist()\n",
    "            results = self.qdrant_client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=query_vector,\n",
    "                limit=top_k\n",
    "            )\n",
    "\n",
    "            memories = [MemoryPacket(**hit.payload) for hit in results]\n",
    "\n",
    "            matching_memories = []\n",
    "            for memory in memories:\n",
    "                memory_metadata = memory.metadata\n",
    "                if all(memory_metadata.get(key) == value for key, value in search_metadata.items()):\n",
    "                    matching_memories.append({\n",
    "                        \"content\": memory.content,\n",
    "                        \"metadata\": memory_metadata\n",
    "                    })\n",
    "\n",
    "            if not matching_memories:\n",
    "                return {\"message\": \"No matching memories found\"}\n",
    "\n",
    "            return {\"memories\": matching_memories}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error recalling memories by metadata: {str(e)}\")\n",
    "            return {\"message\": \"Error during memory recall\"}\n",
    "\n",
    "    async def delete_memories_by_metadata(self, metadata: Dict[str, Any]):\n",
    "        try:\n",
    "            # Scroll through all memories in the collection\n",
    "            scroll_result = self.qdrant_client.scroll(self.collection_name, limit=1000)\n",
    "\n",
    "            memories_to_delete = []\n",
    "            for point in scroll_result:\n",
    "                point_metadata = point.payload.get(\"metadata\", {})\n",
    "                if all(point_metadata.get(key) == value for key, value in metadata.items()):\n",
    "                    memories_to_delete.append(point.id)\n",
    "\n",
    "            if memories_to_delete:\n",
    "                self.qdrant_client.delete(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points_selector={\"points\": memories_to_delete}\n",
    "                )\n",
    "                logger.info(f\"Deleted {len(memories_to_delete)} memories matching the metadata.\")\n",
    "            else:\n",
    "                logger.info(\"No memories found matching the specified metadata.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting memories by metadata: {str(e)}\")\n",
    "\n",
    "# Initialize MemoryManager for Mind Collection\n",
    "memory_manager = MemoryManager(\n",
    "    qdrant_client=qdrant_client_mind,\n",
    "    collection_name=COLLECTION_NAME_MIND,\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def get_embedding(text: str) -> np.ndarray:\n",
    "    OLLAMA_API_URL = os.getenv(\"OLLAMA_API_URL\", \"http://localhost:11434/api/embeddings\")\n",
    "    try:\n",
    "        response = requests.post(OLLAMA_API_URL, json={\n",
    "            \"model\": \"nomic-embed-text\",\n",
    "            \"prompt\": text\n",
    "        })\n",
    "        response.raise_for_status()\n",
    "        return np.array(response.json()['embedding'])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching embedding: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def cluster_sections(sections: List[ReadmeSection], n_clusters: int = 10):\n",
    "    embeddings = np.array([section.vector for section in sections if section.vector is not None])\n",
    "    if embeddings.size == 0:\n",
    "        logger.warning(\"No embeddings available for clustering.\")\n",
    "        return\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    for section, label in zip([s for s in sections if s.vector is not None], cluster_labels):\n",
    "        section.metadata['cluster'] = int(label)\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection, section_graph: nx.DiGraph):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    centrality = nx.degree_centrality(section_graph).get(section.heading, 0)\n",
    "    try:\n",
    "        depth = nx.shortest_path_length(section_graph, source=list(section_graph.nodes)[0], target=section.heading)\n",
    "    except nx.NetworkXNoPath:\n",
    "        depth = 0\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"centrality\": centrality,\n",
    "            \"depth\": depth,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "knn_model_readme: Optional[NearestNeighbors] = None\n",
    "point_id_mapping_readme: Dict[int, str] = {}\n",
    "\n",
    "def build_knn_index_readme():\n",
    "    global knn_model_readme, point_id_mapping_readme\n",
    "    logger.info(\"Building KNN index for Readme Sections...\")\n",
    "    try:\n",
    "        # Scroll retrieves points in batches; adjust batch size as needed\n",
    "        all_points = []\n",
    "        scroll_response = qdrant_client_readme.scroll(collection_name=COLLECTION_NAME_README, limit=10000)\n",
    "        while scroll_response:\n",
    "            all_points.extend(scroll_response.points)\n",
    "            if scroll_response.next_page_offset:\n",
    "                scroll_response = qdrant_client_readme.scroll(\n",
    "                    collection_name=COLLECTION_NAME_README,\n",
    "                    limit=10000,\n",
    "                    offset=scroll_response.next_page_offset\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not all_points:\n",
    "            logger.warning(\"No points found in the Readme collection. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        embeddings = np.array([point.vector for point in all_points])\n",
    "        if embeddings.size == 0:\n",
    "            logger.warning(\"Embeddings array is empty for Readme sections. KNN index not built.\")\n",
    "            knn_model_readme = None\n",
    "            point_id_mapping_readme = {}\n",
    "            return\n",
    "\n",
    "        knn_model_readme = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='euclidean')\n",
    "        knn_model_readme.fit(embeddings)\n",
    "        point_id_mapping_readme = {i: point.id for i, point in enumerate(all_points)}\n",
    "        logger.info(f\"KNN index for Readme sections built successfully with {len(point_id_mapping_readme)} points.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building KNN index for Readme sections: {e}\")\n",
    "        knn_model_readme = None\n",
    "        point_id_mapping_readme = {}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "def calculate_tfidf_similarity(query: str, document: str) -> float:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([query, document])\n",
    "    return (tfidf_matrix * tfidf_matrix.T).A[0, 1]\n",
    "\n",
    "def prepare_training_data(query: str, sections: List[Dict[str, Any]]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for section in sections:\n",
    "        feature_vector = [\n",
    "            section['metadata'].get('tfidf_similarity', 0.0),\n",
    "            section['metadata'].get('semantic_similarity', 0.0),\n",
    "            section['metadata'].get('centrality', 0.0),\n",
    "            section['level'],\n",
    "            section['metadata'].get('cluster', 0)\n",
    "        ]\n",
    "        features.append(feature_vector)\n",
    "        labels.append(section['metadata'].get('relevance_label', 1))  # Placeholder\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    objective='rank:pairwise',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "def train_xgb_ranker():\n",
    "    try:\n",
    "        # Placeholder: Implement actual training logic\n",
    "        # This should be done offline with proper labeled data\n",
    "        # For demonstration, we'll skip training\n",
    "        logger.info(\"Training XGBRanker is not implemented. Using default model.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training XGBRanker: {e}\")\n",
    "\n",
    "# Train the ranker (currently a placeholder)\n",
    "train_xgb_ranker()\n",
    "\n",
    "def search_sections(query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    if knn_model_readme is None:\n",
    "        logger.warning(\"KNN model for Readme sections is not built. No search can be performed.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        query_vector = get_embedding(query).reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get embedding for query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        distances, indices = knn_model_readme.kneighbors(query_vector)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during KNN search: {e}\")\n",
    "        return []\n",
    "\n",
    "    nearest_points = [point_id_mapping_readme[idx] for idx in indices[0]]\n",
    "\n",
    "    sections = []\n",
    "    for idx, point_id in enumerate(nearest_points):\n",
    "        try:\n",
    "            points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "            if not points:\n",
    "                continue\n",
    "            point = points[0]\n",
    "            section = point.payload\n",
    "            section['vector'] = point.vector.tolist()\n",
    "            tfidf_sim = calculate_tfidf_similarity(query, section['content'])\n",
    "            section['metadata']['tfidf_similarity'] = tfidf_sim\n",
    "            # Use the distance directly\n",
    "            semantic_sim = 1 / (1 + distances[0][idx])\n",
    "            section['metadata']['semantic_similarity'] = semantic_sim\n",
    "            sections.append(section)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error retrieving section '{point_id}': {e}\")\n",
    "\n",
    "    if not sections:\n",
    "        return []\n",
    "\n",
    "    X_test, _ = prepare_training_data(query, sections)\n",
    "    if X_test.size == 0:\n",
    "        logger.warning(\"No features available for ranking.\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        relevance_scores = xgb_ranker.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during ranking: {e}\")\n",
    "        relevance_scores = np.ones(len(sections))  # Fallback\n",
    "\n",
    "    for section, score in zip(sections, relevance_scores):\n",
    "        section['score'] = score\n",
    "    sections.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for section in sections[:top_k]:\n",
    "        update_section_relevance(section['id'], section['score'])\n",
    "\n",
    "    return sections[:top_k]\n",
    "\n",
    "def update_section_relevance(point_id: str, score: float):\n",
    "    try:\n",
    "        points = qdrant_client_readme.retrieve(collection_name=COLLECTION_NAME_README, ids=[point_id])\n",
    "        if not points:\n",
    "            logger.warning(f\"Point ID '{point_id}' not found for relevance update.\")\n",
    "            return\n",
    "        current_payload = points[0].payload\n",
    "        current_payload['metadata']['access_count'] += 1\n",
    "        current_payload['metadata']['relevance_score'] = (\n",
    "            current_payload['metadata']['relevance_score'] + score\n",
    "        ) / 2\n",
    "\n",
    "        qdrant_client_readme.upsert(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            points=[PointStruct(id=point_id, vector=points[0].vector.tolist(), payload=current_payload)]\n",
    "        )\n",
    "        logger.info(f\"Updated relevance for point ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating relevance for point ID '{point_id}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match={'value': section_heading})]\n",
    "        )\n",
    "        results = qdrant_client_readme.scroll(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results.points:\n",
    "            return {}\n",
    "\n",
    "        section = results.points[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match={'value': section['parent']})]\n",
    "            )\n",
    "            parent_results = qdrant_client_readme.scroll(\n",
    "                collection_name=COLLECTION_NAME_README,\n",
    "                filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results.points:\n",
    "                context[\"parent\"] = parent_results.points[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "def prune_sections(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = max_age_days * 24 * 60 * 60\n",
    "\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client_readme.delete(\n",
    "            collection_name=COLLECTION_NAME_README,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pruning sections: {e}\")\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/process_readme\")\n",
    "async def process_readme_api(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        sections = parse_readme(content.decode())\n",
    "        section_graph = build_section_graph(sections)\n",
    "        for section in sections:\n",
    "            section.vector = get_embedding(section.content).tolist()\n",
    "        cluster_sections(sections)\n",
    "        for section in sections:\n",
    "            add_section_to_qdrant(section, section_graph)\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"README processed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing README: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to process README.\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "async def search_api(query: str, top_k: int = 5):\n",
    "    try:\n",
    "        results = search_sections(query, top_k)\n",
    "        return {\"results\": results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\")\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        context = get_context(section_heading, depth)\n",
    "        return {\"context\": context}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\")\n",
    "async def prune_api(threshold: float = 0.5, max_age_days: int = 30):\n",
    "    try:\n",
    "        prune_sections(threshold, max_age_days)\n",
    "        return {\"message\": \"Pruning completed successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\")\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        build_knn_index_readme()\n",
    "        return {\"message\": \"KNN index rebuilt successfully\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Example Usage of MemoryManager (Optional)\n",
    "# You can interact with MemoryManager separately if needed\n",
    "\n",
    "# Example: Creating a memory\n",
    "# await memory_manager.create_memory(content=\"Sample memory content.\", metadata={\"tags\": [\"example\", \"test\"], \"reference_tags\": [\"example\"]})\n",
    "\n",
    "# Example: Recalling memories\n",
    "# memories = await memory_manager.recall_memory(query_content=\"Sample query.\")\n",
    "# print(memories)\n",
    "\n",
    "# Example: Pruning memories\n",
    "# await memory_manager.prune_memories()\n",
    "\n",
    "# Example: Purging all memories\n",
    "# await memory_manager.purge_all_memories()\n",
    "\n",
    "# Example: Recalling memories with metadata\n",
    "# memories_with_metadata = await memory_manager.recall_memory_with_metadata(query_content=\"Sample query.\", search_metadata={\"tags\": \"example\"})\n",
    "# print(memories_with_metadata)\n",
    "\n",
    "# Example: Deleting memories by metadata\n",
    "# await memory_manager.delete_memories_by_metadata(metadata={\"tags\": \"test\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Collection 'advanced_readme_sections' already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI server is running on http://0.0.0.0:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [44932]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "# !pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "VECTOR_SIZE = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Initialize SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "VECTOR_SIZE = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Create Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "# Function to send request with metadata\n",
    "import json\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            logger.error(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match=MatchValue(value=section_heading))]\n",
    "        )\n",
    "        results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results:\n",
    "            return {}\n",
    "\n",
    "        section = results[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=section['parent']))]\n",
    "            )\n",
    "            parent_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results:\n",
    "                context[\"parent\"] = parent_results[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/create_memory\")\n",
    "async def create_memory_api(content: str, metadata: Dict[str, Any]):\n",
    "    try:\n",
    "        vector = embedding_model.encode(content).tolist()\n",
    "        point_id = str(uuid.uuid4())\n",
    "        payload = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        return {\"message\": \"Memory created successfully\", \"id\": point_id}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating memory: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create memory.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Function to process README and send sections to database\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    sections = parse_readme(content)\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        # Generate embedding\n",
    "        section.vector = embedding_model.encode(section.content).tolist() if section.content else None\n",
    "\n",
    "        # Prepare title and metadata\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Send to database\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to database. Response: {response}\")\n",
    "\n",
    "        # Optionally, add to Qdrant directly\n",
    "        add_section_to_qdrant(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88176f6912f749389ce6d70c340f6cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID 9b629c23-2901-4794-b8ca-d58bebb5fb3f.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3acbd350dc4ba6b44ded3ff7fdd2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 2efdec15-3fbd-4e8f-858a-aa7cbb39c639.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec115f6f98934720889a948b9c8e0e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID 1dcf1ef9-21df-4927-8e55-621a508722d2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd346be56f348b69d46857f56d0c359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID cb764f5c-c5e6-4d83-89d0-a17dc28991e1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d9d6ff1fee49cd9f3efb915f00abdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID befe5248-b91c-4df2-a05d-1cc71eabef78.\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/advanced_readme_sections/points/scroll \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:Error getting context for section 'Installation': 'list' object has no attribute 'payload'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n",
      "\n",
      "Retrieved Context:\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "# Sample README content (you can replace this with the path to your actual README file)\n",
    "sample_readme_content = \"\"\"\n",
    "# Sample Project\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a sample README file for testing purposes.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Instructions to install...\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "List of prerequisites...\n",
    "\n",
    "### Step-by-Step Guide\n",
    "\n",
    "Step-by-step installation guide...\n",
    "\n",
    "## Usage\n",
    "\n",
    "How to use the project...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save the sample README content to a file\n",
    "readme_path = \"sample_README.md\"\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_readme_content)\n",
    "\n",
    "# Wait for the server to start\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "process_readme_and_send(readme_path)\n",
    "\n",
    "# Retrieve context for a section\n",
    "section_heading = \"Installation\"\n",
    "context = get_context(section_heading, depth=1)\n",
    "\n",
    "# Print the context\n",
    "import json\n",
    "print(\"\\nRetrieved Context:\")\n",
    "print(json.dumps(context, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<Server.serve() done, defined at d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py:63> exception=SystemExit(1)>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 160, in startup\n",
      "    server = await loop.create_server(\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\base_events.py\", line 1519, in create_server\n",
      "    raise OSError(err.errno, 'error while attempting '\n",
      "OSError: [Errno 10048] error while attempting to bind on address ('0.0.0.0', 8000): only one usage of each socket address (protocol/network address/port) is normally permitted\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\nasan\\AppData\\Local\\Temp\\ipykernel_44932\\2900098384.py\", line 262, in run_server\n",
      "    loop.run_until_complete(server.serve())\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\nasan\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\tasks.py\", line 315, in __wakeup\n",
      "    self.__step()\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\asyncio\\tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 78, in serve\n",
      "    await self.startup(sockets=sockets)\n",
      "  File \"d:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\uvicorn\\server.py\", line 170, in startup\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Creating collection 'advanced_readme_sections' with vector size 384.\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections \"HTTP/1.1 409 Conflict\"\n"
     ]
    },
    {
     "ename": "UnexpectedResponse",
     "evalue": "Unexpected Response: 409 (Conflict)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Collection `advanced_readme_sections` already exists!\"},\"time\":0.00003676}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m, in \u001b[0;36minitialize_qdrant_collection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m collection \u001b[38;5;241m=\u001b[39m qdrant_client\u001b[38;5;241m.\u001b[39mget_collection(COLLECTION_NAME)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m VECTOR_SIZE:\n\u001b[0;32m     59\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExisting collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has different vector size. Recreating collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\pydantic\\main.py:811\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CollectionInfo' object has no attribute 'vectors'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Initialize the Qdrant collection\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[43minitialize_qdrant_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Define Data Models\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mReadmeSection\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m, in \u001b[0;36minitialize_qdrant_collection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with vector size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVECTOR_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mqdrant_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVectorParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVECTOR_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOSINE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOLLECTION_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m created successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\qdrant_client.py:2096\u001b[0m, in \u001b[0;36mQdrantClient.create_collection\u001b[1;34m(self, collection_name, vectors_config, sparse_vectors_config, shard_number, sharding_method, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, timeout, **kwargs)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create empty collection with given parameters\u001b[39;00m\n\u001b[0;32m   2048\u001b[0m \n\u001b[0;32m   2049\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;124;03m    Operation result\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[0;32m   2097\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m   2098\u001b[0m     vectors_config\u001b[38;5;241m=\u001b[39mvectors_config,\n\u001b[0;32m   2099\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39mshard_number,\n\u001b[0;32m   2100\u001b[0m     sharding_method\u001b[38;5;241m=\u001b[39msharding_method,\n\u001b[0;32m   2101\u001b[0m     replication_factor\u001b[38;5;241m=\u001b[39mreplication_factor,\n\u001b[0;32m   2102\u001b[0m     write_consistency_factor\u001b[38;5;241m=\u001b[39mwrite_consistency_factor,\n\u001b[0;32m   2103\u001b[0m     on_disk_payload\u001b[38;5;241m=\u001b[39mon_disk_payload,\n\u001b[0;32m   2104\u001b[0m     hnsw_config\u001b[38;5;241m=\u001b[39mhnsw_config,\n\u001b[0;32m   2105\u001b[0m     optimizers_config\u001b[38;5;241m=\u001b[39moptimizers_config,\n\u001b[0;32m   2106\u001b[0m     wal_config\u001b[38;5;241m=\u001b[39mwal_config,\n\u001b[0;32m   2107\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[0;32m   2108\u001b[0m     init_from\u001b[38;5;241m=\u001b[39minit_from,\n\u001b[0;32m   2109\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   2110\u001b[0m     sparse_vectors_config\u001b[38;5;241m=\u001b[39msparse_vectors_config,\n\u001b[0;32m   2111\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2112\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\qdrant_remote.py:2647\u001b[0m, in \u001b[0;36mQdrantRemote.create_collection\u001b[1;34m(self, collection_name, vectors_config, shard_number, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, timeout, sparse_vectors_config, sharding_method, **kwargs)\u001b[0m\n\u001b[0;32m   2630\u001b[0m     init_from \u001b[38;5;241m=\u001b[39m GrpcToRest\u001b[38;5;241m.\u001b[39mconvert_init_from(init_from)\n\u001b[0;32m   2632\u001b[0m create_collection_request \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mCreateCollection(\n\u001b[0;32m   2633\u001b[0m     vectors\u001b[38;5;241m=\u001b[39mvectors_config,\n\u001b[0;32m   2634\u001b[0m     shard_number\u001b[38;5;241m=\u001b[39mshard_number,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2644\u001b[0m     sharding_method\u001b[38;5;241m=\u001b[39msharding_method,\n\u001b[0;32m   2645\u001b[0m )\n\u001b[1;32m-> 2647\u001b[0m result: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_collection_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate collection returned None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:1170\u001b[0m, in \u001b[0;36mSyncCollectionsApi.create_collection\u001b[1;34m(self, collection_name, timeout, create_collection)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1163\u001b[0m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1164\u001b[0m     timeout: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1165\u001b[0m     create_collection: m\u001b[38;5;241m.\u001b[39mCreateCollection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m m\u001b[38;5;241m.\u001b[39mInlineResponse200:\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;124;03m    Create new collection with given parameters\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_for_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api\\collections_api.py:116\u001b[0m, in \u001b[0;36m_CollectionsApi._build_for_create_collection\u001b[1;34m(self, collection_name, timeout, create_collection)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[0;32m    115\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInlineResponse200\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py:79\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, type_, method, url, path_params, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbuild_request(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages\\qdrant_client\\http\\api_client.py:102\u001b[0m, in \u001b[0;36mApiClient.send\u001b[1;34m(self, request, type_)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedResponse\u001b[38;5;241m.\u001b[39mfor_response(response)\n",
      "\u001b[1;31mUnexpectedResponse\u001b[0m: Unexpected Response: 409 (Conflict)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Collection `advanced_readme_sections` already exists!\"},\"time\":0.00003676}'"
     ]
    }
   ],
   "source": [
    "# Install Required Dependencies\n",
    "# You can run this in your terminal or uncomment the following line if using Jupyter.\n",
    "# !pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Define the collection name\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"\n",
    "\n",
    "# Initialize SentenceTransformer for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "VECTOR_SIZE = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Create Collection if it doesn't exist, else verify vector size\n",
    "def initialize_qdrant_collection():\n",
    "    try:\n",
    "        collection = qdrant_client.get_collection(COLLECTION_NAME)\n",
    "        if collection.vectors.size != VECTOR_SIZE:\n",
    "            logger.info(f\"Existing collection '{COLLECTION_NAME}' has different vector size. Recreating collection.\")\n",
    "            qdrant_client.recreate_collection(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "            )\n",
    "            logger.info(f\"Collection '{COLLECTION_NAME}' recreated with vector size {VECTOR_SIZE}.\")\n",
    "        else:\n",
    "            logger.info(f\"Collection '{COLLECTION_NAME}' already exists with correct vector size.\")\n",
    "    except Exception:\n",
    "        logger.info(f\"Creating collection '{COLLECTION_NAME}' with vector size {VECTOR_SIZE}.\")\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "        )\n",
    "        logger.info(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "# Initialize the Qdrant collection\n",
    "initialize_qdrant_collection()\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "class CreateMemoryRequest(BaseModel):\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class CreateMemoryResponse(BaseModel):\n",
    "    message: str\n",
    "    id: Optional[str] = None\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    content: str\n",
    "    metadata: Dict[str, Any]\n",
    "    score: float\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    results: List[SearchResult]\n",
    "\n",
    "class ContextResponse(BaseModel):\n",
    "    context: Dict[str, Any]\n",
    "\n",
    "class PruneRequest(BaseModel):\n",
    "    threshold: float = 0.5\n",
    "    max_age_days: int = 30\n",
    "\n",
    "class PruneResponse(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class RebuildKNNResponse(BaseModel):\n",
    "    message: str\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "\n",
    "@app.post(\"/create_memory\", response_model=CreateMemoryResponse)\n",
    "async def create_memory_api(request: CreateMemoryRequest):\n",
    "    try:\n",
    "        vector = embedding_model.encode(request.content).tolist()\n",
    "        point_id = str(uuid.uuid4())\n",
    "        payload = {\n",
    "            \"content\": request.content,\n",
    "            \"metadata\": request.metadata\n",
    "        }\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        return CreateMemoryResponse(message=\"Memory created successfully\", id=point_id)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating memory: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create memory.\")\n",
    "\n",
    "@app.post(\"/search\", response_model=SearchResponse)\n",
    "async def search_api(request: SearchRequest):\n",
    "    try:\n",
    "        query_vector = embedding_model.encode(request.query).tolist()\n",
    "        search_results = qdrant_client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=query_vector,\n",
    "            limit=request.top_k\n",
    "        )\n",
    "        results = []\n",
    "        for hit in search_results:\n",
    "            # Calculate cosine similarity as semantic similarity\n",
    "            semantic_sim = cosine_similarity([query_vector], [hit.vector])[0][0]\n",
    "            # For demonstration, relevance_score is set as semantic_sim\n",
    "            relevance_score = semantic_sim\n",
    "            results.append(SearchResult(\n",
    "                content=hit.payload[\"content\"],\n",
    "                metadata=hit.payload[\"metadata\"],\n",
    "                score=relevance_score\n",
    "            ))\n",
    "        # Sort results by score descending\n",
    "        results_sorted = sorted(results, key=lambda x: x.score, reverse=True)\n",
    "        return SearchResponse(results=results_sorted)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Search failed.\")\n",
    "\n",
    "@app.get(\"/context/{section_heading}\", response_model=ContextResponse)\n",
    "async def get_context_api(section_heading: str, depth: int = 1):\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match=MatchValue(value=section_heading))]\n",
    "        )\n",
    "        results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results:\n",
    "            raise HTTPException(status_code=404, detail=\"Section not found.\")\n",
    "        \n",
    "        section = results[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "        \n",
    "        # Fetch parent section\n",
    "        parent_heading = section.get(\"parent\")\n",
    "        if parent_heading:\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=parent_heading))]\n",
    "            )\n",
    "            parent_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results:\n",
    "                context[\"parent\"] = parent_results[0].payload\n",
    "        \n",
    "        # Fetch children sections\n",
    "        children_headings = section.get(\"children\", [])\n",
    "        for child_heading in children_headings:\n",
    "            child_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=child_heading))]\n",
    "            )\n",
    "            child_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=child_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if child_results:\n",
    "                context[\"children\"].append(child_results[0].payload)\n",
    "        \n",
    "        # Fetch siblings\n",
    "        if parent_heading:\n",
    "            sibling_headings = context[\"parent\"].get(\"children\", [])\n",
    "            for sibling_heading in sibling_headings:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_filter = Filter(\n",
    "                        must=[FieldCondition(key=\"heading\", match=MatchValue(value=sibling_heading))]\n",
    "                    )\n",
    "                    sibling_results = qdrant_client.scroll(\n",
    "                        collection_name=COLLECTION_NAME,\n",
    "                        scroll_filter=sibling_filter,\n",
    "                        limit=1\n",
    "                    )\n",
    "                    if sibling_results:\n",
    "                        context[\"siblings\"].append(sibling_results[0].payload)\n",
    "        \n",
    "        return ContextResponse(context=context)\n",
    "    except HTTPException as he:\n",
    "        raise he\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving context for section '{section_heading}': {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to retrieve context.\")\n",
    "\n",
    "@app.post(\"/prune\", response_model=PruneResponse)\n",
    "async def prune_api(prune_request: PruneRequest):\n",
    "    try:\n",
    "        current_time = time.time()\n",
    "        max_age_seconds = prune_request.max_age_days * 24 * 60 * 60\n",
    "\n",
    "        # Define filter for low relevance_score and older than max_age_days\n",
    "        filter_condition = Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.relevance_score\",\n",
    "                    range=Range(lt=prune_request.threshold)\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"metadata.timestamp\",\n",
    "                    range=Range(lt=current_time - max_age_seconds)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        qdrant_client.delete(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            filter=filter_condition\n",
    "        )\n",
    "        logger.info(\"Pruned low-relevance and old sections.\")\n",
    "        return PruneResponse(message=\"Pruning completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during pruning: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Pruning failed.\")\n",
    "\n",
    "@app.post(\"/rebuild_knn_index\", response_model=RebuildKNNResponse)\n",
    "async def rebuild_knn_index_api():\n",
    "    try:\n",
    "        # For Qdrant, the index is handled internally, so no action needed\n",
    "        # If you have a separate KNN model, rebuild it here\n",
    "        logger.info(\"KNN index rebuilt successfully (handled internally by Qdrant).\")\n",
    "        return RebuildKNNResponse(message=\"KNN index rebuilt successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error rebuilding KNN index: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to rebuild KNN index.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Function to start the server\n",
    "def start_server():\n",
    "    server_thread = Thread(target=run_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    logger.info(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Initialize and start the server\n",
    "start_server()\n",
    "\n",
    "# Function to process README and send sections to database\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    try:\n",
    "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"README file not found at path: {readme_path}\")\n",
    "        return\n",
    "    \n",
    "    sections = parse_readme(content)\n",
    "    section_graph = build_section_graph(sections)\n",
    "    \n",
    "    for section in sections:\n",
    "        if section.content.strip():\n",
    "            # Generate embedding\n",
    "            section.vector = embedding_model.encode(section.content).tolist()\n",
    "        else:\n",
    "            section.vector = None  # Handle sections with no content\n",
    "    \n",
    "        # Prepare title and metadata\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": int(time.time()),\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0,\n",
    "                \"cluster\": section.metadata.get('cluster', 0),\n",
    "                \"centrality\": 0.0  # Placeholder, can be calculated if needed\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        if not section.vector:\n",
    "            logger.warning(f\"Section '{title}' has no content and will not be sent to the database.\")\n",
    "            continue\n",
    "    \n",
    "        # Send to database via API\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to database. Response: {response}\")\n",
    "    \n",
    "    logger.info(\"Finished processing and sending all sections.\")\n",
    "\n",
    "# Initialize a sample KNN model (if needed)\n",
    "# Currently, Qdrant handles KNN internally\n",
    "\n",
    "# Example: Function to retrieve context (already defined in API)\n",
    "\n",
    "# Note: The server is running in the background as a daemon thread and will terminate when the main program exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.105.0)\n",
      "Requirement already satisfied: uvicorn in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: qdrant-client in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: markdown in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages (4.12.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nnxruntime (d:\\users\\nasan\\anaconda3\\envs\\myenv\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn qdrant-client markdown beautifulsoup4 sklearn xgboost networkx nest_asyncio sentence-transformers numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a9582942e4528b7a6689b8df6ca4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'message': 'Memory created successfully', 'id': '0ba22cf0-adc7-4ffe-a7dd-865eebaac104'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935f7ab94d2a4ca0b79193d1f0478f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eee2fdb9274d32bcf77d96c34e56d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID cac72711-9ccb-4870-be47-c27d3690d443.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'message': 'Memory created successfully', 'id': '75b5749a-ec96-441f-989e-5b4ca7ac245b'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e11e11570345e99e0cf35b2b387a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5138561735541f2b2726b9648644a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 57500e53-e235-4c13-9a10-e6fca60352cc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'message': 'Memory created successfully', 'id': 'c9286f86-cc79-4003-9ee4-701e1ee3ed48'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88690d581004687b10575540f2f96f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8811abac3927470d8ee42d6b4ec16b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID 6ad595c1-1605-4cec-bc7f-aa192857935e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'message': 'Memory created successfully', 'id': '825ba3f0-ccef-43cb-bc87-11fa4007e76f'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5291575c610405da44a289cc65f48c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1a29cad9a24ff6bf833aa5fd3b4b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID 19c69a55-1e84-46de-8399-0ae2bd645657.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'message': 'Memory created successfully', 'id': '767b950e-a53c-4bb5-8f98-22ab06213932'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b843d49898249558af1be5e10d5114a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e05c40f1a54cbea1bee0d578706f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID a7614a63-35ea-4dbd-8149-2e0d8e71347e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'message': 'Memory created successfully', 'id': 'dcbda5b9-28db-4968-9350-1dcf062d566c'}\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Failed to retrieve context.\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5966d8bdd3b64fd4bb357ede943f7017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Search failed.\\\"}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the sample README\n",
    "readme_path = \"README.md\"\n",
    "\n",
    "# Wait briefly to ensure the server is up\n",
    "time.sleep(3)  # Adjust if necessary based on server startup time\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def run_processing():\n",
    "    process_readme_and_send(readme_path)\n",
    "\n",
    "processing_thread = Thread(target=run_processing)\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for the 'Installation' section\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search for \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Function to prune sections (optional)\n",
    "def prune_sections(threshold=0.5, max_age_days=30):\n",
    "    url = \"http://localhost:8000/prune\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"threshold\": threshold,\n",
    "        \"max_age_days\": max_age_days\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, prune sections\n",
    "# prune_response = prune_sections()\n",
    "# print(f\"\\nPrune Response: {prune_response}\")\n",
    "\n",
    "# Function to rebuild KNN index (optional)\n",
    "def rebuild_knn():\n",
    "    url = \"http://localhost:8000/rebuild_knn_index\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, rebuild KNN index\n",
    "# rebuild_response = rebuild_knn()\n",
    "# print(f\"\\nRebuild KNN Index Response: {rebuild_response}\")\n",
    "\n",
    "# Clean up: Remove the sample README file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing README: add_section_to_qdrant() takes 1 positional argument but 2 were given\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Failed to retrieve context.\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4e9e0e9c2e4e69af5c86b5b81dfa7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 500\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Search failed.\\\"}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the actual README (replace with the correct path)\n",
    "readme_path = \"README.md\"  # Make sure this points to your actual README\n",
    "\n",
    "# Wait briefly to ensure the server is up (optional adjustment if necessary)\n",
    "time.sleep(3)\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def process_readme_and_send(readme_path):\n",
    "    try:\n",
    "        with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "            readme_content = f.read()\n",
    "\n",
    "        # Use the actual processing function here to break down the README content\n",
    "        sections = parse_readme(readme_content)  # Assuming parse_readme function processes your README correctly\n",
    "\n",
    "        # Build the section graph (for context of parent-child relationships)\n",
    "        section_graph = build_section_graph(sections)\n",
    "\n",
    "        # Generate embeddings and metadata for each section\n",
    "        for section in sections:\n",
    "            if section.content:\n",
    "                section.vector = get_embedding(section.content).tolist()  # Get vector for the section\n",
    "                add_section_to_qdrant(section, section_graph)  # Add to database\n",
    "                print(f\"Processed and added section '{section.heading}' to the database.\")\n",
    "            else:\n",
    "                print(f\"Skipping section '{section.heading}' due to missing content.\")\n",
    "        \n",
    "        print(\"README processing completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing README: {str(e)}\")\n",
    "\n",
    "# Run the processing in a thread (optional, if necessary to avoid blocking)\n",
    "processing_thread = Thread(target=process_readme_and_send, args=(readme_path,))\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for a specific section, e.g., 'Installation'\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search query, for example \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Note: We have disabled any pruning and unnecessary operations to ensure the README is not altered in any way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "ERROR:__main__:Section 'Sample Project' has no vector.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Sample Project' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6af923a7687428b8fa3b91910852da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Introduction' added to Qdrant with ID 9cff1cd4-a453-4578-a769-ba8eec5647b7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Introduction' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fa6332c1a247c0bfc647363fa161bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Installation' added to Qdrant with ID 680c30b8-5ad4-461b-bfd8-06b5ffa94a19.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Installation' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5ba539c70d4f2f995ac3563058a793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Prerequisites' added to Qdrant with ID b2e8a395-e93d-40b2-89d1-e3d57625948e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Prerequisites' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cdb3bf5fe24444893237a3cb218842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Step-by-Step Guide' added to Qdrant with ID 4d201918-25a7-4b71-9106-c4b0e07c70a6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Step-by-Step Guide' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c88c360b84fcbb20bb90824692eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Request failed with status code 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/advanced_readme_sections/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Section 'Usage' added to Qdrant with ID fb1a01b9-146f-4a2c-a754-6b95fa69b2d7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Usage' to database. Response: {'error': 'Request failed with status code 422', 'details': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"query\",\"content\"],\"msg\":\"Field required\",\"input\":null,\"url\":\"https://errors.pydantic.dev/2.7/v/missing\"}]}'}\n",
      "\n",
      "Retrieved Context for 'Installation':\n",
      "{\n",
      "  \"error\": \"Failed to retrieve context: 404\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Not Found\\\"}\"\n",
      "}\n",
      "\n",
      "Search Results for 'How to install dependencies':\n",
      "{\n",
      "  \"error\": \"Search failed: 404\",\n",
      "  \"details\": \"{\\\"detail\\\":\\\"Not Found\\\"}\"\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test Code\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define the path for the sample README\n",
    "readme_path = \"sample_README.md\"\n",
    "\n",
    "# Wait briefly to ensure the server is up\n",
    "time.sleep(3)  # Adjust if necessary based on server startup time\n",
    "\n",
    "# Function to send POST request to /create_memory\n",
    "def send_create_memory(title, metadata):\n",
    "    url = \"http://localhost:8000/create_memory\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Process the README and send sections to the database\n",
    "from threading import Thread\n",
    "\n",
    "def run_processing():\n",
    "    process_readme_and_send(readme_path)\n",
    "\n",
    "processing_thread = Thread(target=run_processing)\n",
    "processing_thread.start()\n",
    "\n",
    "# Wait for processing to complete\n",
    "processing_thread.join()\n",
    "\n",
    "# Function to retrieve context for a section\n",
    "def get_section_context(section_heading, depth=1):\n",
    "    url = f\"http://localhost:8000/context/{section_heading}\"\n",
    "    params = {\"depth\": depth}\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Failed to retrieve context: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Retrieve context for the 'Installation' section\n",
    "context_response = get_section_context(\"Installation\", depth=1)\n",
    "print(\"\\nRetrieved Context for 'Installation':\")\n",
    "print(json.dumps(context_response, indent=2))\n",
    "\n",
    "# Function to perform a search\n",
    "def perform_search(query, top_k=5):\n",
    "    url = \"http://localhost:8000/search\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": f\"Search failed: {response.status_code}\", \"details\": response.text}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Perform a search for \"How to install dependencies\"\n",
    "search_query = \"How to install dependencies\"\n",
    "search_response = perform_search(search_query, top_k=3)\n",
    "print(f\"\\nSearch Results for '{search_query}':\")\n",
    "print(json.dumps(search_response, indent=2))\n",
    "\n",
    "# Function to prune sections (optional)\n",
    "def prune_sections(threshold=0.5, max_age_days=30):\n",
    "    url = \"http://localhost:8000/prune\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"threshold\": threshold,\n",
    "        \"max_age_days\": max_age_days\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=body)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, prune sections\n",
    "# prune_response = prune_sections()\n",
    "# print(f\"\\nPrune Response: {prune_response}\")\n",
    "\n",
    "# Function to rebuild KNN index (optional)\n",
    "def rebuild_knn():\n",
    "    url = \"http://localhost:8000/rebuild_knn_index\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Optionally, rebuild KNN index\n",
    "# rebuild_response = rebuild_knn()\n",
    "# print(f\"\\nRebuild KNN Index Response: {rebuild_response}\")\n",
    "\n",
    "# Clean up: Remove the sample README file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api endpoints:\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def send_request(content):\n",
    "    url = f\"{baseUrl}/gravrag/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": content,\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "    \n",
    "#send_request(\"Another test\")\n",
    "\n",
    "import requests\n",
    "import json\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/gravrag/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "    \n",
    "\n",
    "#send_request_with_metadata(title,metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn requests numpy qdrant-client markdown beautifulsoup4 scikit-learn xgboost networkx nest_asyncio python-dotenv sentence-transformers\n",
    "\n",
    "# Comprehensive Implementation\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import (\n",
    "    Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, MatchValue\n",
    ")\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File\n",
    "from pydantic import BaseModel\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBRanker\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Qdrant client\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"advanced_readme_sections\"#Change the name to Mind\n",
    "VECTOR_SIZE = 384  # Adjust based on your embedding model\n",
    "\n",
    "# Initialize SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "VECTOR_SIZE = embedding_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# Create Collection if it doesn't exist\n",
    "try:\n",
    "    qdrant_client.get_collection(COLLECTION_NAME)\n",
    "    logger.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "except Exception:\n",
    "    logger.info(f\"Creating collection '{COLLECTION_NAME}'.\")\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Define Data Models\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "    vector: Optional[List[float]] = None\n",
    "\n",
    "# Function to send request with metadata\n",
    "import json\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/create_memory\"\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Define the body\n",
    "    body = {\n",
    "        \"content\": title,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "    # Send the POST request\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            logger.error(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "# Utility Functions for Readme Processing\n",
    "\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            parent = section_stack[-1] if section_stack else None\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=parent.heading if parent else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "            if parent:\n",
    "                parent.children.append(current_section.heading)\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \"\\n\" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def build_section_graph(sections: List[ReadmeSection]) -> nx.DiGraph:\n",
    "    G = nx.DiGraph()\n",
    "    for section in sections:\n",
    "        G.add_node(section.heading, level=section.level)\n",
    "        if section.parent:\n",
    "            G.add_edge(section.parent, section.heading)\n",
    "    return G\n",
    "\n",
    "def add_section_to_qdrant(section: ReadmeSection):\n",
    "    if not section.vector:\n",
    "        logger.error(f\"Section '{section.heading}' has no vector.\")\n",
    "        return\n",
    "\n",
    "    point_id = str(uuid.uuid4())\n",
    "    timestamp = time.time()\n",
    "\n",
    "    payload = {\n",
    "        \"content\": section.content,\n",
    "        \"heading\": section.heading,\n",
    "        \"level\": section.level,\n",
    "        \"parent\": section.parent,\n",
    "        \"children\": section.children,\n",
    "        \"metadata\": {\n",
    "            **section.metadata,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=section.vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Section '{section.heading}' added to Qdrant with ID {point_id}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to upsert section '{section.heading}': {e}\")\n",
    "\n",
    "def get_context(section_heading: str, depth: int = 1) -> Dict[str, Any]:\n",
    "    try:\n",
    "        filter_condition = Filter(\n",
    "            must=[FieldCondition(key=\"heading\", match=MatchValue(value=section_heading))]\n",
    "        )\n",
    "        results = qdrant_client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            scroll_filter=filter_condition,\n",
    "            limit=1\n",
    "        )\n",
    "        if not results:\n",
    "            return {}\n",
    "\n",
    "        section = results[0].payload\n",
    "        context = {\n",
    "            \"current\": section,\n",
    "            \"parent\": None,\n",
    "            \"children\": [],\n",
    "            \"siblings\": []\n",
    "        }\n",
    "\n",
    "        if section.get('parent'):\n",
    "            parent_filter = Filter(\n",
    "                must=[FieldCondition(key=\"heading\", match=MatchValue(value=section['parent']))]\n",
    "            )\n",
    "            parent_results = qdrant_client.scroll(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                scroll_filter=parent_filter,\n",
    "                limit=1\n",
    "            )\n",
    "            if parent_results:\n",
    "                context[\"parent\"] = parent_results[0].payload\n",
    "\n",
    "        if depth > 0 and 'children' in section:\n",
    "            for child_heading in section['children']:\n",
    "                child_context = get_context(child_heading, depth - 1)\n",
    "                if child_context:\n",
    "                    context[\"children\"].append(child_context[\"current\"])\n",
    "\n",
    "        if context.get(\"parent\") and 'children' in context[\"parent\"]:\n",
    "            for sibling_heading in context[\"parent\"][\"children\"]:\n",
    "                if sibling_heading != section_heading:\n",
    "                    sibling_context = get_context(sibling_heading, 0)\n",
    "                    if sibling_context:\n",
    "                        context[\"siblings\"].append(sibling_context[\"current\"])\n",
    "\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting context for section '{section_heading}': {e}\")\n",
    "        return {}\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define Endpoints\n",
    "@app.post(\"/create_memory\")\n",
    "async def create_memory_api(content: str, metadata: Dict[str, Any]):\n",
    "    try:\n",
    "        vector = embedding_model.encode(content).tolist()\n",
    "        point_id = str(uuid.uuid4())\n",
    "        payload = {\n",
    "            \"content\": content,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[PointStruct(id=point_id, vector=vector, payload=payload)]\n",
    "        )\n",
    "        logger.info(f\"Memory created successfully with ID: {point_id}\")\n",
    "        return {\"message\": \"Memory created successfully\", \"id\": point_id}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating memory: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to create memory.\")\n",
    "\n",
    "# Function to run Uvicorn server in a separate thread\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "    server = uvicorn.Server(config)\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"FastAPI server is running on http://0.0.0.0:8000\")\n",
    "\n",
    "# Function to process README and send sections to database\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    sections = parse_readme(content)\n",
    "    section_graph = build_section_graph(sections)\n",
    "    for section in sections:\n",
    "        # Generate embedding\n",
    "        section.vector = embedding_model.encode(section.content).tolist() if section.content else None\n",
    "\n",
    "        # Prepare title and metadata\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"metadata\": {\n",
    "                **section.metadata,\n",
    "                \"timestamp\": time.time(),\n",
    "                \"access_count\": 0,\n",
    "                \"relevance_score\": 1.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Send to database\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to database. Response: {response}\")\n",
    "\n",
    "        # Optionally, add to Qdrant directly\n",
    "        add_section_to_qdrant(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Version:\n",
    "\n",
    "- Embeddings Model removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Table of Contents' to API. Metadata: {'content': '1. Overview 2. Backend Components 2.1. Schemas 2.1.1. User Schema 2.1.2. Project Schema 2.1.3. Task Schema 2.1.4. Submission Schema 2.1.5. Comment Schema 2.1.6. Notification Schema 2.1.7. Email Schema 2.1.8. Leaderboard Schema 2.2. Server Setup 2.2.1. Imports and Dependencies 2.2.2. Express Application Configuration 2.2.3. Middleware Configuration 2.2.4. File Upload Handling 2.2.5. Authentication Middleware 2.2.6. Request Logging 2.3. Utility Functions 2.3.1. createNewTask Function 2.4. Routes 2.4.1. Authentication Routes 2.4.2. Task Management Routes 2.4.3. Submission Management Routes 2.4.5. Comment Management Routes 2.4.6. Admin Routes 2.4.7. Miscellaneous Routes 3. Frontend Components 3.1. HTML Structure 3.2. CSS and Styling 3.3. JavaScript Functionality 4. Security Considerations 5. Deployment and Environment Configuration 6. Conclusion 2.1. Schemas 2.1.1. User Schema 2.1.2. Project Schema 2.1.3. Task Schema 2.1.4. Submission Schema 2.1.5. Comment Schema 2.1.6. Notification Schema 2.1.7. Email Schema 2.1.8. Leaderboard Schema 2.2. Server Setup 2.2.1. Imports and Dependencies 2.2.2. Express Application Configuration 2.2.3. Middleware Configuration 2.2.4. File Upload Handling 2.2.5. Authentication Middleware 2.2.6. Request Logging 2.3. Utility Functions 2.3.1. createNewTask Function 2.4. Routes 2.4.1. Authentication Routes 2.4.2. Task Management Routes 2.4.3. Submission Management Routes 2.4.5. Comment Management Routes 2.4.6. Admin Routes 2.4.7. Miscellaneous Routes 2.1.1. User Schema 2.1.2. Project Schema 2.1.3. Task Schema 2.1.4. Submission Schema 2.1.5. Comment Schema 2.1.6. Notification Schema 2.1.7. Email Schema 2.1.8. Leaderboard Schema 2.2.1. Imports and Dependencies 2.2.2. Express Application Configuration 2.2.3. Middleware Configuration 2.2.4. File Upload Handling 2.2.5. Authentication Middleware 2.2.6. Request Logging 2.3.1. createNewTask Function 2.4.1. Authentication Routes 2.4.2. Task Management Routes 2.4.3. Submission Management Routes 2.4.5. Comment Management Routes 2.4.6. Admin Routes 2.4.7. Miscellaneous Routes 3.1. HTML Structure 3.2. CSS and Styling 3.3. JavaScript Functionality', 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315444.8349047, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Table of Contents' to API. Metadata: {'content': 'Overview Backend Components Schemas User Schema Project Schema Task Schema Submission Schema Comment Schema Notification Schema Email Schema Leaderboard Schema Server Setup Imports and Dependencies Express Application Configuration Middleware Configuration File Upload Handling Authentication Middleware Request Logging Utility Functions createNewTask Function Routes Authentication Routes User Registration User Login Promote User to Admin Get Current User Profile Task Management Routes Create New Task Update Task Get All Tasks for User Delete Task Submission Management Routes Get Submissions by User Project Management Routes Create New Project Get All Projects Get Specific Project by ID Update Project Delete Project Comment Management Routes Add a Comment to a Task Get Comments for a Task Admin Routes Get All Users Update a User Delete a User Admin Overview Admin Task Status Admin User Activity Admin Analytics Routes User Activity Analytics Task Completion Analytics Admin Task Assignment Miscellaneous Routes File Upload Endpoint Processor Endpoint Frontend Routes Frontend Components HTML Structure CSS and Styling JavaScript Functionality Background Effects Navigation Bar Main Content Initial View Pitch Form Pitch Deck Slides Interactive Features Star Generation Pitch Form Handling Pitch Deck Generation Slide Navigation Tooltips Invest Modal Keyboard and Swipe Navigation Security Considerations Authentication and Authorization Data Sanitization File Handling Security Deployment and Environment Configuration Conclusion Schemas User Schema Project Schema Task Schema Submission Schema Comment Schema Notification Schema Email Schema Leaderboard Schema Server Setup Imports and Dependencies Express Application Configuration Middleware Configuration File Upload Handling Authentication Middleware Request Logging Utility Functions createNewTask Function Routes Authentication Routes User Registration User Login Promote User to Admin Get Current User Profile Task Management Routes Create New Task Update Task Get All Tasks for User Delete Task Submission Management Routes Get Submissions by User Project Management Routes Create New Project Get All Projects Get Specific Project by ID Update Project Delete Project Comment Management Routes Add a Comment to a Task Get Comments for a Task Admin Routes Get All Users Update a User Delete a User Admin Overview Admin Task Status Admin User Activity Admin Analytics Routes User Activity Analytics Task Completion Analytics Admin Task Assignment Miscellaneous Routes File Upload Endpoint Processor Endpoint Frontend Routes User Schema Project Schema Task Schema Submission Schema Comment Schema Notification Schema Email Schema Leaderboard Schema Imports and Dependencies Express Application Configuration Middleware Configuration File Upload Handling Authentication Middleware Request Logging createNewTask Function Authentication Routes User Registration User Login Promote User to Admin Get Current User Profile Task Management Routes Create New Task Update Task Get All Tasks for User Delete Task Submission Management Routes Get Submissions by User Project Management Routes Create New Project Get All Projects Get Specific Project by ID Update Project Delete Project Comment Management Routes Add a Comment to a Task Get Comments for a Task Admin Routes Get All Users Update a User Delete a User Admin Overview Admin Task Status Admin User Activity Admin Analytics Routes User Activity Analytics Task Completion Analytics Admin Task Assignment Miscellaneous Routes File Upload Endpoint Processor Endpoint Frontend Routes User Registration User Login Promote User to Admin Get Current User Profile Create New Task Update Task Get All Tasks for User Delete Task Get Submissions by User Create New Project Get All Projects Get Specific Project by ID Update Project Delete Project Add a Comment to a Task Get Comments for a Task Get All Users Update a User Delete a User Admin Overview Admin Task Status Admin User Activity Admin Analytics Routes User Activity Analytics Task Completion Analytics Admin Task Assignment User Activity Analytics Task Completion Analytics File Upload Endpoint Processor Endpoint Frontend Routes HTML Structure CSS and Styling JavaScript Functionality Background Effects Navigation Bar Main Content Initial View Pitch Form Pitch Deck Slides Interactive Features Star Generation Pitch Form Handling Pitch Deck Generation Slide Navigation Tooltips Invest Modal Keyboard and Swipe Navigation Background Effects Navigation Bar Main Content Initial View Pitch Form Pitch Deck Slides Interactive Features Star Generation Pitch Form Handling Pitch Deck Generation Slide Navigation Tooltips Invest Modal Keyboard and Swipe Navigation Initial View Pitch Form Pitch Deck Slides Star Generation Pitch Form Handling Pitch Deck Generation Slide Navigation Tooltips Invest Modal Keyboard and Swipe Navigation Authentication and Authorization Data Sanitization File Handling Security', 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315446.923142, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Overview' to API. Metadata: {'content': 'Your application, named Groqy , is a comprehensive platform designed to manage users, projects, tasks, submissions, comments, notifications, emails, and leaderboards. It integrates a robust backend powered by Node.js , Express.js , and MongoDB with a dynamic frontend using HTML , Tailwind CSS , and JavaScript . The system emphasizes secure user authentication, role-based access control, file handling, and interactive user experiences.', 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315449.0006444, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Backend Components' to API. Metadata: {'content': '', 'level': 2, 'parent': None, 'children': ['Schemas', 'User Schema', 'Project Schema', 'Task Schema', 'Submission Schema', 'Comment Schema', 'Notification Schema', 'Email Schema', 'Leaderboard Schema', 'Server Setup', 'Imports and Dependencies', 'Express Application Configuration', 'Middleware Configuration', 'File Upload Handling', 'Authentication Middleware', 'Request Logging', 'Utility Functions', 'createNewTask Function', 'Routes', 'Authentication Routes', 'User Registration', 'User Login', 'Promote User to Admin', 'Get Current User Profile', 'Task Management Routes', 'Create New Task', 'Update Task', 'Get All Tasks for User', 'Delete Task', 'Submission Management Routes', 'Get Submissions by User', 'Project Management Routes', 'Create New Project', 'Get All Projects', 'Get Specific Project by ID', 'Update Project', 'Delete Project', 'Comment Management Routes', 'Add a Comment to a Task', 'Get Comments for a Task', 'Admin Routes', 'Get All Users', 'Update a User', 'Delete a User', 'Admin Overview', 'Admin Task Status', 'Admin User Activity', 'Admin Analytics Routes', 'User Activity Analytics', 'Task Completion Analytics', 'Admin Task Assignment', 'Miscellaneous Routes', 'File Upload Endpoint', 'Processor Endpoint', 'Frontend Routes'], 'timestamp': 1728315451.0687778, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Schemas' to API. Metadata: {'content': 'The backend utilizes Mongoose to define and interact with MongoDB schemas. Each schema represents a distinct entity within the system, encapsulating relevant data and relationships.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315453.1143498, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'User Schema' to API. Metadata: {'content': \"Purpose : Represents a user within the Groqy system. Fields : username (String): User's display name. Defaults to an empty string. email (String): User's email address. Defaults to an empty string. password_hash (String): Hashed password for secure authentication. Defaults to an empty string. role (String): Defines user role, such as 'user' or 'admin'. Defaults to 'user'. created_at (Date): Timestamp of user creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last profile update. Defaults to the current date and time. bio (String): Short biography or description of the user. Defaults to an empty string. skills ([String]): Array of user skills. Defaults to an empty array. total_points (Number): Total points earned by the user, possibly for gamification. Defaults to 0. last_notification_at (Date): Timestamp of the last notification received. Defaults to null . last_email_at (Date): Timestamp of the last email received. Defaults to null . last_task_viewed (String): ID of the last task viewed by the user. Defaults to null . username (String): User's display name. Defaults to an empty string. email (String): User's email address. Defaults to an empty string. password_hash (String): Hashed password for secure authentication. Defaults to an empty string. role (String): Defines user role, such as 'user' or 'admin'. Defaults to 'user'. created_at (Date): Timestamp of user creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last profile update. Defaults to the current date and time. bio (String): Short biography or description of the user. Defaults to an empty string. skills ([String]): Array of user skills. Defaults to an empty array. total_points (Number): Total points earned by the user, possibly for gamification. Defaults to 0. last_notification_at (Date): Timestamp of the last notification received. Defaults to null . last_email_at (Date): Timestamp of the last email received. Defaults to null . last_task_viewed (String): ID of the last task viewed by the user. Defaults to null .\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315455.197409, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Project Schema' to API. Metadata: {'content': \"Purpose : Represents a project created by a user. Fields : title (String): Project title. Required . description (String): Detailed description of the project. Defaults to an empty string. created_by (String): ID of the user who created the project. Required . status (String): Current status of the project, such as 'planning', 'in_progress', or 'completed'. Defaults to 'planning'. created_at (Date): Timestamp of project creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last project update. Defaults to the current date and time. title (String): Project title. Required . description (String): Detailed description of the project. Defaults to an empty string. created_by (String): ID of the user who created the project. Required . status (String): Current status of the project, such as 'planning', 'in_progress', or 'completed'. Defaults to 'planning'. created_at (Date): Timestamp of project creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last project update. Defaults to the current date and time.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315457.2886286, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Task Schema' to API. Metadata: {'content': \"Purpose : Defines tasks assigned to users, potentially linked to projects. Fields : prompt (String): Instructions or prompt for the task. Defaults to an empty string. user_id (Schema.Types.ObjectId): Reference to the User schema, indicating the assigned user. Defaults to null . completed (Boolean): Indicates if the task is completed. Defaults to false . inProgress (Boolean): Indicates if the task is currently in progress. Defaults to false . code (String): Stores any code related to the task. Defaults to null . created_at (Date): Timestamp of task creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last task update. Defaults to the current date and time. project_id (Schema.Types.ObjectId): Reference to the Project schema, linking the task to a project. Defaults to null . difficulty (String): Difficulty level of the task, such as 'easy', 'medium', or 'hard'. Defaults to an empty string. due_date (Date): Due date for task completion. Defaults to null . required_skills ([String]): Skills required to complete the task. Defaults to an empty array. last_notification_at (Date): Timestamp of the last notification related to the task. Defaults to null . last_email_at (Date): Timestamp of the last email related to the task. Defaults to null . title (String): Task title. Defaults to 'Default Task Title'. description (String): Detailed description of the task. Defaults to 'Default Task Description'. task_url (String): URL linking to more details about the task. Defaults to an empty string. file_upload_required (Boolean): Indicates if file upload is required for the task. Defaults to false . downloadable_file_url (String): URL to any file required for the task. Defaults to null . prompt_type (String): Type of task prompt, such as 'text' or 'code'. Defaults to 'text'. prompt (String): Instructions or prompt for the task. Defaults to an empty string. user_id (Schema.Types.ObjectId): Reference to the User schema, indicating the assigned user. Defaults to null . completed (Boolean): Indicates if the task is completed. Defaults to false . inProgress (Boolean): Indicates if the task is currently in progress. Defaults to false . code (String): Stores any code related to the task. Defaults to null . created_at (Date): Timestamp of task creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last task update. Defaults to the current date and time. project_id (Schema.Types.ObjectId): Reference to the Project schema, linking the task to a project. Defaults to null . difficulty (String): Difficulty level of the task, such as 'easy', 'medium', or 'hard'. Defaults to an empty string. due_date (Date): Due date for task completion. Defaults to null . required_skills ([String]): Skills required to complete the task. Defaults to an empty array. last_notification_at (Date): Timestamp of the last notification related to the task. Defaults to null . last_email_at (Date): Timestamp of the last email related to the task. Defaults to null . title (String): Task title. Defaults to 'Default Task Title'. description (String): Detailed description of the task. Defaults to 'Default Task Description'. task_url (String): URL linking to more details about the task. Defaults to an empty string. file_upload_required (Boolean): Indicates if file upload is required for the task. Defaults to false . downloadable_file_url (String): URL to any file required for the task. Defaults to null . prompt_type (String): Type of task prompt, such as 'text' or 'code'. Defaults to 'text'.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315459.3781939, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Submission Schema' to API. Metadata: {'content': \"Purpose : Tracks submissions made by users in response to tasks. Fields : user_id (String): ID of the user making the submission. Defaults to null . task_id (String): ID of the task the submission pertains to. Defaults to null . code (String): Code submitted for the task. Defaults to null . submitted_at (Date): Timestamp of submission. Defaults to the current date and time. uploaded_file_url (String): URL of any file uploaded as part of the submission. Defaults to null . status (String): Status of the submission, such as 'pending' or 'reviewed'. Defaults to an empty string. feedback (String): Feedback provided on the submission. Defaults to null . submission_type (String): Type of submission, such as 'code' or 'file'. Defaults to 'code'. user_id (String): ID of the user making the submission. Defaults to null . task_id (String): ID of the task the submission pertains to. Defaults to null . code (String): Code submitted for the task. Defaults to null . submitted_at (Date): Timestamp of submission. Defaults to the current date and time. uploaded_file_url (String): URL of any file uploaded as part of the submission. Defaults to null . status (String): Status of the submission, such as 'pending' or 'reviewed'. Defaults to an empty string. feedback (String): Feedback provided on the submission. Defaults to null . submission_type (String): Type of submission, such as 'code' or 'file'. Defaults to 'code'.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315461.438485, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Comment Schema' to API. Metadata: {'content': 'Purpose : Allows users to leave comments on tasks. Fields : user_id (String): ID of the user leaving the comment. Defaults to null . task_id (String): ID of the task being commented on. Defaults to null . content (String): Text content of the comment. Defaults to an empty string. created_at (Date): Timestamp of comment creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last comment update. Defaults to the current date and time. user_id (String): ID of the user leaving the comment. Defaults to null . task_id (String): ID of the task being commented on. Defaults to null . content (String): Text content of the comment. Defaults to an empty string. created_at (Date): Timestamp of comment creation. Defaults to the current date and time. updated_at (Date): Timestamp of the last comment update. Defaults to the current date and time.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315463.485039, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Notification Schema' to API. Metadata: {'content': \"Purpose : Stores notifications for users. Fields : user_id (String): ID of the user receiving the notification. Defaults to null . type (String): Type of notification, such as 'task_assigned' or 'submission_feedback'. Defaults to an empty string. content (String): Content/message of the notification. Defaults to an empty string. related_id (String): ID of the related entity (task, project, submission, etc.). Defaults to null . read (Boolean): Indicates if the notification has been read. Defaults to false . created_at (Date): Timestamp of notification creation. Defaults to the current date and time. read_at (Date): Timestamp of when the notification was read. Defaults to null . user_id (String): ID of the user receiving the notification. Defaults to null . type (String): Type of notification, such as 'task_assigned' or 'submission_feedback'. Defaults to an empty string. content (String): Content/message of the notification. Defaults to an empty string. related_id (String): ID of the related entity (task, project, submission, etc.). Defaults to null . read (Boolean): Indicates if the notification has been read. Defaults to false . created_at (Date): Timestamp of notification creation. Defaults to the current date and time. read_at (Date): Timestamp of when the notification was read. Defaults to null .\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315465.568472, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Email Schema' to API. Metadata: {'content': \"Purpose : Logs emails sent to users. Fields : user_id (String): ID of the user receiving the email. Defaults to null . subject (String): Subject line of the email. Defaults to an empty string. content (String): Body content of the email. Defaults to an empty string. related_id (String): ID of the related entity (task, project, etc.). Defaults to null . sent_at (Date): Timestamp of when the email was sent. Defaults to the current date and time. status (String): Status of the email, such as 'sent' or 'failed'. Defaults to 'sent'. user_id (String): ID of the user receiving the email. Defaults to null . subject (String): Subject line of the email. Defaults to an empty string. content (String): Body content of the email. Defaults to an empty string. related_id (String): ID of the related entity (task, project, etc.). Defaults to null . sent_at (Date): Timestamp of when the email was sent. Defaults to the current date and time. status (String): Status of the email, such as 'sent' or 'failed'. Defaults to 'sent'.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315467.630378, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Leaderboard Schema' to API. Metadata: {'content': 'Purpose : Tracks user performance metrics for gamification purposes. Fields : user_id (String): ID of the user being tracked. Defaults to null . points (Number): Total points earned by the user. Defaults to 0. tasks_completed (Number): Number of tasks the user has completed. Defaults to 0. last_updated (Date): Timestamp of the last leaderboard update. Defaults to the current date and time. user_id (String): ID of the user being tracked. Defaults to null . points (Number): Total points earned by the user. Defaults to 0. tasks_completed (Number): Number of tasks the user has completed. Defaults to 0. last_updated (Date): Timestamp of the last leaderboard update. Defaults to the current date and time.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315469.7004287, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Server Setup' to API. Metadata: {'content': 'The server is built using Express.js , a minimal and flexible Node.js web application framework. It leverages Mongoose for MongoDB interactions, Multer for handling file uploads, CORS for cross-origin resource sharing, and JWT for authentication.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315471.760279, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Imports and Dependencies' to API. Metadata: {'content': 'Core Modules : express : Web framework for building the server. path : Utility for handling file and directory paths. fs : File system module for reading and writing files. url : For parsing URL strings. Third-Party Modules : mongoose : ODM (Object Data Modeling) library for MongoDB. cors : Middleware to enable CORS. bcrypt : Library for hashing passwords. jsonwebtoken ( jwt ): For generating and verifying JWT tokens. multer : Middleware for handling multipart/form-data , primarily used for file uploads. sanitize-html : Library to sanitize user input and prevent XSS attacks. Local Modules : ./models.js : Contains all Mongoose models (User, Project, Task, Submission, Comment, Notification, Email, Leaderboard). express : Web framework for building the server. path : Utility for handling file and directory paths. fs : File system module for reading and writing files. url : For parsing URL strings. mongoose : ODM (Object Data Modeling) library for MongoDB. cors : Middleware to enable CORS. bcrypt : Library for hashing passwords. jsonwebtoken ( jwt ): For generating and verifying JWT tokens. multer : Middleware for handling multipart/form-data , primarily used for file uploads. sanitize-html : Library to sanitize user input and prevent XSS attacks. ./models.js : Contains all Mongoose models (User, Project, Task, Submission, Comment, Notification, Email, Leaderboard).', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315473.8167603, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Express Application Configuration' to API. Metadata: {'content': \"Initialization : The Express app is instantiated using express() . The server listens on a port defined by environment variables or defaults to 4000 (or 4001 in test environments). Environment Variables : SECRET_KEY : Used for signing JWT tokens. Defaults to 'banana' if not provided. SERVER_URL : Base URL for constructing file URLs. Defaults to 'https://groqy.com' if not provided. The Express app is instantiated using express() . The server listens on a port defined by environment variables or defaults to 4000 (or 4001 in test environments). SECRET_KEY : Used for signing JWT tokens. Defaults to 'banana' if not provided. SERVER_URL : Base URL for constructing file URLs. Defaults to 'https://groqy.com' if not provided.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315475.8839948, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Middleware Configuration' to API. Metadata: {'content': \"CORS (Cross-Origin Resource Sharing) : Configured to allow specific origins: 'https://groqy.com' and 'https://pitchdeck.ai' . Allowed HTTP methods: GET , POST , PUT , DELETE . Allowed headers: Content-Type , Authorization . Credentials are allowed to support cookies. Body Parsing : express.json() and express.urlencoded() are used to parse incoming request bodies. Both are configured with a size limit of 50mb to handle large payloads, such as file uploads. Request Logging : A custom middleware logs each incoming request with its timestamp, HTTP method, URL, and body content. Secure File Serving : secureFileServe middleware ensures that only authenticated users can access files within the 'uploads' directory. It checks for user authentication, verifies file existence, and serves the file securely. Static File Serving : Static files are served from the 'public' directory. The 'uploads' directory is also served statically but protected by the secureFileServe middleware. View Engine : The server uses ejs as its templating engine. Views are stored in the 'views' directory. Configured to allow specific origins: 'https://groqy.com' and 'https://pitchdeck.ai' . Allowed HTTP methods: GET , POST , PUT , DELETE . Allowed headers: Content-Type , Authorization . Credentials are allowed to support cookies. express.json() and express.urlencoded() are used to parse incoming request bodies. Both are configured with a size limit of 50mb to handle large payloads, such as file uploads. A custom middleware logs each incoming request with its timestamp, HTTP method, URL, and body content. secureFileServe middleware ensures that only authenticated users can access files within the 'uploads' directory. It checks for user authentication, verifies file existence, and serves the file securely. Static files are served from the 'public' directory. The 'uploads' directory is also served statically but protected by the secureFileServe middleware. The server uses ejs as its templating engine. Views are stored in the 'views' directory.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315477.9700036, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'File Upload Handling' to API. Metadata: {'content': \"Multer Configuration : Storage : Configured to store files in the 'uploads' directory with unique filenames to prevent conflicts. Filename Structure : Combines the original field name, a unique suffix (timestamp and random number), and the original file extension. File Size Limit : Set to 50MB to accommodate large files. Upload Middleware : upload is an instance of Multer configured with the defined storage and file size limits. Used in routes that handle file uploads, ensuring files are processed and stored correctly. Storage : Configured to store files in the 'uploads' directory with unique filenames to prevent conflicts. Filename Structure : Combines the original field name, a unique suffix (timestamp and random number), and the original file extension. File Size Limit : Set to 50MB to accommodate large files. upload is an instance of Multer configured with the defined storage and file size limits. Used in routes that handle file uploads, ensuring files are processed and stored correctly.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315480.0440333, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Authentication Middleware' to API. Metadata: {'content': 'authenticateToken Function : Purpose : Protects routes by verifying JWT tokens. Process : Extracts the Authorization header from the request. Validates the format ( Bearer TOKEN ). Verifies the token using the SECRET_KEY . Attaches the decoded user information to the req.user object. Handles errors such as missing headers, invalid tokens, and expired tokens by responding with appropriate HTTP status codes and messages. Purpose : Protects routes by verifying JWT tokens. Process : Extracts the Authorization header from the request. Validates the format ( Bearer TOKEN ). Verifies the token using the SECRET_KEY . Attaches the decoded user information to the req.user object. Handles errors such as missing headers, invalid tokens, and expired tokens by responding with appropriate HTTP status codes and messages. Extracts the Authorization header from the request. Validates the format ( Bearer TOKEN ). Verifies the token using the SECRET_KEY . Attaches the decoded user information to the req.user object. Handles errors such as missing headers, invalid tokens, and expired tokens by responding with appropriate HTTP status codes and messages.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315482.1293304, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Request Logging' to API. Metadata: {'content': \"Middleware Function : Logs each incoming request's timestamp, HTTP method, URL, and body content. Aids in monitoring, debugging, and auditing by providing a trail of user interactions and server responses. Logs each incoming request's timestamp, HTTP method, URL, and body content. Aids in monitoring, debugging, and auditing by providing a trail of user interactions and server responses.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315484.220837, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Utility Functions' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315486.308705, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'createNewTask Function' to API. Metadata: {'content': \"Purpose : Simplifies the creation of new tasks by encapsulating default values and ensuring consistency. Inputs : An object containing task-related fields such as prompt , user_id , project_id , difficulty , due_date , required_skills , title , description , file_upload_required , prompt_type , downloadable_file_url , task_url , completed , inProgress , code , last_notification_at , and last_email_at . Some fields have default values if not provided. Outputs : Returns a new instance of the Task model, ready to be saved to the database. Logic : Accepts input parameters, applies defaults where necessary, and constructs a new Task object using Mongoose's Task model. An object containing task-related fields such as prompt , user_id , project_id , difficulty , due_date , required_skills , title , description , file_upload_required , prompt_type , downloadable_file_url , task_url , completed , inProgress , code , last_notification_at , and last_email_at . Some fields have default values if not provided. Returns a new instance of the Task model, ready to be saved to the database. Accepts input parameters, applies defaults where necessary, and constructs a new Task object using Mongoose's Task model.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315488.3989537, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Routes' to API. Metadata: {'content': 'The application defines a variety of routes to handle different functionalities, including user authentication, task management, project management, admin operations, file uploads, and frontend rendering.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315490.487225, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Authentication Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315492.5530238, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'User Registration' to API. Metadata: {'content': \"Endpoint : POST /api/users/register Purpose : Allows new users to create an account. Process : Extracts username , email , password , role , bio , and skills from the request body. Checks if a user with the provided email already exists. If so, responds with a 400 Bad Request . Hashes the password using bcrypt with a salt round of 10 . Creates a new User instance with the provided and default values. Saves the new user to the database. Creates a welcome task for the new user using createNewTask with predefined title, description, and prompt. Saves the welcome task to the database. Generates a JWT token containing the user's ID, email, and role, valid for 1 hour . Responds with the JWT token, sanitized user object (excluding password_hash ), and the welcome task. Extracts username , email , password , role , bio , and skills from the request body. Checks if a user with the provided email already exists. If so, responds with a 400 Bad Request . Hashes the password using bcrypt with a salt round of 10 . Creates a new User instance with the provided and default values. Saves the new user to the database. Creates a welcome task for the new user using createNewTask with predefined title, description, and prompt. Saves the welcome task to the database. Generates a JWT token containing the user's ID, email, and role, valid for 1 hour . Responds with the JWT token, sanitized user object (excluding password_hash ), and the welcome task.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315494.6402435, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'User Login' to API. Metadata: {'content': \"Endpoint : POST /api/users/login Purpose : Authenticates existing users and provides access tokens. Process : Extracts email and password from the request body. Searches for a user with the provided email. If the user is not found, responds with a 400 Bad Request . Compares the provided password with the stored password_hash using bcrypt . If the password is invalid, responds with a 400 Bad Request . Generates a JWT token containing the user's ID, email, and role, valid for 1 hour . Responds with the JWT token and sanitized user object (excluding password_hash ). Extracts email and password from the request body. Searches for a user with the provided email. If the user is not found, responds with a 400 Bad Request . Compares the provided password with the stored password_hash using bcrypt . If the password is invalid, responds with a 400 Bad Request . Generates a JWT token containing the user's ID, email, and role, valid for 1 hour . Responds with the JWT token and sanitized user object (excluding password_hash ).\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315496.7193856, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Promote User to Admin' to API. Metadata: {'content': \"Endpoint : POST /api/admin/promote Purpose : Allows an admin to elevate a user's role to 'admin'. Access Control : Restricted to users with the 'admin' role. Process : Checks if the authenticated user ( req.user ) has the 'admin' role. If not, responds with a 403 Forbidden . Extracts the email from the request body. Searches for the user with the provided email. If the user is not found, responds with a 404 Not Found . Updates the user's role to 'admin'. Saves the updated user to the database. Responds with a success message indicating the user has been promoted. Checks if the authenticated user ( req.user ) has the 'admin' role. If not, responds with a 403 Forbidden . Extracts the email from the request body. Searches for the user with the provided email. If the user is not found, responds with a 404 Not Found . Updates the user's role to 'admin'. Saves the updated user to the database. Responds with a success message indicating the user has been promoted.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315498.8016636, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get Current User Profile' to API. Metadata: {'content': 'Endpoint : GET /api/users/me Purpose : Retrieves the profile information of the currently authenticated user. Access Control : Requires authentication. Process : Extracts the user ID from req.user (populated by authenticateToken middleware). Searches for the user by ID. If the user is not found, responds with a 404 Not Found . Responds with the sanitized user object (excluding password_hash ). Extracts the user ID from req.user (populated by authenticateToken middleware). Searches for the user by ID. If the user is not found, responds with a 404 Not Found . Responds with the sanitized user object (excluding password_hash ).', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315500.8803716, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Task Management Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315502.9454622, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Create New Task' to API. Metadata: {'content': 'Endpoint : POST /api/tasks Purpose : Allows authenticated users to create new tasks. Access Control : Requires authentication. Process : Extracts task-related fields such as prompt , project_id , difficulty , due_date , required_skills , and file_upload_required from the request body. Calls createNewTask with the provided fields and additional defaults like user_id from req.user.id , title , description , and prompt_type . Saves the new task to the database. Responds with the newly created task object. Extracts task-related fields such as prompt , project_id , difficulty , due_date , required_skills , and file_upload_required from the request body. Calls createNewTask with the provided fields and additional defaults like user_id from req.user.id , title , description , and prompt_type . Saves the new task to the database. Responds with the newly created task object.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315505.0052183, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Update Task' to API. Metadata: {'content': \"Endpoint : PUT /api/tasks/:id Purpose : Allows users to update their tasks. Access Control : Requires authentication. Users can update their own tasks; admins can update any task. Process : Extracts completed , inProgress , and code from the request body. Searches for the task by ID ( req.params.id ). Checks if the task exists and if the requesting user is either the task owner or an admin. If not, responds with 403 Forbidden . Updates the task's completed and inProgress fields if provided. Updates the updated_at timestamp. If code is provided and the task's prompt_type is 'code', updates the code field and creates a new Submission with the provided code. Saves the updated task to the database. Responds with the updated task object and a success message. Extracts completed , inProgress , and code from the request body. Searches for the task by ID ( req.params.id ). Checks if the task exists and if the requesting user is either the task owner or an admin. If not, responds with 403 Forbidden . Updates the task's completed and inProgress fields if provided. Updates the updated_at timestamp. If code is provided and the task's prompt_type is 'code', updates the code field and creates a new Submission with the provided code. Saves the updated task to the database. Responds with the updated task object and a success message.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315507.0937207, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get All Tasks for User' to API. Metadata: {'content': \"Endpoint : GET /api/tasks Purpose : Retrieves all tasks assigned to the authenticated user. Admins can retrieve all tasks. Access Control : Requires authentication. Process : Checks if the authenticated user has the 'admin' role. If the user is an admin, retrieves all tasks, populating the user_id field with the username . If the user is not an admin, retrieves tasks where user_id matches req.user.id . Responds with an array of task objects. Checks if the authenticated user has the 'admin' role. If the user is an admin, retrieves all tasks, populating the user_id field with the username . If the user is not an admin, retrieves tasks where user_id matches req.user.id . Responds with an array of task objects.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315509.1665447, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Delete Task' to API. Metadata: {'content': 'Endpoint : DELETE /api/tasks/:id Purpose : Allows users to delete their tasks. Access Control : Requires authentication. Users can delete their own tasks. Process : Searches for and deletes the task by ID ( req.params.id ) where user_id matches req.user.id . If the task is not found, responds with 404 Not Found . Responds with a success message indicating the task has been deleted. Searches for and deletes the task by ID ( req.params.id ) where user_id matches req.user.id . If the task is not found, responds with 404 Not Found . Responds with a success message indicating the task has been deleted.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315511.2575123, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Submission Management Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315513.3276365, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get Submissions by User' to API. Metadata: {'content': 'Endpoint : GET /api/submissions Purpose : Retrieves all submissions made by the authenticated user. Access Control : Requires authentication. Process : Searches for submissions where user_id matches req.user.id . Responds with an array of submission objects. Searches for submissions where user_id matches req.user.id . Responds with an array of submission objects.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315515.3982162, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Project Management Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315517.4784272, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Create New Project' to API. Metadata: {'content': 'Endpoint : POST /api/projects Purpose : Allows authenticated users to create new projects. Access Control : Requires authentication. Process : Extracts title , description , and status from the request body. Creates a new Project instance with the provided fields and created_by set to req.user.id . Saves the new project to the database. Responds with the newly created project object. Extracts title , description , and status from the request body. Creates a new Project instance with the provided fields and created_by set to req.user.id . Saves the new project to the database. Responds with the newly created project object.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315519.5649014, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Get All Projects' to API. Metadata: {'content': 'Endpoint : GET /api/projects Purpose : Retrieves all projects. Access Control : Requires authentication. Process : Retrieves all projects from the database. Responds with an array of project objects. Retrieves all projects from the database. Responds with an array of project objects.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315521.6223369, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get Specific Project by ID' to API. Metadata: {'content': 'Endpoint : GET /api/projects/:id Purpose : Retrieves a specific project by its ID. Access Control : Requires authentication. Process : Searches for the project by ID ( req.params.id ). If the project is not found, responds with 404 Not Found . Responds with the project object. Searches for the project by ID ( req.params.id ). If the project is not found, responds with 404 Not Found . Responds with the project object.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315523.7189567, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Update Project' to API. Metadata: {'content': \"Endpoint : PUT /api/projects/:id Purpose : Allows users to update their projects. Admins can update any project. Access Control : Requires authentication. Users can update projects they created; admins can update any project. Process : Extracts title , description , and status from the request body. Searches for the project by ID ( req.params.id ). Checks if the project exists and if the requesting user is either the project creator or an admin. If not, responds with 403 Forbidden . Updates the project's title , description , and status fields if provided. Updates the updated_at timestamp. Saves the updated project to the database. Responds with the updated project object and a success message. Extracts title , description , and status from the request body. Searches for the project by ID ( req.params.id ). Checks if the project exists and if the requesting user is either the project creator or an admin. If not, responds with 403 Forbidden . Updates the project's title , description , and status fields if provided. Updates the updated_at timestamp. Saves the updated project to the database. Responds with the updated project object and a success message.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315525.8302243, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Delete Project' to API. Metadata: {'content': 'Endpoint : DELETE /api/projects/:id Purpose : Allows users to delete their projects. Access Control : Requires authentication. Users can delete projects they created. Process : Searches for and deletes the project by ID ( req.params.id ) where created_by matches req.user.id . If the project is not found, responds with 404 Not Found . Responds with a success message indicating the project has been deleted. Searches for and deletes the project by ID ( req.params.id ) where created_by matches req.user.id . If the project is not found, responds with 404 Not Found . Responds with a success message indicating the project has been deleted.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315527.9024458, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Comment Management Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315529.9782405, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Add a Comment to a Task' to API. Metadata: {'content': 'Endpoint : POST /api/comments Purpose : Allows authenticated users to add comments to tasks. Access Control : Requires authentication. Process : Extracts task_id and content from the request body. Searches for the task by ID ( task_id ). If the task is not found, responds with 404 Not Found . Creates a new Comment instance with user_id set to req.user.id , task_id , and content . Saves the new comment to the database. Responds with the newly created comment object. Extracts task_id and content from the request body. Searches for the task by ID ( task_id ). If the task is not found, responds with 404 Not Found . Creates a new Comment instance with user_id set to req.user.id , task_id , and content . Saves the new comment to the database. Responds with the newly created comment object.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315532.049649, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get Comments for a Task' to API. Metadata: {'content': 'Endpoint : GET /api/comments/:task_id Purpose : Retrieves all comments associated with a specific task. Access Control : Requires authentication. Process : Extracts task_id from the route parameter. Searches for comments where task_id matches the provided ID. Responds with an array of comment objects. Extracts task_id from the route parameter. Searches for comments where task_id matches the provided ID. Responds with an array of comment objects.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315534.1454256, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Admin Routes' to API. Metadata: {'content': \"Admin routes are specialized endpoints that provide administrative functionalities such as managing users, overseeing projects and tasks, and accessing analytics. These routes are protected and accessible only to users with the 'admin' role.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315536.2338037, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Get All Users' to API. Metadata: {'content': \"Endpoint : GET /api/admin/users Purpose : Retrieves a list of all users along with a summary of their tasks. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates task statistics: Total number of tasks. Number of completed tasks. Number of tasks in progress. Constructs an array of user summaries containing user details and task statistics. Responds with the array of user summaries. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates task statistics: Total number of tasks. Number of completed tasks. Number of tasks in progress. Constructs an array of user summaries containing user details and task statistics. Responds with the array of user summaries. Total number of tasks. Number of completed tasks. Number of tasks in progress.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315538.322406, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Update a User' to API. Metadata: {'content': \"Endpoint : PUT /api/users/:id Purpose : Allows admins to update user information. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Extracts username , email , role , bio , and skills from the request body. Searches for the user by ID ( req.params.id ). If the user is not found, responds with 404 Not Found . Updates the user's username , email , role , bio , and skills fields if provided. Updates the updated_at timestamp. Saves the updated user to the database. Responds with the sanitized user object (excluding password_hash ). Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Extracts username , email , role , bio , and skills from the request body. Searches for the user by ID ( req.params.id ). If the user is not found, responds with 404 Not Found . Updates the user's username , email , role , bio , and skills fields if provided. Updates the updated_at timestamp. Saves the updated user to the database. Responds with the sanitized user object (excluding password_hash ).\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315540.406721, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Delete a User' to API. Metadata: {'content': \"Endpoint : DELETE /api/users/:id Purpose : Allows admins to delete a user and all associated data. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Searches for and deletes the user by ID ( req.params.id ). If the user is not found, responds with 404 Not Found . Deletes all tasks and submissions associated with the user. Responds with a success message indicating the user and associated data have been deleted. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Searches for and deletes the user by ID ( req.params.id ). If the user is not found, responds with 404 Not Found . Deletes all tasks and submissions associated with the user. Responds with a success message indicating the user and associated data have been deleted.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315542.4762554, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Admin Overview' to API. Metadata: {'content': \"Endpoint : GET /api/admin/overview Purpose : Provides high-level statistics about the platform. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Aggregates key statistics: Total number of users. Total number of tasks. Number of completed tasks. Completion rate (percentage of tasks completed). Responds with the aggregated statistics. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Aggregates key statistics: Total number of users. Total number of tasks. Number of completed tasks. Completion rate (percentage of tasks completed). Responds with the aggregated statistics. Total number of users. Total number of tasks. Number of completed tasks. Completion rate (percentage of tasks completed).\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315544.5350187, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Admin Task Status' to API. Metadata: {'content': \"Endpoint : GET /api/admin/task-status Purpose : Provides a breakdown of tasks based on their status. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Counts tasks in different statuses: completed : Number of tasks marked as completed. inProgress : Number of tasks currently in progress. notStarted : Number of tasks not yet started (neither completed nor in progress). Responds with the counts for each status category. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Counts tasks in different statuses: completed : Number of tasks marked as completed. inProgress : Number of tasks currently in progress. notStarted : Number of tasks not yet started (neither completed nor in progress). Responds with the counts for each status category. completed : Number of tasks marked as completed. inProgress : Number of tasks currently in progress. notStarted : Number of tasks not yet started (neither completed nor in progress).\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315546.6138167, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Admin User Activity' to API. Metadata: {'content': \"Endpoint : GET /api/admin/user-activity Purpose : Provides detailed activity metrics for each user. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates task statistics: Total number of tasks. Number of completed tasks. Number of tasks in progress. Constructs an array of user activity summaries containing username and task statistics. Responds with the array of user activity summaries. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates task statistics: Total number of tasks. Number of completed tasks. Number of tasks in progress. Constructs an array of user activity summaries containing username and task statistics. Responds with the array of user activity summaries. Total number of tasks. Number of completed tasks. Number of tasks in progress.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315548.6686282, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Admin Analytics Routes' to API. Metadata: {'content': 'These routes provide advanced analytical insights into user behavior and task completion trends.', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315550.7485147, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'User Activity Analytics' to API. Metadata: {'content': \"Endpoint : GET /api/admin/analytics/user-activity Purpose : Provides comprehensive activity data for each user. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates: Total number of tasks. Number of completed tasks. Number of tasks in progress. Timestamp of the last activity ( lastActive ), determined by the most recent updated_at timestamp of their tasks. Constructs an array of user activity data containing user ID, username, task counts, and last active timestamp. Responds with the array of user activity data. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Retrieves all users from the database. For each user, aggregates: Total number of tasks. Number of completed tasks. Number of tasks in progress. Timestamp of the last activity ( lastActive ), determined by the most recent updated_at timestamp of their tasks. Constructs an array of user activity data containing user ID, username, task counts, and last active timestamp. Responds with the array of user activity data. Total number of tasks. Number of completed tasks. Number of tasks in progress. Timestamp of the last activity ( lastActive ), determined by the most recent updated_at timestamp of their tasks.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315552.832865, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Task Completion Analytics' to API. Metadata: {'content': \"Endpoint : GET /api/admin/analytics/task-completion Purpose : Tracks task completion trends over the past week. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Calculates the date range for the past 7 days. For each day in the past week: Counts the number of tasks marked as completed within that day. Records the date and the count of completed tasks. Compiles the data into an array representing each day's completed task count. Responds with the array of daily task completion counts in chronological order. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Calculates the date range for the past 7 days. For each day in the past week: Counts the number of tasks marked as completed within that day. Records the date and the count of completed tasks. Compiles the data into an array representing each day's completed task count. Responds with the array of daily task completion counts in chronological order. Counts the number of tasks marked as completed within that day. Records the date and the count of completed tasks.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315554.9059086, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Admin Task Assignment' to API. Metadata: {'content': \"Endpoint : POST /api/admin/tasks/assign Purpose : Allows admins to assign new tasks to users, with optional file uploads. Access Control : Requires authentication and 'admin' role. Process : Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Extracts task-related fields such as user_id , prompt , difficulty , prompt_type , due_date , project_id , and file upload data from the request body. Searches for the user by user_id . If the user is not found, responds with 404 Not Found . Handles file upload (if any) using Multer: If a file is uploaded, constructs the downloadable_file_url using the createFileUrl utility function. Calls createNewTask with the provided fields, including any uploaded file URL and required skills. Saves the new task to the database. Responds with the newly created task object. Verifies the authenticated user has the 'admin' role. If not, responds with 403 Forbidden . Extracts task-related fields such as user_id , prompt , difficulty , prompt_type , due_date , project_id , and file upload data from the request body. Searches for the user by user_id . If the user is not found, responds with 404 Not Found . Handles file upload (if any) using Multer: If a file is uploaded, constructs the downloadable_file_url using the createFileUrl utility function. Calls createNewTask with the provided fields, including any uploaded file URL and required skills. Saves the new task to the database. Responds with the newly created task object. If a file is uploaded, constructs the downloadable_file_url using the createFileUrl utility function.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315556.9794354, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Miscellaneous Routes' to API. Metadata: {'content': '', 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315559.0525799, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'File Upload Endpoint' to API. Metadata: {'content': \"Endpoint : POST /api/upload Purpose : Allows authenticated users to upload files associated with tasks. Access Control : Requires authentication. Process : Uses the authenticateToken middleware to verify the user's identity. Handles the file upload using Multer's upload.single('file') middleware. Validates the presence of the uploaded file. If absent, responds with 400 Bad Request . Constructs the file URL and identifies the file type ( mimetype ). Extracts taskId from the request body and searches for the associated task. If the task is not found, responds with 404 Not Found . Updates the task's downloadable_file_url and file_type with the uploaded file's details. Updates the task's updated_at timestamp. If the task's prompt_type is 'code', updates the code field with the provided code. Saves the updated task to the database. Creates a new Submission instance linking the upload to the task and user. Saves the new submission to the database. Responds with a success message, including file details and submission information. Uses the authenticateToken middleware to verify the user's identity. Handles the file upload using Multer's upload.single('file') middleware. Validates the presence of the uploaded file. If absent, responds with 400 Bad Request . Constructs the file URL and identifies the file type ( mimetype ). Extracts taskId from the request body and searches for the associated task. If the task is not found, responds with 404 Not Found . Updates the task's downloadable_file_url and file_type with the uploaded file's details. Updates the task's updated_at timestamp. If the task's prompt_type is 'code', updates the code field with the provided code. Saves the updated task to the database. Creates a new Submission instance linking the upload to the task and user. Saves the new submission to the database. Responds with a success message, including file details and submission information.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315561.1317098, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Processor Endpoint' to API. Metadata: {'content': \"Endpoint : POST /processor Purpose : Processes incoming data, sanitizes it, and stores it in a JSON file. Access Control : Not explicitly restricted; consider adding authentication if necessary. Process : Extracts prompt , response , and timestamp from the request body. Sanitizes the extracted data using sanitize-html to prevent XSS and other injection attacks. Attempts to read existing data from 'data.json' . If the file doesn't exist or is empty, initializes an empty array. Appends the sanitized data to the existing data array. Writes the updated data array back to 'data.json' with pretty formatting. Responds with a success message indicating the data has been processed and saved. Handles any errors by responding with a 500 Internal Server Error and an appropriate message. Extracts prompt , response , and timestamp from the request body. Sanitizes the extracted data using sanitize-html to prevent XSS and other injection attacks. Attempts to read existing data from 'data.json' . If the file doesn't exist or is empty, initializes an empty array. Appends the sanitized data to the existing data array. Writes the updated data array back to 'data.json' with pretty formatting. Responds with a success message indicating the data has been processed and saved. Handles any errors by responding with a 500 Internal Server Error and an appropriate message.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315563.2229211, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Frontend Routes' to API. Metadata: {'content': \"These routes render frontend pages using the EJS templating engine. Root Route ( GET / ) : Purpose : Renders the home page. Process : Uses EJS to render the 'home' view. Staff Route ( GET /staff ) : Purpose : Renders the staff page. Process : Uses EJS to render the 'indexrework' view. Test4 Route ( GET /test4 ) : Purpose : Renders the test4 page. Process : Uses EJS to render the 'test4' view. Index Rework Route ( GET /indexrework ) : Purpose : Renders the index rework page. Process : Uses EJS to render the 'indexrework' view. Admin Route ( GET /admin ) : Purpose : Renders the admin dashboard. Process : Uses EJS to render the 'admin' view. Purpose : Renders the home page. Process : Uses EJS to render the 'home' view. Purpose : Renders the staff page. Process : Uses EJS to render the 'indexrework' view. Purpose : Renders the test4 page. Process : Uses EJS to render the 'test4' view. Purpose : Renders the index rework page. Process : Uses EJS to render the 'indexrework' view. Purpose : Renders the admin dashboard. Process : Uses EJS to render the 'admin' view. Understood. We'll continue the comprehensive description of your Next.js 14 project by picking up from where the interruption occurred, specifically focusing on the Users Routes . We'll ensure that each file is thoroughly described, including its purpose, key components/functions, integration points, data flow, and error handling. Additionally, we'll incorporate Next.js 14 best practices and provide clear file paths to maintain a well-structured project.\", 'level': 3, 'parent': 'Backend Components', 'children': [], 'timestamp': 1728315565.3084226, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Users Routes' to API. Metadata: {'content': '', 'level': 2, 'parent': None, 'children': ['1. users/route.js', '2. users/login/route.js', '3. users/me/route.js', '4. users/register/route.js', '5. users/[id]/route.js', 'Summary of Users Routes Implementation'], 'timestamp': 1728315567.3715298, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '1. users/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/route.js ``` Purpose: Manages user-related operations, including fetching user information, updating user details, and deleting users. This route is primarily intended for administrative tasks, allowing admins to perform bulk user management actions. Key Components/Functions: GET : Retrieves a list of all users along with their task summaries. This is restricted to admin users to ensure that only authorized personnel can access sensitive user data. POST , PUT , DELETE : While the primary user creation and authentication are handled in sub-routes (e.g., register , login ), these methods can be extended for additional user-related actions if necessary. Implementation Guidelines: Purpose : Facilitate comprehensive user management, enabling administrators to retrieve, update, and delete user data as required. Key Functions : get : Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Ensures that only users with the 'admin' role can access this endpoint. Data Retrieval : Utilizes MongoDB queries to aggregate user data and their related tasks. post , put , delete : Potential Uses : Could handle bulk updates, assignments, or deletions if required in the future. Current State : If not needed immediately, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Integrates with middleware to verify that the requester has the appropriate admin privileges. Database Models : Interacts with the User and Task models to perform CRUD operations. Data Flow : Incoming Request : The admin sends a GET request to retrieve user data. Processing : The server verifies admin privileges, queries the database, and compiles user summaries. Response : Returns a JSON array of users with their task summaries. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Database Errors : Returns 500 Internal Server Error for issues during data retrieval. Method Not Allowed : Returns 405 Method Not Allowed for unsupported HTTP methods. get : Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Ensures that only users with the 'admin' role can access this endpoint. Data Retrieval : Utilizes MongoDB queries to aggregate user data and their related tasks. post , put , delete : Potential Uses : Could handle bulk updates, assignments, or deletions if required in the future. Current State : If not needed immediately, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Ensures that only users with the 'admin' role can access this endpoint. Data Retrieval : Utilizes MongoDB queries to aggregate user data and their related tasks. Potential Uses : Could handle bulk updates, assignments, or deletions if required in the future. Current State : If not needed immediately, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Integrates with middleware to verify that the requester has the appropriate admin privileges. Database Models : Interacts with the User and Task models to perform CRUD operations. Incoming Request : The admin sends a GET request to retrieve user data. Processing : The server verifies admin privileges, queries the database, and compiles user summaries. Response : Returns a JSON array of users with their task summaries. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Database Errors : Returns 500 Internal Server Error for issues during data retrieval. Method Not Allowed : Returns 405 Method Not Allowed for unsupported HTTP methods. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/route.js': get Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks for admin users, retrieves user data along with their task summaries from MongoDB, and handles errors appropriately. ```\", 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315569.438441, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '2. users/login/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/login/route.js ``` Purpose: Handles user authentication by verifying credentials and issuing JWT tokens. This endpoint is essential for allowing users to securely log into the system. Key Components/Functions: POST : Authenticates users by validating their email and password, then generates and returns a JWT token upon successful authentication. Implementation Guidelines: Purpose : Enable users to securely log into the application by validating their credentials and providing authentication tokens for subsequent requests. Key Functions : post : Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Integration Points : Database Models : Interacts with the User model to fetch and verify user data. Authentication Services : Utilizes JWT for token generation. Data Flow : Incoming Request : The user submits their email and password via a POST request. Processing : The server validates the credentials and, if successful, generates a JWT token. Response : Returns the JWT token and sanitized user information. Error Handling : Invalid Credentials : Returns 400 Bad Request with a message indicating invalid email or password. User Not Found : Returns 404 Not Found if the email does not correspond to any user. Server Errors : Returns 500 Internal Server Error for unexpected issues. post : Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Database Models : Interacts with the User model to fetch and verify user data. Authentication Services : Utilizes JWT for token generation. Incoming Request : The user submits their email and password via a POST request. Processing : The server validates the credentials and, if successful, generates a JWT token. Response : Returns the JWT token and sanitized user information. Invalid Credentials : Returns 400 Bad Request with a message indicating invalid email or password. User Not Found : Returns 404 Not Found if the email does not correspond to any user. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Prompt: ```\\nImplement the following function for the file 'app/api/users/login/route.js': post Ensure that the implementation follows Next.js 14 best practices, securely authenticates users by verifying their email and password against the database, generates JWT tokens upon successful authentication, and handles errors appropriately. ```\", 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315571.5369012, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '3. users/me/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/me/route.js ``` Purpose: Provides an endpoint for authenticated users to retrieve their own profile information. This allows users to view and possibly update their personal details. Key Components/Functions: GET : Retrieves the authenticated user's profile information. PUT : Allows the authenticated user to update their profile details. DELETE : Potentially allows the authenticated user to delete their own account (if permitted). Implementation Guidelines: Purpose : Enable users to access and manage their personal profile information securely. Key Functions : get : Functionality : Retrieves the current authenticated user's data from the database. Ensures that sensitive information (e.g., password_hash ) is excluded from the response. Security : Requires the user to be authenticated via JWT. put : Functionality : Allows users to update their profile information such as username , bio , and skills . Validates the input data before updating. Security : Ensures that only the authenticated user's data can be modified. delete : Functionality : Allows users to delete their own account. May require additional confirmation or password re-entry for security. Security : Ensures that only the authenticated user can delete their account. Integration Points : Authentication Middleware : Ensures that the user is authenticated before accessing these routes. Database Models : Interacts with the User model to fetch and update user data. Data Flow : Incoming Request : The user sends a GET , PUT , or DELETE request with their JWT token. Processing : The server authenticates the user and performs the requested action on the user's data. Response : Returns the updated user data, confirmation of deletion, or relevant error messages. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Not Found Errors : Returns 404 Not Found if the user does not exist (though unlikely in this context). get : Functionality : Retrieves the current authenticated user's data from the database. Ensures that sensitive information (e.g., password_hash ) is excluded from the response. Security : Requires the user to be authenticated via JWT. put : Functionality : Allows users to update their profile information such as username , bio , and skills . Validates the input data before updating. Security : Ensures that only the authenticated user's data can be modified. delete : Functionality : Allows users to delete their own account. May require additional confirmation or password re-entry for security. Security : Ensures that only the authenticated user can delete their account. Functionality : Retrieves the current authenticated user's data from the database. Ensures that sensitive information (e.g., password_hash ) is excluded from the response. Security : Requires the user to be authenticated via JWT. Retrieves the current authenticated user's data from the database. Ensures that sensitive information (e.g., password_hash ) is excluded from the response. Requires the user to be authenticated via JWT. Functionality : Allows users to update their profile information such as username , bio , and skills . Validates the input data before updating. Security : Ensures that only the authenticated user's data can be modified. Allows users to update their profile information such as username , bio , and skills . Validates the input data before updating. Ensures that only the authenticated user's data can be modified. Functionality : Allows users to delete their own account. May require additional confirmation or password re-entry for security. Security : Ensures that only the authenticated user can delete their account. Allows users to delete their own account. May require additional confirmation or password re-entry for security. Ensures that only the authenticated user can delete their account. Authentication Middleware : Ensures that the user is authenticated before accessing these routes. Database Models : Interacts with the User model to fetch and update user data. Incoming Request : The user sends a GET , PUT , or DELETE request with their JWT token. Processing : The server authenticates the user and performs the requested action on the user's data. Response : Returns the updated user data, confirmation of deletion, or relevant error messages. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Not Found Errors : Returns 404 Not Found if the user does not exist (though unlikely in this context). Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/me/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication checks, securely handles user data retrieval and updates, and manages errors appropriately. ```\", 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315573.6378112, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '4. users/register/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/register/route.js ``` Purpose: Handles user registration by creating new user accounts. This endpoint manages the signup process, including validating input data, hashing passwords, and storing user information in the database. Key Components/Functions: POST : Registers a new user by accepting user details, validating them, hashing the password, and storing the user in the database. Implementation Guidelines: Purpose : Allow new users to create accounts by providing necessary information such as username , email , and password . Key Functions : post : Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for completeness and correctness. Checks for existing users with the same email to prevent duplicates. Hashes the user's password using bcrypt before storing it. Creates a new user entry in the database with default roles and additional information ( bio , skills ). Optionally, creates a welcome task for the new user to encourage profile completion. Generates a JWT token for the newly registered user. Returns the token and sanitized user information in the response. Security : Ensures passwords are never stored in plain text. Validates and sanitizes all input data to prevent injection attacks. Integration Points : Database Models : Interacts with the User and Task models to create new entries. Authentication Services : Utilizes JWT for token generation. Data Flow : Incoming Request : The user submits their registration details via a POST request. Processing : The server validates the data, hashes the password, creates the user, assigns a welcome task, and generates a JWT token. Response : Returns the JWT token and user information, enabling the user to authenticate subsequent requests. Error Handling : Validation Errors : Returns 400 Bad Request if required fields are missing or invalid. Duplicate Users : Returns 400 Bad Request if a user with the provided email already exists. Server Errors : Returns 500 Internal Server Error for unexpected issues during registration. post : Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for completeness and correctness. Checks for existing users with the same email to prevent duplicates. Hashes the user's password using bcrypt before storing it. Creates a new user entry in the database with default roles and additional information ( bio , skills ). Optionally, creates a welcome task for the new user to encourage profile completion. Generates a JWT token for the newly registered user. Returns the token and sanitized user information in the response. Security : Ensures passwords are never stored in plain text. Validates and sanitizes all input data to prevent injection attacks. Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for completeness and correctness. Checks for existing users with the same email to prevent duplicates. Hashes the user's password using bcrypt before storing it. Creates a new user entry in the database with default roles and additional information ( bio , skills ). Optionally, creates a welcome task for the new user to encourage profile completion. Generates a JWT token for the newly registered user. Returns the token and sanitized user information in the response. Security : Ensures passwords are never stored in plain text. Validates and sanitizes all input data to prevent injection attacks. Receives user registration details ( username , email , password , etc.). Validates the input data for completeness and correctness. Checks for existing users with the same email to prevent duplicates. Hashes the user's password using bcrypt before storing it. Creates a new user entry in the database with default roles and additional information ( bio , skills ). Optionally, creates a welcome task for the new user to encourage profile completion. Generates a JWT token for the newly registered user. Returns the token and sanitized user information in the response. Ensures passwords are never stored in plain text. Validates and sanitizes all input data to prevent injection attacks. Database Models : Interacts with the User and Task models to create new entries. Authentication Services : Utilizes JWT for token generation. Incoming Request : The user submits their registration details via a POST request. Processing : The server validates the data, hashes the password, creates the user, assigns a welcome task, and generates a JWT token. Response : Returns the JWT token and user information, enabling the user to authenticate subsequent requests. Validation Errors : Returns 400 Bad Request if required fields are missing or invalid. Duplicate Users : Returns 400 Bad Request if a user with the provided email already exists. Server Errors : Returns 500 Internal Server Error for unexpected issues during registration. Implementation Guidelines: Input Validation : Ensure all required fields are provided and adhere to expected formats (e.g., valid email, strong password). Password Security : Use bcrypt to hash passwords before storing them in the database. Role Assignment : Assign a default role (e.g., 'user') upon registration. Optionally, create a welcome task to guide new users. JWT Token Generation : Create a JWT token containing user ID, email, and role, signed with a secure secret key. Response Structure : Return the JWT token and user data excluding sensitive information like password_hash . Implementation Prompt: ```\\nImplement the following function for the file 'app/api/users/register/route.js': post Ensure that the implementation follows Next.js 14 best practices, securely handles user registration by validating input data, hashing passwords, storing user information in MongoDB, assigning a welcome task, generating JWT tokens, and managing errors appropriately. ```\", 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315575.7286656, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '5. users/[id]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/[id]/route.js ``` Purpose: Handles operations on individual user accounts based on their unique identifier ( id ). This includes fetching, updating, and deleting specific users. Primarily intended for administrative use to manage individual user profiles. Key Components/Functions: GET : Retrieves detailed information about a specific user. PUT : Updates the details of a specific user. DELETE : Deletes a specific user from the system. Implementation Guidelines: Purpose : Enable administrators to perform CRUD (Create, Read, Update, Delete) operations on individual user accounts. Key Functions : get : Functionality : Retrieves detailed information about a user identified by id . Ensures that sensitive data (e.g., password_hash ) is excluded from the response. Authorization : Restricted to admin users to prevent unauthorized access to user data. put : Functionality : Allows admins to update user details such as username , email , role , bio , and skills . Validates the input data before updating. Authorization : Ensures that only admins can modify user data. delete : Functionality : Deletes the user identified by id from the database. Optionally, handles cascading deletions or data clean-up (e.g., removing associated tasks and submissions). Authorization : Restricted to admin users to prevent unauthorized account deletions. Integration Points : Authentication Middleware : Verifies admin privileges before allowing access to these routes. Database Models : Interacts with the User , Task , and Submission models to perform necessary operations. Data Flow : Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific user ID. Processing : The server authenticates the admin, performs the requested action on the user data. Response : Returns the updated user data, confirmation of deletion, or relevant error messages. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the user with the specified ID does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. get : Functionality : Retrieves detailed information about a user identified by id . Ensures that sensitive data (e.g., password_hash ) is excluded from the response. Authorization : Restricted to admin users to prevent unauthorized access to user data. put : Functionality : Allows admins to update user details such as username , email , role , bio , and skills . Validates the input data before updating. Authorization : Ensures that only admins can modify user data. delete : Functionality : Deletes the user identified by id from the database. Optionally, handles cascading deletions or data clean-up (e.g., removing associated tasks and submissions). Authorization : Restricted to admin users to prevent unauthorized account deletions. Functionality : Retrieves detailed information about a user identified by id . Ensures that sensitive data (e.g., password_hash ) is excluded from the response. Authorization : Restricted to admin users to prevent unauthorized access to user data. Retrieves detailed information about a user identified by id . Ensures that sensitive data (e.g., password_hash ) is excluded from the response. Restricted to admin users to prevent unauthorized access to user data. Functionality : Allows admins to update user details such as username , email , role , bio , and skills . Validates the input data before updating. Authorization : Ensures that only admins can modify user data. Allows admins to update user details such as username , email , role , bio , and skills . Validates the input data before updating. Ensures that only admins can modify user data. Functionality : Deletes the user identified by id from the database. Optionally, handles cascading deletions or data clean-up (e.g., removing associated tasks and submissions). Authorization : Restricted to admin users to prevent unauthorized account deletions. Deletes the user identified by id from the database. Optionally, handles cascading deletions or data clean-up (e.g., removing associated tasks and submissions). Restricted to admin users to prevent unauthorized account deletions. Authentication Middleware : Verifies admin privileges before allowing access to these routes. Database Models : Interacts with the User , Task , and Submission models to perform necessary operations. Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific user ID. Processing : The server authenticates the admin, performs the requested action on the user data. Response : Returns the updated user data, confirmation of deletion, or relevant error messages. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the user with the specified ID does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Guidelines: Dynamic Routing : Utilize Next.js dynamic route segments ( [id] ) to target specific users. Input Validation : Validate all incoming data to ensure it meets the required formats and constraints. Data Sanitization : Sanitize input data to prevent injection attacks and ensure data integrity. Role-Based Access Control : Enforce that only users with the 'admin' role can access these endpoints. Cascading Deletions : When deleting a user, consider whether to also delete or reassign their associated tasks and submissions to maintain database consistency. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/[id]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks for admin users, performs secure data retrieval and manipulation for individual users, handles cascading deletions appropriately, and manages errors effectively. ```\", 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315577.8192573, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section 'Summary of Users Routes Implementation' to API. Metadata: {'content': 'By following the above guidelines, the Users Routes in your Next.js 14 project will effectively manage user-related operations with robust security and adherence to best practices. Each route ensures that only authorized users (primarily admins) can perform sensitive actions, maintains data integrity through validation and sanitization, and provides clear error messaging to facilitate debugging and user feedback.', 'level': 3, 'parent': 'Users Routes', 'children': [], 'timestamp': 1728315579.9253528, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section 'Continuing with Other API Routes' to API. Metadata: {'content': 'Following the detailed descriptions for the Users Routes, the same comprehensive approach should be applied to all other API routes in your project structure. Below is a brief outline for each remaining route to ensure consistency and completeness.', 'level': 2, 'parent': None, 'children': ['6. admin/analytics/task-completion/route.js', '7. admin/analytics/user-activity/route.js', '8. admin/overview/route.js', '9. admin/promote/route.js', '10. admin/task-status/route.js', '11. admin/tasks/assign/route.js', '12. admin/tasks/[id]/route.js', '13. admin/user-activity/route.js', '14. admin/users/route.js', '15. admin/overview/route.js', '16. processor/route.js', '17. comments/route.js', '18. comments/[taskId]/route.js'], 'timestamp': 1728315582.0125196, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '6. admin/analytics/task-completion/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/analytics/task-completion/route.js ``` Purpose: Provides analytical data on task completion rates, enabling administrators to monitor productivity and identify trends over time. Key Components/Functions: GET : Retrieves statistics on completed tasks, possibly segmented by time periods or user demographics. Implementation Guidelines: Data Aggregation : Utilize MongoDB's aggregation framework to compute task completion statistics. Authorization : Restrict access to admin users. Data Presentation : Structure the response data in a format suitable for frontend visualization (e.g., charts, graphs). Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/analytics/task-completion/route.js' to retrieve task completion statistics. Ensure secure access for admin users and structure the data for frontend consumption. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315584.106238, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '7. admin/analytics/user-activity/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/analytics/user-activity/route.js ``` Purpose: Provides insights into user activity patterns, helping administrators understand engagement levels and identify active or inactive users. Key Components/Functions: GET : Retrieves data on user activities, such as login frequency, task participation, and submission rates. Implementation Guidelines: Data Analysis : Aggregate user activity data using MongoDB queries. Authorization : Ensure only admins can access this data. Reporting : Format the data to support detailed reports or dashboards in the frontend. Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/analytics/user-activity/route.js' to fetch user activity data. Ensure it is accessible only to admin users and the data is structured for effective reporting. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315586.1928365, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '8. admin/overview/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/overview/route.js ``` Purpose: Serves as a summary endpoint providing high-level metrics about the overall system, such as total users, total tasks, and completion rates. Key Components/Functions: GET : Retrieves aggregate data summarizing the system's state. Implementation Guidelines: Data Summarization : Compute totals and percentages using MongoDB aggregation. Authorization : Limit access to admin users. Response Structure : Provide concise summary metrics suitable for an admin dashboard. Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/overview/route.js' to provide summary metrics of the system. Ensure only admin users can access this data and format the response for dashboard display. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315588.258834, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '9. admin/promote/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/promote/route.js ``` Purpose: Allows administrators to promote regular users to higher roles (e.g., 'admin'), granting them elevated privileges within the system. Key Components/Functions: POST : Promotes a user by updating their role in the database. Implementation Guidelines: Input Validation : Ensure that the request contains a valid user identifier. Authorization : Confirm that the requester has admin privileges. Role Update : Safely update the user's role in MongoDB. Response : Confirm the successful promotion of the user. Implementation Prompt: ```\\nImplement the POST function for the file 'app/api/admin/promote/route.js' to promote a user to an admin role. Ensure that only admins can perform this action, validate the input, update the user's role in the database, and handle errors appropriately. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315590.3531866, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '10. admin/task-status/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/task-status/route.js ``` Purpose: Provides real-time status updates on tasks, such as how many are completed, in progress, or not started. This helps administrators monitor workflow and identify bottlenecks. Key Components/Functions: GET : Retrieves counts of tasks based on their current status. Implementation Guidelines: Data Retrieval : Use MongoDB queries to count tasks in different statuses. Authorization : Restrict access to admin users. Response Format : Return a JSON object with counts for each task status category. Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/task-status/route.js' to fetch counts of tasks based on their status (completed, in progress, not started). Ensure secure access for admin users and structure the response appropriately. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315592.4384418, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '11. admin/tasks/assign/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/tasks/assign/route.js ``` Purpose: Enables administrators to assign new tasks to users. This endpoint handles the creation of tasks and their association with specific users or projects. Key Components/Functions: POST : Creates and assigns a new task to a user. Implementation Guidelines: Input Validation : Ensure that all required task details are provided and valid. Authorization : Only admin users should be able to assign tasks. Task Creation : Create a new task entry in MongoDB and associate it with the specified user and project. File Handling : If the task includes file uploads, handle them securely using Next.js's file handling capabilities. Response : Return the newly created task details. Implementation Prompt: ```\\nImplement the POST function for the file 'app/api/admin/tasks/assign/route.js' to assign a new task to a user. Ensure that only admins can perform this action, validate the input data, handle any file uploads if necessary, create the task in the database, and respond with the task details. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315594.50712, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '12. admin/tasks/[id]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/tasks/[id]/route.js ``` Purpose: Manages individual tasks based on their unique identifier ( id ). This includes fetching task details, updating task information, and deleting tasks. Key Components/Functions: GET : Retrieves detailed information about a specific task. PUT : Updates the details of a specific task. DELETE : Deletes a specific task from the system. Implementation Guidelines: Purpose : Enable administrators to perform CRUD operations on individual tasks, ensuring efficient task management. Key Functions : get : Functionality : Fetches detailed information about the task identified by id . Includes associated user and project details if necessary. Authorization : Restricted to admin users. put : Functionality : Updates task attributes such as title , description , status , due_date , etc. Validates input data to maintain data integrity. Authorization : Ensures only admins can modify tasks. delete : Functionality : Removes the task identified by id from the database. Optionally handles cascading deletions or reassignments. Authorization : Restricted to admin users. Integration Points : Authentication Middleware : Confirms admin privileges. Database Models : Interacts with the Task model for data manipulation. Data Flow : Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific task ID. Processing : The server authenticates the admin, retrieves or modifies the task as requested. Response : Returns the task details, confirmation of updates, or deletion acknowledgments. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the task does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. get : Functionality : Fetches detailed information about the task identified by id . Includes associated user and project details if necessary. Authorization : Restricted to admin users. put : Functionality : Updates task attributes such as title , description , status , due_date , etc. Validates input data to maintain data integrity. Authorization : Ensures only admins can modify tasks. delete : Functionality : Removes the task identified by id from the database. Optionally handles cascading deletions or reassignments. Authorization : Restricted to admin users. Functionality : Fetches detailed information about the task identified by id . Includes associated user and project details if necessary. Authorization : Restricted to admin users. Fetches detailed information about the task identified by id . Includes associated user and project details if necessary. Restricted to admin users. Functionality : Updates task attributes such as title , description , status , due_date , etc. Validates input data to maintain data integrity. Authorization : Ensures only admins can modify tasks. Updates task attributes such as title , description , status , due_date , etc. Validates input data to maintain data integrity. Ensures only admins can modify tasks. Functionality : Removes the task identified by id from the database. Optionally handles cascading deletions or reassignments. Authorization : Restricted to admin users. Removes the task identified by id from the database. Optionally handles cascading deletions or reassignments. Restricted to admin users. Authentication Middleware : Confirms admin privileges. Database Models : Interacts with the Task model for data manipulation. Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific task ID. Processing : The server authenticates the admin, retrieves or modifies the task as requested. Response : Returns the task details, confirmation of updates, or deletion acknowledgments. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the task does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Guidelines: Dynamic Routing : Utilize Next.js dynamic route segments ( [id] ) to target specific tasks. Input Validation : Ensure all incoming data for updates meets the required formats and constraints. Data Sanitization : Sanitize input data to prevent injection attacks and maintain data integrity. Role-Based Access Control : Enforce that only users with the 'admin' role can access these endpoints. Cascading Deletions : When deleting a task, consider whether to also delete or archive associated submissions or comments. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/admin/tasks/[id]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks for admin users, performs secure data retrieval and manipulation for individual tasks, handles cascading deletions appropriately, and manages errors effectively. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315596.5802088, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '13. admin/user-activity/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/user-activity/route.js ``` Purpose: Provides detailed insights into user activities, such as task completions, submissions, and login frequencies. This data assists administrators in monitoring user engagement and system utilization. Key Components/Functions: GET : Retrieves comprehensive user activity data for analysis. Implementation Guidelines: Data Aggregation : Use MongoDB's aggregation capabilities to compile detailed activity logs. Authorization : Restrict access to admin users. Data Presentation : Structure the response data to support in-depth analysis and visualization on the frontend. Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/user-activity/route.js' to fetch detailed user activity data. Ensure that only admin users can access this endpoint and that the data is formatted to support comprehensive analysis. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315598.679046, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '14. admin/users/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/users/route.js ``` Purpose: Manages administrative actions on user accounts, including listing all users, updating user roles, and deleting users. This endpoint is central to user administration tasks. Key Components/Functions: GET : Retrieves a list of all users with summaries of their activities and roles. PUT : Updates user information, such as roles and personal details. DELETE : Deletes a user account from the system. Implementation Guidelines: Purpose : Provide administrators with the tools to manage user accounts effectively, including role assignments and account deletions. Key Functions : get : Functionality : Fetches all users from the database. Includes summaries such as total tasks, completed tasks, and in-progress tasks. Authorization : Restricted to admin users. put : Functionality : Allows admins to update user details like username , email , role , bio , and skills . Validates and sanitizes input data before updating. Authorization : Ensures only admins can modify user data. delete : Functionality : Deletes a user account identified by id . Optionally handles the deletion of associated tasks and submissions to maintain data consistency. Authorization : Restricted to admin users. Integration Points : Authentication Middleware : Validates admin privileges. Database Models : Interacts with User , Task , and Submission models for data operations. Data Flow : Incoming Request : Admin sends a GET , PUT , or DELETE request to manage user accounts. Processing : The server authenticates the admin and performs the requested operation on user data. Response : Returns the list of users, updated user information, or deletion confirmations. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the targeted user does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. get : Functionality : Fetches all users from the database. Includes summaries such as total tasks, completed tasks, and in-progress tasks. Authorization : Restricted to admin users. put : Functionality : Allows admins to update user details like username , email , role , bio , and skills . Validates and sanitizes input data before updating. Authorization : Ensures only admins can modify user data. delete : Functionality : Deletes a user account identified by id . Optionally handles the deletion of associated tasks and submissions to maintain data consistency. Authorization : Restricted to admin users. Functionality : Fetches all users from the database. Includes summaries such as total tasks, completed tasks, and in-progress tasks. Authorization : Restricted to admin users. Fetches all users from the database. Includes summaries such as total tasks, completed tasks, and in-progress tasks. Restricted to admin users. Functionality : Allows admins to update user details like username , email , role , bio , and skills . Validates and sanitizes input data before updating. Authorization : Ensures only admins can modify user data. Allows admins to update user details like username , email , role , bio , and skills . Validates and sanitizes input data before updating. Ensures only admins can modify user data. Functionality : Deletes a user account identified by id . Optionally handles the deletion of associated tasks and submissions to maintain data consistency. Authorization : Restricted to admin users. Deletes a user account identified by id . Optionally handles the deletion of associated tasks and submissions to maintain data consistency. Restricted to admin users. Authentication Middleware : Validates admin privileges. Database Models : Interacts with User , Task , and Submission models for data operations. Incoming Request : Admin sends a GET , PUT , or DELETE request to manage user accounts. Processing : The server authenticates the admin and performs the requested operation on user data. Response : Returns the list of users, updated user information, or deletion confirmations. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Not Found Errors : Returns 404 Not Found if the targeted user does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Guidelines: Bulk Operations : Consider implementing bulk update or delete functionalities if needed. Input Validation : Ensure that updates do not allow unauthorized changes (e.g., escalating one's own privileges). Data Sanitization : Sanitize all inputs to prevent injection attacks and ensure data integrity. Response Structure : Provide clear and concise responses, especially when dealing with multiple users. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/admin/users/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks for admin users, securely handles user data retrieval and manipulation, manages cascading deletions appropriately, and handles errors effectively. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315600.7574675, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '15. admin/overview/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/admin/overview/route.js ``` Purpose: Provides a high-level overview of the system's current state, including metrics like total users, total tasks, completed tasks, and completion rates. This endpoint is essential for administrators to get quick insights into the platform's performance. Key Components/Functions: GET : Retrieves aggregate metrics summarizing the system's status. Implementation Guidelines: Data Summarization : Utilize MongoDB's aggregation framework to compute totals and rates. Authorization : Restrict access to admin users. Response Structure : Return a JSON object containing key metrics suitable for dashboard display. Implementation Prompt: ```\\nImplement the GET function for the file 'app/api/admin/overview/route.js' to provide summary metrics such as total users, total tasks, completed tasks, and completion rates. Ensure that only admin users can access this endpoint and that the data is formatted for easy integration into an admin dashboard. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315602.8721342, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '16. processor/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/processor/route.js ``` Purpose: Handles data processing tasks, such as ingesting and storing data from external sources or performing batch operations. This endpoint may serve as a bridge for data manipulation and analysis within the system. Key Components/Functions: POST : Accepts data payloads for processing and stores them appropriately. Implementation Guidelines: Data Validation : Ensure that incoming data is validated and sanitized before processing. Security : Implement authentication checks if the processor is sensitive to unauthorized data submissions. Data Storage : Define how and where processed data will be stored, whether in the database or as files. Error Handling : Manage errors related to data processing, storage failures, or invalid inputs. Implementation Prompt: ```\\nImplement the POST function for the file 'app/api/processor/route.js' to handle incoming data payloads, validate and sanitize the data, process it as required, store it in the database or file system, and respond with appropriate success or error messages. Ensure adherence to Next.js 14 best practices and secure handling of data. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315604.9401739, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '17. comments/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/comments/route.js ``` Purpose: Manages comment-related operations, allowing users to add comments to tasks and administrators to oversee or moderate them. Key Components/Functions: POST : Adds a new comment to a specific task. GET , PUT , DELETE : Potentially handles fetching, updating, or deleting comments if necessary. Implementation Guidelines: Purpose : Enable users to provide feedback, ask questions, or engage in discussions related to tasks. Key Functions : post : Functionality : Accepts comment data ( task_id , content ). Validates that the task exists. Associates the comment with the specified task and user. Saves the comment to the database. Authorization : Typically requires the user to be authenticated to post comments. get , put , delete : Potential Uses : Fetching comments for a task, editing comments, or deleting inappropriate comments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Ensures that only authenticated users can post comments. Database Models : Interacts with the Comment and Task models to manage comment data. Data Flow : Incoming Request : User sends a POST request with comment data. Processing : The server validates the task, associates the comment with the user and task, and saves it to the database. Response : Returns the newly created comment details. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Validation Errors : Returns 400 Bad Request if required fields are missing or invalid. Not Found Errors : Returns 404 Not Found if the specified task does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. post : Functionality : Accepts comment data ( task_id , content ). Validates that the task exists. Associates the comment with the specified task and user. Saves the comment to the database. Authorization : Typically requires the user to be authenticated to post comments. get , put , delete : Potential Uses : Fetching comments for a task, editing comments, or deleting inappropriate comments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Accepts comment data ( task_id , content ). Validates that the task exists. Associates the comment with the specified task and user. Saves the comment to the database. Authorization : Typically requires the user to be authenticated to post comments. Accepts comment data ( task_id , content ). Validates that the task exists. Associates the comment with the specified task and user. Saves the comment to the database. Typically requires the user to be authenticated to post comments. Potential Uses : Fetching comments for a task, editing comments, or deleting inappropriate comments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Ensures that only authenticated users can post comments. Database Models : Interacts with the Comment and Task models to manage comment data. Incoming Request : User sends a POST request with comment data. Processing : The server validates the task, associates the comment with the user and task, and saves it to the database. Response : Returns the newly created comment details. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Validation Errors : Returns 400 Bad Request if required fields are missing or invalid. Not Found Errors : Returns 404 Not Found if the specified task does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Prompt: ```\\nImplement the following function for the file 'app/api/comments/route.js': post Ensure that the implementation follows Next.js 14 best practices, includes authentication checks, validates and sanitizes input data, associates comments with the correct tasks and users, saves them in MongoDB, and handles errors appropriately. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315607.0228508, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '18. comments/[taskId]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/comments/[taskId]/route.js ``` Purpose: Manages operations on comments associated with a specific task, identified by taskId . This includes fetching all comments for a task, updating specific comments, and deleting them. Key Components/Functions: GET : Retrieves all comments related to a specific task. PUT : Updates a particular comment. DELETE : Deletes a specific comment. Implementation Guidelines: Purpose : Facilitate detailed management of comments on individual tasks, enabling users to view, modify, or remove their comments. Key Functions : get : Functionality : Fetches all comments associated with the taskId . Optionally, supports pagination or filtering. Authorization : May allow any authenticated user to view comments. put : Functionality : Allows users to edit their own comments. Validates input data and ensures that users can only modify their comments. Authorization : Ensures that only the comment owner or admins can update comments. delete : Functionality : Enables users to delete their own comments or allows admins to remove any comment. Authorization : Restricted to the comment owner or admin users. Integration Points : Authentication Middleware : Validates user authentication and ownership of comments. Database Models : Interacts with the Comment and Task models to manage comment data. Data Flow : Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific taskId . Processing : The server authenticates the user, verifies permissions, and performs the requested action on the comments. Response : Returns the list of comments, updated comment details, or deletion confirmations. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permissions. Not Found Errors : Returns 404 Not Found if the task or comment does not exist. Validation Errors : Returns 400 Bad Request for invalid input data. Server Errors : Returns 500 Internal Server Error for unexpected issues. get : Functionality : Fetches all comments associated with the taskId . Optionally, supports pagination or filtering. Authorization : May allow any authenticated user to view comments. put : Functionality : Allows users to edit their own comments. Validates input data and ensures that users can only modify their comments. Authorization : Ensures that only the comment owner or admins can update comments. delete : Functionality : Enables users to delete their own comments or allows admins to remove any comment. Authorization : Restricted to the comment owner or admin users. Functionality : Fetches all comments associated with the taskId . Optionally, supports pagination or filtering. Authorization : May allow any authenticated user to view comments. Fetches all comments associated with the taskId . Optionally, supports pagination or filtering. May allow any authenticated user to view comments. Functionality : Allows users to edit their own comments. Validates input data and ensures that users can only modify their comments. Authorization : Ensures that only the comment owner or admins can update comments. Allows users to edit their own comments. Validates input data and ensures that users can only modify their comments. Ensures that only the comment owner or admins can update comments. Functionality : Enables users to delete their own comments or allows admins to remove any comment. Authorization : Restricted to the comment owner or admin users. Enables users to delete their own comments or allows admins to remove any comment. Restricted to the comment owner or admin users. Authentication Middleware : Validates user authentication and ownership of comments. Database Models : Interacts with the Comment and Task models to manage comment data. Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific taskId . Processing : The server authenticates the user, verifies permissions, and performs the requested action on the comments. Response : Returns the list of comments, updated comment details, or deletion confirmations. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permissions. Not Found Errors : Returns 404 Not Found if the task or comment does not exist. Validation Errors : Returns 400 Bad Request for invalid input data. Server Errors : Returns 500 Internal Server Error for unexpected issues. Implementation Guidelines: Dynamic Routing : Utilize Next.js dynamic route segments ( [taskId] ) to target specific tasks. Input Validation : Ensure that comment content is valid and adheres to length and format requirements. Data Sanitization : Sanitize all inputs to prevent injection attacks and maintain data integrity. Role-Based Access Control : Enforce that users can only modify or delete their own comments unless they have admin privileges. Response Structure : Provide clear and concise responses, especially when dealing with multiple comments. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/comments/[taskId]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, securely handles data retrieval and manipulation for comments associated with specific tasks, manages errors effectively, and ensures data integrity. ```\", 'level': 3, 'parent': 'Continuing with Other API Routes', 'children': [], 'timestamp': 1728315609.1613271, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '19. projects/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/projects/route.js ``` Purpose: Manages project-related operations, allowing users and administrators to create, retrieve, update, and delete projects. Projects can encompass multiple tasks and are central to organizing work within the system. Key Components/Functions: GET : Retrieves a list of all projects. POST : Creates a new project. PUT , DELETE : Potentially handles bulk updates or deletions if necessary. Implementation Guidelines: Purpose : Facilitate comprehensive project management, enabling the creation and oversight of projects that contain multiple tasks. Key Functions : get : Functionality : Fetches all projects from the database. May include associated tasks and user details if necessary. Authorization : Accessible to authenticated users, with potential role-based restrictions (e.g., only admins can view all projects). post : Functionality : Creates a new project with details like title, description, and status. Associates the project with the user who created it ( created_by ). Authorization : Ensures that only authenticated users can create projects. put , delete : Potential Uses : Handle bulk updates or deletions of projects. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the user is authenticated before allowing project creation or retrieval. Database Models : Interacts with the Project and Task models to manage project data. Data Flow : Incoming Request : User sends a GET or POST request to manage projects. Processing : The server authenticates the user. For GET : Retrieves projects from the database. For POST : Validates input data and creates a new project. Response : Returns the list of projects or the newly created project details in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks the necessary permissions. Validation Errors : Returns 400 Bad Request for invalid input data during project creation. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all required project details (e.g., title, description) are provided and valid during creation. get : Functionality : Fetches all projects from the database. May include associated tasks and user details if necessary. Authorization : Accessible to authenticated users, with potential role-based restrictions (e.g., only admins can view all projects). post : Functionality : Creates a new project with details like title, description, and status. Associates the project with the user who created it ( created_by ). Authorization : Ensures that only authenticated users can create projects. put , delete : Potential Uses : Handle bulk updates or deletions of projects. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Fetches all projects from the database. May include associated tasks and user details if necessary. Authorization : Accessible to authenticated users, with potential role-based restrictions (e.g., only admins can view all projects). Fetches all projects from the database. May include associated tasks and user details if necessary. Accessible to authenticated users, with potential role-based restrictions (e.g., only admins can view all projects). Functionality : Creates a new project with details like title, description, and status. Associates the project with the user who created it ( created_by ). Authorization : Ensures that only authenticated users can create projects. Creates a new project with details like title, description, and status. Associates the project with the user who created it ( created_by ). Ensures that only authenticated users can create projects. Potential Uses : Handle bulk updates or deletions of projects. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Handle bulk updates or deletions of projects. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the user is authenticated before allowing project creation or retrieval. Database Models : Interacts with the Project and Task models to manage project data. Verifies that the user is authenticated before allowing project creation or retrieval. Interacts with the Project and Task models to manage project data. Incoming Request : User sends a GET or POST request to manage projects. Processing : The server authenticates the user. For GET : Retrieves projects from the database. For POST : Validates input data and creates a new project. Response : Returns the list of projects or the newly created project details in JSON format. User sends a GET or POST request to manage projects. The server authenticates the user. For GET : Retrieves projects from the database. For POST : Validates input data and creates a new project. Returns the list of projects or the newly created project details in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks the necessary permissions. Validation Errors : Returns 400 Bad Request for invalid input data during project creation. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks the necessary permissions. Returns 400 Bad Request for invalid input data during project creation. Returns 500 Internal Server Error for unexpected issues. Ensure that all required project details (e.g., title, description) are provided and valid during creation. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/projects/route.js': get post put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Project and Task models in MongoDB, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315611.265849, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '20. projects/[id]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/projects/[id]/route.js ``` Purpose: Handles operations for a specific project identified by its unique ID. This includes retrieving, updating, and deleting individual projects. Key Components/Functions: GET : Retrieves details of a specific project. PUT : Updates the details of a specific project. DELETE : Deletes a specific project. Implementation Guidelines: Purpose : Enable detailed management of individual projects, allowing for retrieval, modification, and removal based on the project's unique identifier. Key Functions : get : Functionality : Fetches the project identified by the [id] parameter. May include associated tasks and user details. Authorization : Accessible to authenticated users. Admins may have broader access compared to regular users (e.g., viewing all projects vs. only their own). put : Functionality : Updates project details such as title, description, status, etc. Authorization : Ensures that only the project owner or admins can update the project. delete : Functionality : Removes the project from the database. May also handle cascading deletions of associated tasks. Authorization : Ensures that only the project owner or admins can delete the project. Integration Points : Authentication Middleware : Verifies user authentication and authorization based on roles. Database Models : Interacts with the Project and Task models to manage specific project data. Data Flow : Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific project via its [id] . Processing : The server authenticates and authorizes the user. For GET : Retrieves the specific project from the database. For PUT : Validates input data and updates the project. For DELETE : Removes the project and handles any necessary cleanup. Response : Returns the project details, confirmation of update, or deletion status in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the project with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during project updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all updated project details are valid and conform to expected formats and constraints. get : Functionality : Fetches the project identified by the [id] parameter. May include associated tasks and user details. Authorization : Accessible to authenticated users. Admins may have broader access compared to regular users (e.g., viewing all projects vs. only their own). put : Functionality : Updates project details such as title, description, status, etc. Authorization : Ensures that only the project owner or admins can update the project. delete : Functionality : Removes the project from the database. May also handle cascading deletions of associated tasks. Authorization : Ensures that only the project owner or admins can delete the project. Functionality : Fetches the project identified by the [id] parameter. May include associated tasks and user details. Authorization : Accessible to authenticated users. Admins may have broader access compared to regular users (e.g., viewing all projects vs. only their own). Fetches the project identified by the [id] parameter. May include associated tasks and user details. Accessible to authenticated users. Admins may have broader access compared to regular users (e.g., viewing all projects vs. only their own). Functionality : Updates project details such as title, description, status, etc. Authorization : Ensures that only the project owner or admins can update the project. Updates project details such as title, description, status, etc. Ensures that only the project owner or admins can update the project. Functionality : Removes the project from the database. May also handle cascading deletions of associated tasks. Authorization : Ensures that only the project owner or admins can delete the project. Removes the project from the database. May also handle cascading deletions of associated tasks. Ensures that only the project owner or admins can delete the project. Authentication Middleware : Verifies user authentication and authorization based on roles. Database Models : Interacts with the Project and Task models to manage specific project data. Verifies user authentication and authorization based on roles. Interacts with the Project and Task models to manage specific project data. Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific project via its [id] . Processing : The server authenticates and authorizes the user. For GET : Retrieves the specific project from the database. For PUT : Validates input data and updates the project. For DELETE : Removes the project and handles any necessary cleanup. Response : Returns the project details, confirmation of update, or deletion status in JSON format. User sends a GET , PUT , or DELETE request targeting a specific project via its [id] . The server authenticates and authorizes the user. For GET : Retrieves the specific project from the database. For PUT : Validates input data and updates the project. For DELETE : Removes the project and handles any necessary cleanup. Returns the project details, confirmation of update, or deletion status in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the project with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during project updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to perform the action. Returns 404 Not Found if the project with the specified [id] does not exist. Returns 400 Bad Request for invalid input data during project updates. Returns 500 Internal Server Error for unexpected issues. Ensure that all updated project details are valid and conform to expected formats and constraints. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/projects/[id]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Project and Task models in MongoDB, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315613.3394232, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '21. submissions/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/submissions/route.js ``` Purpose: Manages submission-related operations, allowing users to submit their work in response to tasks and enabling administrators to review and manage these submissions. Key Components/Functions: GET : Retrieves a list of all submissions for the authenticated user. POST : Creates a new submission for a specific task. PUT , DELETE : Potentially handles updates to submissions or deletion of inappropriate ones. Implementation Guidelines: Purpose : Facilitate the creation, retrieval, and management of user submissions in response to assigned tasks. Key Functions : get : Functionality : Retrieves all submissions associated with the authenticated user. May include details such as submission status, feedback, and related task information. Authorization : Accessible only to authenticated users. Admins may have the ability to view all submissions across users. post : Functionality : Creates a new submission linked to a specific task. Handles file uploads if the submission includes files. Sets initial submission status (e.g., 'pending'). Authorization : Ensures that only users assigned to the task can submit work. put , delete : Potential Uses : Update submission status or feedback. Delete inappropriate or erroneous submissions. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the user is authenticated before allowing submission actions. Database Models : Interacts with the Submission , Task , and User models to manage submission data. Data Flow : Incoming Request : User sends a GET request to retrieve submissions or a POST request to create a new submission. Processing : For GET : Retrieves submissions from the database. For POST : Validates input data, handles file uploads, and creates a new submission record. Response : Returns the list of submissions or confirmation of the new submission in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user is not assigned to the task they're trying to submit for. Validation Errors : Returns 400 Bad Request for invalid input data during submission creation. Not Found Errors : Returns 404 Not Found if the related task does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all required submission details (e.g., task ID, submission content) are provided and valid. Validate file types and sizes if handling file uploads. get : Functionality : Retrieves all submissions associated with the authenticated user. May include details such as submission status, feedback, and related task information. Authorization : Accessible only to authenticated users. Admins may have the ability to view all submissions across users. post : Functionality : Creates a new submission linked to a specific task. Handles file uploads if the submission includes files. Sets initial submission status (e.g., 'pending'). Authorization : Ensures that only users assigned to the task can submit work. put , delete : Potential Uses : Update submission status or feedback. Delete inappropriate or erroneous submissions. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Retrieves all submissions associated with the authenticated user. May include details such as submission status, feedback, and related task information. Authorization : Accessible only to authenticated users. Admins may have the ability to view all submissions across users. Retrieves all submissions associated with the authenticated user. May include details such as submission status, feedback, and related task information. Accessible only to authenticated users. Admins may have the ability to view all submissions across users. Functionality : Creates a new submission linked to a specific task. Handles file uploads if the submission includes files. Sets initial submission status (e.g., 'pending'). Authorization : Ensures that only users assigned to the task can submit work. Creates a new submission linked to a specific task. Handles file uploads if the submission includes files. Sets initial submission status (e.g., 'pending'). Ensures that only users assigned to the task can submit work. Potential Uses : Update submission status or feedback. Delete inappropriate or erroneous submissions. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Update submission status or feedback. Delete inappropriate or erroneous submissions. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the user is authenticated before allowing submission actions. Database Models : Interacts with the Submission , Task , and User models to manage submission data. Verifies that the user is authenticated before allowing submission actions. Interacts with the Submission , Task , and User models to manage submission data. Incoming Request : User sends a GET request to retrieve submissions or a POST request to create a new submission. Processing : For GET : Retrieves submissions from the database. For POST : Validates input data, handles file uploads, and creates a new submission record. Response : Returns the list of submissions or confirmation of the new submission in JSON format. User sends a GET request to retrieve submissions or a POST request to create a new submission. For GET : Retrieves submissions from the database. For POST : Validates input data, handles file uploads, and creates a new submission record. Returns the list of submissions or confirmation of the new submission in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user is not assigned to the task they're trying to submit for. Validation Errors : Returns 400 Bad Request for invalid input data during submission creation. Not Found Errors : Returns 404 Not Found if the related task does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user is not assigned to the task they're trying to submit for. Returns 400 Bad Request for invalid input data during submission creation. Returns 404 Not Found if the related task does not exist. Returns 500 Internal Server Error for unexpected issues. Ensure that all required submission details (e.g., task ID, submission content) are provided and valid. Validate file types and sizes if handling file uploads. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/submissions/route.js': get post put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Submission, Task, and User models in MongoDB, handles file uploads securely, validates input data, and manages errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315615.419544, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '22. tasks/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/tasks/route.js ``` Purpose: Manages task-related operations, enabling users to create, retrieve, update, and delete tasks. Tasks are the fundamental units of work assigned to users within projects. Key Components/Functions: GET : Retrieves a list of all tasks, potentially filtered by user or project. POST : Creates a new task. PUT , DELETE : Potentially handles bulk updates or deletions if necessary. Implementation Guidelines: Purpose : Facilitate comprehensive task management, allowing for the creation and oversight of tasks within projects. Key Functions : get : Functionality : Fetches all tasks from the database. May include filtering options (e.g., by user, project, status). Authorization : Accessible to authenticated users. Admins may have the ability to view all tasks, while regular users may only view their own. post : Functionality : Creates a new task with details like prompt, difficulty, due date, required skills, etc. Associates the task with a specific project and user. Authorization : Ensures that only authorized users (e.g., project owners or admins) can create tasks. put , delete : Potential Uses : Handle bulk updates or deletions of tasks. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the user is authenticated before allowing task actions. Database Models : Interacts with the Task , Project , and User models to manage task data. Data Flow : Incoming Request : User sends a GET request to retrieve tasks or a POST request to create a new task. Processing : For GET : Retrieves tasks from the database, possibly applying filters. For POST : Validates input data and creates a new task record. Response : Returns the list of tasks or confirmation of the new task in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Validation Errors : Returns 400 Bad Request for invalid input data during task creation. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all required task details (e.g., prompt, difficulty) are provided and valid during creation. get : Functionality : Fetches all tasks from the database. May include filtering options (e.g., by user, project, status). Authorization : Accessible to authenticated users. Admins may have the ability to view all tasks, while regular users may only view their own. post : Functionality : Creates a new task with details like prompt, difficulty, due date, required skills, etc. Associates the task with a specific project and user. Authorization : Ensures that only authorized users (e.g., project owners or admins) can create tasks. put , delete : Potential Uses : Handle bulk updates or deletions of tasks. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Fetches all tasks from the database. May include filtering options (e.g., by user, project, status). Authorization : Accessible to authenticated users. Admins may have the ability to view all tasks, while regular users may only view their own. Fetches all tasks from the database. May include filtering options (e.g., by user, project, status). Accessible to authenticated users. Admins may have the ability to view all tasks, while regular users may only view their own. Functionality : Creates a new task with details like prompt, difficulty, due date, required skills, etc. Associates the task with a specific project and user. Authorization : Ensures that only authorized users (e.g., project owners or admins) can create tasks. Creates a new task with details like prompt, difficulty, due date, required skills, etc. Associates the task with a specific project and user. Ensures that only authorized users (e.g., project owners or admins) can create tasks. Potential Uses : Handle bulk updates or deletions of tasks. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Handle bulk updates or deletions of tasks. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the user is authenticated before allowing task actions. Database Models : Interacts with the Task , Project , and User models to manage task data. Verifies that the user is authenticated before allowing task actions. Interacts with the Task , Project , and User models to manage task data. Incoming Request : User sends a GET request to retrieve tasks or a POST request to create a new task. Processing : For GET : Retrieves tasks from the database, possibly applying filters. For POST : Validates input data and creates a new task record. Response : Returns the list of tasks or confirmation of the new task in JSON format. User sends a GET request to retrieve tasks or a POST request to create a new task. For GET : Retrieves tasks from the database, possibly applying filters. For POST : Validates input data and creates a new task record. Returns the list of tasks or confirmation of the new task in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Validation Errors : Returns 400 Bad Request for invalid input data during task creation. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to perform the action. Returns 400 Bad Request for invalid input data during task creation. Returns 500 Internal Server Error for unexpected issues. Ensure that all required task details (e.g., prompt, difficulty) are provided and valid during creation. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/tasks/route.js': get post put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Task, Project, and User models in MongoDB, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315617.510493, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '23. tasks/assign/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/tasks/assign/route.js ``` Purpose: Handles the assignment of tasks to users, enabling administrators to assign specific tasks to users within projects. Key Components/Functions: POST : Assigns a new task to a user. PUT , DELETE : Potentially handles reassignments or cancellations of task assignments. Implementation Guidelines: Purpose : Streamline the process of task allocation, ensuring that tasks are appropriately assigned to users based on project requirements and user capabilities. Key Functions : post : Functionality : Assigns a new task to a specified user. Handles details such as task prompt, difficulty, due date, and required skills. Associates the task with a specific project. Authorization : Ensures that only authorized users (e.g., admins or project owners) can assign tasks. put , delete : Potential Uses : Reassign tasks to different users. Cancel or remove task assignments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the user is authenticated and authorized to assign tasks. Database Models : Interacts with the Task , Project , and User models to manage task assignments. Data Flow : Incoming Request : Admin sends a POST request to assign a task to a user. Processing : Validates input data. Creates a new task record associated with the specified user and project. Response : Returns the details of the newly assigned task in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to assign tasks. Validation Errors : Returns 400 Bad Request for invalid input data during task assignment. Not Found Errors : Returns 404 Not Found if the specified user or project does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all required task assignment details (e.g., user ID, project ID, prompt) are provided and valid. post : Functionality : Assigns a new task to a specified user. Handles details such as task prompt, difficulty, due date, and required skills. Associates the task with a specific project. Authorization : Ensures that only authorized users (e.g., admins or project owners) can assign tasks. put , delete : Potential Uses : Reassign tasks to different users. Cancel or remove task assignments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Assigns a new task to a specified user. Handles details such as task prompt, difficulty, due date, and required skills. Associates the task with a specific project. Authorization : Ensures that only authorized users (e.g., admins or project owners) can assign tasks. Assigns a new task to a specified user. Handles details such as task prompt, difficulty, due date, and required skills. Associates the task with a specific project. Ensures that only authorized users (e.g., admins or project owners) can assign tasks. Potential Uses : Reassign tasks to different users. Cancel or remove task assignments. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Reassign tasks to different users. Cancel or remove task assignments. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the user is authenticated and authorized to assign tasks. Database Models : Interacts with the Task , Project , and User models to manage task assignments. Verifies that the user is authenticated and authorized to assign tasks. Interacts with the Task , Project , and User models to manage task assignments. Incoming Request : Admin sends a POST request to assign a task to a user. Processing : Validates input data. Creates a new task record associated with the specified user and project. Response : Returns the details of the newly assigned task in JSON format. Admin sends a POST request to assign a task to a user. Validates input data. Creates a new task record associated with the specified user and project. Returns the details of the newly assigned task in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to assign tasks. Validation Errors : Returns 400 Bad Request for invalid input data during task assignment. Not Found Errors : Returns 404 Not Found if the specified user or project does not exist. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to assign tasks. Returns 400 Bad Request for invalid input data during task assignment. Returns 404 Not Found if the specified user or project does not exist. Returns 500 Internal Server Error for unexpected issues. Ensure that all required task assignment details (e.g., user ID, project ID, prompt) are provided and valid. Implementation Guidelines: Input Validation : Validate that the user_id and project_id exist in the database. Ensure that the prompt , difficulty , and other task details meet the required formats and constraints. Security Considerations : Prevent assignment of tasks to unauthorized users. Ensure that sensitive information is not exposed in the response. Validate that the user_id and project_id exist in the database. Ensure that the prompt , difficulty , and other task details meet the required formats and constraints. Prevent assignment of tasks to unauthorized users. Ensure that sensitive information is not exposed in the response. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/tasks/assign/route.js': post put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Task, Project, and User models in MongoDB, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315619.6007488, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '24. tasks/[id]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/tasks/[id]/route.js ``` Purpose: Handles operations for a specific task identified by its unique ID. This includes retrieving, updating, and deleting individual tasks. Key Components/Functions: GET : Retrieves details of a specific task. PUT : Updates the details or status of a specific task. DELETE : Deletes a specific task. Implementation Guidelines: Purpose : Enable detailed management of individual tasks, allowing for retrieval, modification, and removal based on the task's unique identifier. Key Functions : get : Functionality : Fetches the task identified by the [id] parameter. May include associated project and user details. Authorization : Accessible to authenticated users. Admins or assigned users may have different levels of access. put : Functionality : Updates task details such as prompt, difficulty, due date, status, etc. Can also handle status updates like marking a task as completed or in progress. Authorization : Ensures that only the task owner, assigned user, or admins can update the task. delete : Functionality : Removes the task from the database. May also handle cascading deletions of related submissions or comments. Authorization : Ensures that only the task owner or admins can delete the task. Integration Points : Authentication Middleware : Verifies that the user is authenticated and authorized to perform actions on the task. Database Models : Interacts with the Task , Project , Submission , and Comment models to manage task data. Data Flow : Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific task via its [id] . Processing : The server authenticates and authorizes the user. For GET : Retrieves the specific task from the database. For PUT : Validates input data and updates the task. For DELETE : Removes the task and handles any necessary cleanup. Response : Returns the task details, confirmation of update, or deletion status in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the task with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during task updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all updated task details are valid and conform to expected formats and constraints. get : Functionality : Fetches the task identified by the [id] parameter. May include associated project and user details. Authorization : Accessible to authenticated users. Admins or assigned users may have different levels of access. put : Functionality : Updates task details such as prompt, difficulty, due date, status, etc. Can also handle status updates like marking a task as completed or in progress. Authorization : Ensures that only the task owner, assigned user, or admins can update the task. delete : Functionality : Removes the task from the database. May also handle cascading deletions of related submissions or comments. Authorization : Ensures that only the task owner or admins can delete the task. Functionality : Fetches the task identified by the [id] parameter. May include associated project and user details. Authorization : Accessible to authenticated users. Admins or assigned users may have different levels of access. Fetches the task identified by the [id] parameter. May include associated project and user details. Accessible to authenticated users. Admins or assigned users may have different levels of access. Functionality : Updates task details such as prompt, difficulty, due date, status, etc. Can also handle status updates like marking a task as completed or in progress. Authorization : Ensures that only the task owner, assigned user, or admins can update the task. Updates task details such as prompt, difficulty, due date, status, etc. Can also handle status updates like marking a task as completed or in progress. Ensures that only the task owner, assigned user, or admins can update the task. Functionality : Removes the task from the database. May also handle cascading deletions of related submissions or comments. Authorization : Ensures that only the task owner or admins can delete the task. Removes the task from the database. May also handle cascading deletions of related submissions or comments. Ensures that only the task owner or admins can delete the task. Authentication Middleware : Verifies that the user is authenticated and authorized to perform actions on the task. Database Models : Interacts with the Task , Project , Submission , and Comment models to manage task data. Verifies that the user is authenticated and authorized to perform actions on the task. Interacts with the Task , Project , Submission , and Comment models to manage task data. Incoming Request : User sends a GET , PUT , or DELETE request targeting a specific task via its [id] . Processing : The server authenticates and authorizes the user. For GET : Retrieves the specific task from the database. For PUT : Validates input data and updates the task. For DELETE : Removes the task and handles any necessary cleanup. Response : Returns the task details, confirmation of update, or deletion status in JSON format. User sends a GET , PUT , or DELETE request targeting a specific task via its [id] . The server authenticates and authorizes the user. For GET : Retrieves the specific task from the database. For PUT : Validates input data and updates the task. For DELETE : Removes the task and handles any necessary cleanup. Returns the task details, confirmation of update, or deletion status in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the task with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during task updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to perform the action. Returns 404 Not Found if the task with the specified [id] does not exist. Returns 400 Bad Request for invalid input data during task updates. Returns 500 Internal Server Error for unexpected issues. Ensure that all updated task details are valid and conform to expected formats and constraints. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/tasks/[id]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, interacts with the Task, Project, Submission, and Comment models in MongoDB, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315621.6907604, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '25. upload/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/upload/route.js ``` Purpose: Handles file upload operations, allowing users to upload files associated with tasks or projects. This can include uploading documents, images, code files, etc. Key Components/Functions: POST : Handles the uploading of files. GET , PUT , DELETE : Potentially handle retrieval, updating, or deletion of uploaded files. Implementation Guidelines: Purpose : Enable users to upload and manage files related to their tasks or projects, ensuring secure and efficient file handling. Key Functions : post : Functionality : Receives file uploads from users. Stores files in a designated directory or cloud storage (e.g., AWS S3). Associates uploaded files with specific tasks or projects in the database. Validates file types and sizes to ensure security and compliance. Authorization : Ensures that only authorized users can upload files to specific tasks or projects. get , put , delete : Potential Uses : Retrieve uploaded files. Update file metadata. Delete uploaded files if necessary. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the user is authenticated before allowing file uploads. Database Models : Interacts with the Task , Project , and potentially a File model to manage file associations. File Storage Service : Utilizes services like multer for handling multipart/form-data or integrates with cloud storage APIs. Data Flow : Incoming Request : User sends a POST request with a file to upload. Processing : The server authenticates the user. Validates the file type and size. Stores the file in the designated storage. Updates the relevant task or project record with the file's URL or reference. Response : Returns confirmation of the successful upload along with file details in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to upload files to the specified task or project. Validation Errors : Returns 400 Bad Request for invalid file types or oversized files. Server Errors : Returns 500 Internal Server Error for unexpected issues during file handling. Security Considerations : File Validation : Restrict allowed file types to prevent malicious uploads. Limit file sizes to manage storage and performance. Storage Security : Store files in secure locations with appropriate access controls. Sanitize file names to prevent directory traversal attacks. Scalability : Consider integrating with cloud storage solutions for scalable and reliable file handling. post : Functionality : Receives file uploads from users. Stores files in a designated directory or cloud storage (e.g., AWS S3). Associates uploaded files with specific tasks or projects in the database. Validates file types and sizes to ensure security and compliance. Authorization : Ensures that only authorized users can upload files to specific tasks or projects. get , put , delete : Potential Uses : Retrieve uploaded files. Update file metadata. Delete uploaded files if necessary. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Receives file uploads from users. Stores files in a designated directory or cloud storage (e.g., AWS S3). Associates uploaded files with specific tasks or projects in the database. Validates file types and sizes to ensure security and compliance. Authorization : Ensures that only authorized users can upload files to specific tasks or projects. Receives file uploads from users. Stores files in a designated directory or cloud storage (e.g., AWS S3). Associates uploaded files with specific tasks or projects in the database. Validates file types and sizes to ensure security and compliance. Ensures that only authorized users can upload files to specific tasks or projects. Potential Uses : Retrieve uploaded files. Update file metadata. Delete uploaded files if necessary. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Retrieve uploaded files. Update file metadata. Delete uploaded files if necessary. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the user is authenticated before allowing file uploads. Database Models : Interacts with the Task , Project , and potentially a File model to manage file associations. File Storage Service : Utilizes services like multer for handling multipart/form-data or integrates with cloud storage APIs. Verifies that the user is authenticated before allowing file uploads. Interacts with the Task , Project , and potentially a File model to manage file associations. Utilizes services like multer for handling multipart/form-data or integrates with cloud storage APIs. Incoming Request : User sends a POST request with a file to upload. Processing : The server authenticates the user. Validates the file type and size. Stores the file in the designated storage. Updates the relevant task or project record with the file's URL or reference. Response : Returns confirmation of the successful upload along with file details in JSON format. User sends a POST request with a file to upload. The server authenticates the user. Validates the file type and size. Stores the file in the designated storage. Updates the relevant task or project record with the file's URL or reference. Returns confirmation of the successful upload along with file details in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to upload files to the specified task or project. Validation Errors : Returns 400 Bad Request for invalid file types or oversized files. Server Errors : Returns 500 Internal Server Error for unexpected issues during file handling. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to upload files to the specified task or project. Returns 400 Bad Request for invalid file types or oversized files. Returns 500 Internal Server Error for unexpected issues during file handling. File Validation : Restrict allowed file types to prevent malicious uploads. Limit file sizes to manage storage and performance. Storage Security : Store files in secure locations with appropriate access controls. Sanitize file names to prevent directory traversal attacks. Restrict allowed file types to prevent malicious uploads. Limit file sizes to manage storage and performance. Store files in secure locations with appropriate access controls. Sanitize file names to prevent directory traversal attacks. Consider integrating with cloud storage solutions for scalable and reliable file handling. Implementation Guidelines: File Handling : Utilize multer or similar middleware for handling file uploads in Next.js API routes. Configure storage destinations and file naming conventions. Associations : Update relevant database records (e.g., Task , Project ) with references to the uploaded files. Error Messaging : Provide clear and actionable error messages to users in case of upload failures. Utilize multer or similar middleware for handling file uploads in Next.js API routes. Configure storage destinations and file naming conventions. Update relevant database records (e.g., Task , Project ) with references to the uploaded files. Provide clear and actionable error messages to users in case of upload failures. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/upload/route.js': post get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, handles file uploads securely using appropriate middleware or services, interacts with the Task and Project models in MongoDB to associate files, validates file types and sizes, and manages errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315623.7839735, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '26. users/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/route.js ``` Purpose: Manages user-related operations, including fetching user information, updating user details, and deleting users. This route is primarily intended for administrative tasks, allowing admins to perform bulk user management actions. Key Components/Functions: GET : Retrieves a list of all users with task summaries (admin only). POST , PUT , DELETE : Potentially handles user-related actions, though specific functionalities may be defined in sub-routes. Implementation Guidelines: Purpose : Facilitate comprehensive user management, enabling administrators to retrieve, update, and delete user data as required. Key Functions : get : Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Accessible only to users with the 'admin' role. post , put , delete : Potential Uses : Handle bulk user creation, updates, or deletions. Implement user activation/deactivation workflows. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Integration Points : Authentication Middleware : Verifies that the requester has admin privileges. Database Models : Interacts with the User and Task models to manage user data and their associated tasks. Data Flow : Incoming Request : Admin sends a GET request to retrieve user data or a POST request for bulk actions. Processing : For GET : Retrieves users and aggregates their task summaries. For POST : Processes bulk user actions as defined. Response : Returns the list of users with task summaries or confirmation of bulk actions in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that any data provided for user creation or updates is valid and conforms to required formats. get : Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Accessible only to users with the 'admin' role. post , put , delete : Potential Uses : Handle bulk user creation, updates, or deletions. Implement user activation/deactivation workflows. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Functionality : Fetches all users from the database along with summaries of their associated tasks. Authorization : Accessible only to users with the 'admin' role. Fetches all users from the database along with summaries of their associated tasks. Accessible only to users with the 'admin' role. Potential Uses : Handle bulk user creation, updates, or deletions. Implement user activation/deactivation workflows. Current State : If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Handle bulk user creation, updates, or deletions. Implement user activation/deactivation workflows. If not immediately required, these can remain unimplemented or return a 405 Method Not Allowed status. Authentication Middleware : Verifies that the requester has admin privileges. Database Models : Interacts with the User and Task models to manage user data and their associated tasks. Verifies that the requester has admin privileges. Interacts with the User and Task models to manage user data and their associated tasks. Incoming Request : Admin sends a GET request to retrieve user data or a POST request for bulk actions. Processing : For GET : Retrieves users and aggregates their task summaries. For POST : Processes bulk user actions as defined. Response : Returns the list of users with task summaries or confirmation of bulk actions in JSON format. Admin sends a GET request to retrieve user data or a POST request for bulk actions. For GET : Retrieves users and aggregates their task summaries. For POST : Processes bulk user actions as defined. Returns the list of users with task summaries or confirmation of bulk actions in JSON format. Authentication Errors : Returns 401 Unauthorized if the token is missing or invalid. Authorization Errors : Returns 403 Forbidden if the user lacks admin privileges. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the token is missing or invalid. Returns 403 Forbidden if the user lacks admin privileges. Returns 500 Internal Server Error for unexpected issues. Ensure that any data provided for user creation or updates is valid and conforms to required formats. Implementation Guidelines: Data Aggregation : Use MongoDB aggregation pipelines to compile task summaries for each user. Security : Ensure that sensitive information (e.g., password_hash ) is excluded from responses. Pagination and Filtering : Implement pagination and filtering options for handling large user datasets efficiently. Use MongoDB aggregation pipelines to compile task summaries for each user. Ensure that sensitive information (e.g., password_hash ) is excluded from responses. Implement pagination and filtering options for handling large user datasets efficiently. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/route.js': get post put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks for admin users, retrieves user data along with their task summaries from MongoDB, excludes sensitive information, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315625.8781586, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '27. users/login/route.js' to API. Metadata: {'content': 'File Path in Next.js: ```bash\\napp/api/users/login/route.js ``` Purpose: Handles user authentication by verifying credentials and issuing JWT tokens. This endpoint is essential for allowing users to securely log into the system. Key Components/Functions: POST : Authenticates users by validating their email and password, then generates and returns a JWT token upon successful authentication. Implementation Guidelines: Purpose : Enable users to securely log into the application by validating their credentials and providing authentication tokens for subsequent requests. Key Functions : post : Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Integration Points : Database Models : Interacts with the User model to retrieve and verify user credentials. Authentication Utilities : Uses utilities for hashing ( bcrypt ) and token generation ( jsonwebtoken ). Data Flow : Incoming Request : User sends a POST request with their email and password . Processing : The server retrieves the user by email. Compares the provided password with the stored password_hash . Generates a JWT token upon successful authentication. Response : Returns the JWT token and sanitized user information in JSON format. Error Handling : Authentication Errors : Returns 400 Bad Request if the user is not found or the password is incorrect. Validation Errors : Returns 400 Bad Request for missing or invalid input data. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that both email and password are provided and conform to expected formats. Security Considerations : Implement rate limiting to prevent brute-force attacks. Use secure HTTP headers and HTTPS to protect data in transit. post : Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Functionality : Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Security : Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Receives user login credentials ( email and password ). Validates the credentials against the stored User data in MongoDB. If valid, generates a JWT token containing user information. Returns the token and user details (excluding sensitive information like password_hash ). Utilizes bcrypt to compare hashed passwords. Ensures JWT tokens are signed with a secure secret key and have appropriate expiration times. Database Models : Interacts with the User model to retrieve and verify user credentials. Authentication Utilities : Uses utilities for hashing ( bcrypt ) and token generation ( jsonwebtoken ). Interacts with the User model to retrieve and verify user credentials. Uses utilities for hashing ( bcrypt ) and token generation ( jsonwebtoken ). Incoming Request : User sends a POST request with their email and password . Processing : The server retrieves the user by email. Compares the provided password with the stored password_hash . Generates a JWT token upon successful authentication. Response : Returns the JWT token and sanitized user information in JSON format. User sends a POST request with their email and password . The server retrieves the user by email. Compares the provided password with the stored password_hash . Generates a JWT token upon successful authentication. Returns the JWT token and sanitized user information in JSON format. Authentication Errors : Returns 400 Bad Request if the user is not found or the password is incorrect. Validation Errors : Returns 400 Bad Request for missing or invalid input data. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 400 Bad Request if the user is not found or the password is incorrect. Returns 400 Bad Request for missing or invalid input data. Returns 500 Internal Server Error for unexpected issues. Ensure that both email and password are provided and conform to expected formats. Implement rate limiting to prevent brute-force attacks. Use secure HTTP headers and HTTPS to protect data in transit. Implementation Guidelines: Password Handling : Use bcrypt for secure password hashing and comparison. Token Generation : Use jsonwebtoken to create JWT tokens. Store the secret key securely, preferably using environment variables. Response Sanitization : Exclude sensitive fields like password_hash from the response. Error Messaging : Provide generic error messages to prevent information leakage (e.g., \"Invalid credentials\" instead of specifying whether the email or password was incorrect). Use bcrypt for secure password hashing and comparison. Use jsonwebtoken to create JWT tokens. Store the secret key securely, preferably using environment variables. Exclude sensitive fields like password_hash from the response. Provide generic error messages to prevent information leakage (e.g., \"Invalid credentials\" instead of specifying whether the email or password was incorrect). Implementation Prompt: ```\\nImplement the following function for the file \\'app/api/users/login/route.js\\': post Ensure that the implementation follows Next.js 14 best practices, includes input validation, authenticates user credentials against the User model in MongoDB, generates secure JWT tokens, excludes sensitive information from responses, and handles errors appropriately. ```', 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315627.9659636, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '28. users/me/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/me/route.js ``` Purpose: Provides an endpoint for authenticated users to retrieve their own profile information. This allows users to view and manage their personal data within the system. Key Components/Functions: GET : Retrieves the profile information of the authenticated user. PUT : Updates the profile information of the authenticated user. DELETE : Potentially allows users to delete their own accounts. Implementation Guidelines: Purpose : Enable users to access and manage their personal profile information securely. Key Functions : get : Functionality : Fetches the authenticated user's profile information from the database. Excludes sensitive information like password_hash . Authorization : Accessible only to authenticated users. put : Functionality : Allows users to update their profile details (e.g., username, bio, skills). Authorization : Ensures that users can only update their own profiles. delete : Potential Uses : Allows users to delete their own accounts. Authorization : Ensures that users can only delete their own profiles. Integration Points : Authentication Middleware : Verifies that the user is authenticated and attaches user information to the request. Database Models : Interacts with the User model to retrieve and update user data. Data Flow : Incoming Request : User sends a GET request to retrieve their profile or a PUT request to update their profile. Processing : For GET : Retrieves user data from the database. For PUT : Validates input data and updates user information. Response : Returns the user's profile information or confirmation of updates in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user attempts to access or modify another user's profile. Validation Errors : Returns 400 Bad Request for invalid input data during profile updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that any data provided for profile updates (e.g., username, bio) is valid and conforms to required formats. get : Functionality : Fetches the authenticated user's profile information from the database. Excludes sensitive information like password_hash . Authorization : Accessible only to authenticated users. put : Functionality : Allows users to update their profile details (e.g., username, bio, skills). Authorization : Ensures that users can only update their own profiles. delete : Potential Uses : Allows users to delete their own accounts. Authorization : Ensures that users can only delete their own profiles. Functionality : Fetches the authenticated user's profile information from the database. Excludes sensitive information like password_hash . Authorization : Accessible only to authenticated users. Fetches the authenticated user's profile information from the database. Excludes sensitive information like password_hash . Accessible only to authenticated users. Functionality : Allows users to update their profile details (e.g., username, bio, skills). Authorization : Ensures that users can only update their own profiles. Allows users to update their profile details (e.g., username, bio, skills). Ensures that users can only update their own profiles. Potential Uses : Allows users to delete their own accounts. Authorization : Ensures that users can only delete their own profiles. Allows users to delete their own accounts. Ensures that users can only delete their own profiles. Authentication Middleware : Verifies that the user is authenticated and attaches user information to the request. Database Models : Interacts with the User model to retrieve and update user data. Verifies that the user is authenticated and attaches user information to the request. Interacts with the User model to retrieve and update user data. Incoming Request : User sends a GET request to retrieve their profile or a PUT request to update their profile. Processing : For GET : Retrieves user data from the database. For PUT : Validates input data and updates user information. Response : Returns the user's profile information or confirmation of updates in JSON format. User sends a GET request to retrieve their profile or a PUT request to update their profile. For GET : Retrieves user data from the database. For PUT : Validates input data and updates user information. Returns the user's profile information or confirmation of updates in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user attempts to access or modify another user's profile. Validation Errors : Returns 400 Bad Request for invalid input data during profile updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user attempts to access or modify another user's profile. Returns 400 Bad Request for invalid input data during profile updates. Returns 500 Internal Server Error for unexpected issues. Ensure that any data provided for profile updates (e.g., username, bio) is valid and conforms to required formats. Implementation Guidelines: Response Sanitization : Exclude sensitive fields like password_hash from the response. Profile Updates : Allow users to update permissible fields only. Implement checks to prevent users from modifying restricted fields like role . Security Considerations : Protect against injection attacks by sanitizing input data. Use HTTPS to secure data transmission. Exclude sensitive fields like password_hash from the response. Allow users to update permissible fields only. Implement checks to prevent users from modifying restricted fields like role . Protect against injection attacks by sanitizing input data. Use HTTPS to secure data transmission. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/me/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks, retrieves and updates user data from the User model in MongoDB, excludes sensitive information from responses, validates input data, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315630.0447583, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '29. users/register/route.js' to API. Metadata: {'content': 'File Path in Next.js: ```bash\\napp/api/users/register/route.js ``` Purpose: Handles user registration by creating new user accounts. This endpoint allows new users to sign up by providing necessary details such as username, email, and password. Key Components/Functions: POST : Registers a new user by creating a user account in the database. Implementation Guidelines: Purpose : Enable new users to create accounts within the system, ensuring that user data is securely stored and validated. Key Functions : post : Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for correctness and completeness. Checks if the email already exists in the database to prevent duplicate accounts. Hashes the user\\'s password using bcrypt before storing it. Creates a new user record in MongoDB. Optionally assigns a default role (e.g., \\'user\\') and initializes other fields. Generates a JWT token upon successful registration. Returns the token and sanitized user information. Security : Ensures that passwords are securely hashed. Validates input to prevent injection attacks and other vulnerabilities. Integration Points : Database Models : Interacts with the User model to create new user records. Authentication Utilities : Uses bcrypt for password hashing and jsonwebtoken for token generation. Data Flow : Incoming Request : New user sends a POST request with registration details. Processing : Validates and sanitizes input data. Checks for existing users with the same email. Hashes the password and creates a new user record. Generates a JWT token. Response : Returns the JWT token and user details (excluding password_hash ) in JSON format. Error Handling : Validation Errors : Returns 400 Bad Request for missing or invalid input data. Conflict Errors : Returns 409 Conflict if a user with the provided email already exists. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all required fields are provided and meet format requirements (e.g., valid email format). Enforce password strength requirements to enhance security. post : Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for correctness and completeness. Checks if the email already exists in the database to prevent duplicate accounts. Hashes the user\\'s password using bcrypt before storing it. Creates a new user record in MongoDB. Optionally assigns a default role (e.g., \\'user\\') and initializes other fields. Generates a JWT token upon successful registration. Returns the token and sanitized user information. Security : Ensures that passwords are securely hashed. Validates input to prevent injection attacks and other vulnerabilities. Functionality : Receives user registration details ( username , email , password , etc.). Validates the input data for correctness and completeness. Checks if the email already exists in the database to prevent duplicate accounts. Hashes the user\\'s password using bcrypt before storing it. Creates a new user record in MongoDB. Optionally assigns a default role (e.g., \\'user\\') and initializes other fields. Generates a JWT token upon successful registration. Returns the token and sanitized user information. Security : Ensures that passwords are securely hashed. Validates input to prevent injection attacks and other vulnerabilities. Receives user registration details ( username , email , password , etc.). Validates the input data for correctness and completeness. Checks if the email already exists in the database to prevent duplicate accounts. Hashes the user\\'s password using bcrypt before storing it. Creates a new user record in MongoDB. Optionally assigns a default role (e.g., \\'user\\') and initializes other fields. Generates a JWT token upon successful registration. Returns the token and sanitized user information. Ensures that passwords are securely hashed. Validates input to prevent injection attacks and other vulnerabilities. Database Models : Interacts with the User model to create new user records. Authentication Utilities : Uses bcrypt for password hashing and jsonwebtoken for token generation. Interacts with the User model to create new user records. Uses bcrypt for password hashing and jsonwebtoken for token generation. Incoming Request : New user sends a POST request with registration details. Processing : Validates and sanitizes input data. Checks for existing users with the same email. Hashes the password and creates a new user record. Generates a JWT token. Response : Returns the JWT token and user details (excluding password_hash ) in JSON format. New user sends a POST request with registration details. Validates and sanitizes input data. Checks for existing users with the same email. Hashes the password and creates a new user record. Generates a JWT token. Returns the JWT token and user details (excluding password_hash ) in JSON format. Validation Errors : Returns 400 Bad Request for missing or invalid input data. Conflict Errors : Returns 409 Conflict if a user with the provided email already exists. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 400 Bad Request for missing or invalid input data. Returns 409 Conflict if a user with the provided email already exists. Returns 500 Internal Server Error for unexpected issues. Ensure that all required fields are provided and meet format requirements (e.g., valid email format). Enforce password strength requirements to enhance security. Implementation Guidelines: Password Security : Use bcrypt to hash passwords before storing them. Do not store plain-text passwords. Token Generation : Use jsonwebtoken to create JWT tokens. Store the secret key securely using environment variables. Response Sanitization : Exclude sensitive fields like password_hash from the response. Error Messaging : Provide clear and generic error messages to prevent information leakage (e.g., \"Email already in use\"). Use bcrypt to hash passwords before storing them. Do not store plain-text passwords. Use jsonwebtoken to create JWT tokens. Store the secret key securely using environment variables. Exclude sensitive fields like password_hash from the response. Provide clear and generic error messages to prevent information leakage (e.g., \"Email already in use\"). Implementation Prompt: ```\\nImplement the following function for the file \\'app/api/users/register/route.js\\': post Ensure that the implementation follows Next.js 14 best practices, includes input validation, securely hashes user passwords using bcrypt, checks for existing users to prevent duplicates, creates new user records in MongoDB, generates secure JWT tokens, excludes sensitive information from responses, and handles errors appropriately. ```', 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315632.118097, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '30. users/[id]/route.js' to API. Metadata: {'content': \"File Path in Next.js: ```bash\\napp/api/users/[id]/route.js ``` Purpose: Handles operations for a specific user identified by their unique ID. This includes retrieving, updating, and deleting individual user accounts. Primarily intended for administrative actions, allowing admins to manage user accounts effectively. Key Components/Functions: GET : Retrieves details of a specific user. PUT : Updates the details of a specific user. DELETE : Deletes a specific user account. Implementation Guidelines: Purpose : Enable detailed management of individual user accounts, allowing for retrieval, modification, and removal based on the user's unique identifier. Key Functions : get : Functionality : Fetches the user identified by the [id] parameter. Includes user details and associated task summaries. Authorization : Accessible only to admins or the user themselves. put : Functionality : Updates user details such as username, email, role, bio, and skills. Allows admins to promote or demote users by changing roles. Authorization : Ensures that only admins or the user themselves can update the user details. delete : Functionality : Removes the user from the database. Handles cascading deletions of associated tasks, submissions, and comments. Authorization : Ensures that only admins or the user themselves can delete the user account. Integration Points : Authentication Middleware : Verifies that the requester has the necessary permissions (admin or user themselves). Database Models : Interacts with the User , Task , Submission , and Comment models to manage user data and associated records. Data Flow : Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific user via their [id] . Processing : The server authenticates and authorizes the requester. For GET : Retrieves the user from the database. For PUT : Validates input data and updates user information. For DELETE : Removes the user and handles cascading deletions. Response : Returns the user details, confirmation of updates, or deletion status in JSON format. Error Handling : Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the user with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during user updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Input Validation : Ensure that all updated user details are valid and conform to expected formats and constraints. Prevent unauthorized fields from being updated (e.g., role changes restricted to admins). get : Functionality : Fetches the user identified by the [id] parameter. Includes user details and associated task summaries. Authorization : Accessible only to admins or the user themselves. put : Functionality : Updates user details such as username, email, role, bio, and skills. Allows admins to promote or demote users by changing roles. Authorization : Ensures that only admins or the user themselves can update the user details. delete : Functionality : Removes the user from the database. Handles cascading deletions of associated tasks, submissions, and comments. Authorization : Ensures that only admins or the user themselves can delete the user account. Functionality : Fetches the user identified by the [id] parameter. Includes user details and associated task summaries. Authorization : Accessible only to admins or the user themselves. Fetches the user identified by the [id] parameter. Includes user details and associated task summaries. Accessible only to admins or the user themselves. Functionality : Updates user details such as username, email, role, bio, and skills. Allows admins to promote or demote users by changing roles. Authorization : Ensures that only admins or the user themselves can update the user details. Updates user details such as username, email, role, bio, and skills. Allows admins to promote or demote users by changing roles. Ensures that only admins or the user themselves can update the user details. Functionality : Removes the user from the database. Handles cascading deletions of associated tasks, submissions, and comments. Authorization : Ensures that only admins or the user themselves can delete the user account. Removes the user from the database. Handles cascading deletions of associated tasks, submissions, and comments. Ensures that only admins or the user themselves can delete the user account. Authentication Middleware : Verifies that the requester has the necessary permissions (admin or user themselves). Database Models : Interacts with the User , Task , Submission , and Comment models to manage user data and associated records. Verifies that the requester has the necessary permissions (admin or user themselves). Interacts with the User , Task , Submission , and Comment models to manage user data and associated records. Incoming Request : Admin sends a GET , PUT , or DELETE request targeting a specific user via their [id] . Processing : The server authenticates and authorizes the requester. For GET : Retrieves the user from the database. For PUT : Validates input data and updates user information. For DELETE : Removes the user and handles cascading deletions. Response : Returns the user details, confirmation of updates, or deletion status in JSON format. Admin sends a GET , PUT , or DELETE request targeting a specific user via their [id] . The server authenticates and authorizes the requester. For GET : Retrieves the user from the database. For PUT : Validates input data and updates user information. For DELETE : Removes the user and handles cascading deletions. Returns the user details, confirmation of updates, or deletion status in JSON format. Authentication Errors : Returns 401 Unauthorized if the user is not authenticated. Authorization Errors : Returns 403 Forbidden if the user lacks permission to perform the action. Not Found Errors : Returns 404 Not Found if the user with the specified [id] does not exist. Validation Errors : Returns 400 Bad Request for invalid input data during user updates. Server Errors : Returns 500 Internal Server Error for unexpected issues. Returns 401 Unauthorized if the user is not authenticated. Returns 403 Forbidden if the user lacks permission to perform the action. Returns 404 Not Found if the user with the specified [id] does not exist. Returns 400 Bad Request for invalid input data during user updates. Returns 500 Internal Server Error for unexpected issues. Ensure that all updated user details are valid and conform to expected formats and constraints. Prevent unauthorized fields from being updated (e.g., role changes restricted to admins). Implementation Guidelines: Response Sanitization : Exclude sensitive fields like password_hash from the response. Role Management : Implement safeguards to prevent non-admin users from escalating privileges. Data Integrity : Ensure that cascading deletions do not leave orphaned records in the database. Security Considerations : Protect against injection attacks by sanitizing input data. Use HTTPS to secure data transmission. Exclude sensitive fields like password_hash from the response. Implement safeguards to prevent non-admin users from escalating privileges. Ensure that cascading deletions do not leave orphaned records in the database. Protect against injection attacks by sanitizing input data. Use HTTPS to secure data transmission. Implementation Prompt: ```\\nImplement the following functions for the file 'app/api/users/[id]/route.js': get put delete Ensure that the implementation follows Next.js 14 best practices, includes authentication and authorization checks (admin or the user themselves), interacts with the User, Task, Submission, and Comment models in MongoDB, validates input data, excludes sensitive information from responses, manages cascading deletions, and handles errors appropriately. ```\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315634.1861472, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '31. Summary and Additional Considerations' to API. Metadata: {'content': \"Having detailed the core API routes, it's essential to consider the following aspects to ensure a robust and maintainable Next.js 14 application:\", 'level': 2, 'parent': None, 'children': ['1. Middleware and Authentication', '2. Database Integration', '3. File Uploads and Storage', '4. Frontend Components', '5. Security Best Practices', '6. Error Handling and Logging', '7. Performance Optimization', '8. Testing and Deployment', '9. Documentation', '10. Scalability Considerations'], 'timestamp': 1728315636.256061, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '1. Middleware and Authentication' to API. Metadata: {'content': \"Middleware Implementation : Utilize Next.js's built-in middleware to handle authentication and authorization. Implement a global middleware to verify JWT tokens on protected routes. Role-Based Access Control (RBAC) : Define user roles (e.g., 'user', 'admin') and assign permissions accordingly. Ensure that role checks are enforced in API routes to protect sensitive operations. Utilize Next.js's built-in middleware to handle authentication and authorization. Implement a global middleware to verify JWT tokens on protected routes. Define user roles (e.g., 'user', 'admin') and assign permissions accordingly. Ensure that role checks are enforced in API routes to protect sensitive operations.\", 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315638.3493364, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '2. Database Integration' to API. Metadata: {'content': 'Mongoose Setup : Configure Mongoose to connect to your MongoDB instance. Define schemas and models corresponding to User , Project , Task , Submission , and Comment . Connection Management : Implement efficient connection pooling and error handling for database interactions. Configure Mongoose to connect to your MongoDB instance. Define schemas and models corresponding to User , Project , Task , Submission , and Comment . Implement efficient connection pooling and error handling for database interactions.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315640.4377127, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '3. File Uploads and Storage' to API. Metadata: {'content': 'Handling File Uploads : Use multer or similar middleware within Next.js API routes to manage multipart/form-data. Store uploaded files in a secure and scalable storage solution (e.g., AWS S3, Google Cloud Storage) if necessary. Static File Serving : Configure Next.js to serve static files securely, ensuring that only authorized users can access them. Use multer or similar middleware within Next.js API routes to manage multipart/form-data. Store uploaded files in a secure and scalable storage solution (e.g., AWS S3, Google Cloud Storage) if necessary. Configure Next.js to serve static files securely, ensuring that only authorized users can access them.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315642.53756, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '4. Frontend Components' to API. Metadata: {'content': \"Page Structure : Utilize Next.js's app/ directory for defining pages and layouts. Create React components for user interfaces such as dashboards, task lists, project views, and admin panels. Styling with Tailwind CSS : Integrate Tailwind CSS for rapid and responsive UI development. Leverage Next.js's built-in support for PostCSS and Tailwind configurations. State Management : Use React's Context API or state management libraries (e.g., Redux) to manage global state, such as user authentication status and application data. Utilize Next.js's app/ directory for defining pages and layouts. Create React components for user interfaces such as dashboards, task lists, project views, and admin panels. Integrate Tailwind CSS for rapid and responsive UI development. Leverage Next.js's built-in support for PostCSS and Tailwind configurations. Use React's Context API or state management libraries (e.g., Redux) to manage global state, such as user authentication status and application data.\", 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315644.6020095, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '5. Security Best Practices' to API. Metadata: {'content': 'Input Sanitization : Sanitize all incoming data to prevent injection attacks and ensure data integrity. HTTPS Enforcement : Ensure that the application is served over HTTPS to secure data in transit. Secure Storage of Secrets : Store sensitive information like JWT secret keys and database credentials in environment variables. Sanitize all incoming data to prevent injection attacks and ensure data integrity. Ensure that the application is served over HTTPS to secure data in transit. Store sensitive information like JWT secret keys and database credentials in environment variables.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315646.7040293, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '6. Error Handling and Logging' to API. Metadata: {'content': 'Consistent Error Responses : Implement a standardized error response format across all API routes. Logging Mechanisms : Use logging libraries (e.g., winston , pino ) to log errors and important events for monitoring and debugging. User-Friendly Error Messages : Provide clear and concise error messages to users without exposing sensitive information. Implement a standardized error response format across all API routes. Use logging libraries (e.g., winston , pino ) to log errors and important events for monitoring and debugging. Provide clear and concise error messages to users without exposing sensitive information.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315648.7920818, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '7. Performance Optimization' to API. Metadata: {'content': \"API Route Optimization : Optimize database queries to reduce latency and improve response times. Caching Strategies : Implement caching for frequently accessed data to enhance performance. Code Splitting and Lazy Loading : Utilize Next.js's features to split code and lazy load components as needed, improving initial load times. Optimize database queries to reduce latency and improve response times. Implement caching for frequently accessed data to enhance performance. Utilize Next.js's features to split code and lazy load components as needed, improving initial load times.\", 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315650.8615124, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '8. Testing and Deployment' to API. Metadata: {'content': 'Testing Frameworks : Use testing frameworks like Jest and React Testing Library to write unit and integration tests. Continuous Integration/Continuous Deployment (CI/CD) : Set up CI/CD pipelines to automate testing and deployment processes. Monitoring and Analytics : Integrate monitoring tools (e.g., Sentry, New Relic) to track application performance and errors in real-time. Use testing frameworks like Jest and React Testing Library to write unit and integration tests. Set up CI/CD pipelines to automate testing and deployment processes. Integrate monitoring tools (e.g., Sentry, New Relic) to track application performance and errors in real-time.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315652.9297254, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '9. Documentation' to API. Metadata: {'content': 'API Documentation : Use tools like Swagger or API Blueprint to document API endpoints, request/response structures, and authentication mechanisms. Code Documentation : Maintain comprehensive inline comments and documentation for ease of maintenance and onboarding. Use tools like Swagger or API Blueprint to document API endpoints, request/response structures, and authentication mechanisms. Maintain comprehensive inline comments and documentation for ease of maintenance and onboarding.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315655.0052428, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent section '10. Scalability Considerations' to API. Metadata: {'content': 'Horizontal Scaling : Design the application to support horizontal scaling, allowing it to handle increased load by adding more instances. Database Scaling : Implement database indexing and sharding strategies to manage large datasets efficiently. Load Balancing : Use load balancers to distribute incoming traffic evenly across server instances. Design the application to support horizontal scaling, allowing it to handle increased load by adding more instances. Implement database indexing and sharding strategies to manage large datasets efficiently. Use load balancers to distribute incoming traffic evenly across server instances.', 'level': 3, 'parent': '31. Summary and Additional Considerations', 'children': [], 'timestamp': 1728315657.1006052, 'access_count': 0, 'relevance_score': 1.0}\n",
      "Sent section '32. Conclusion' to API. Metadata: {'content': \"By following this comprehensive guide, you can effectively transition your existing Express.js project to Next.js 14, leveraging its modern features for both frontend and backend development. Each API route has been meticulously detailed to ensure proper implementation, security, and scalability. Additionally, considering middleware, database integration, frontend components, security best practices, error handling, performance optimization, testing, deployment, documentation, and scalability will contribute to building a robust and maintainable application. Remember to continuously refer to Next.js 14's official documentation and best practices to stay updated with the latest features and recommendations. If you require further assistance with specific implementations or have additional questions, feel free to ask!\", 'level': 2, 'parent': None, 'children': [], 'timestamp': 1728315659.194535, 'access_count': 0, 'relevance_score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API endpoint information\n",
    "baseUrl = \"http://localhost:8000\"\n",
    "\n",
    "# Function to send request with metadata\n",
    "def send_request_with_metadata(title, metadata):\n",
    "    url = f\"{baseUrl}/gravrag/create_memory\"  # Use your existing API endpoint\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    body = {\"content\": title, \"metadata\": metadata}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            data=json.dumps(body)  # Convert the body to JSON format\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # Return the response as JSON\n",
    "        else:\n",
    "            logger.error(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "            return {\"error\": f\"Request failed with status code {response.status_code}\", \"details\": response.text}\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "        return {\"error\": \"Request failed\", \"details\": str(e)}\n",
    "\n",
    "# Data model to represent sections of the README\n",
    "@dataclass\n",
    "class ReadmeSection:\n",
    "    content: str\n",
    "    heading: str\n",
    "    level: int\n",
    "    parent: Optional[str]\n",
    "    children: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "# Function to parse README and break it into sections\n",
    "def parse_readme(content: str) -> List[ReadmeSection]:\n",
    "    html = markdown.markdown(content)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    sections = []\n",
    "    section_stack = []\n",
    "    current_section = None\n",
    "\n",
    "    for elem in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre', 'ul', 'ol']):\n",
    "        if elem.name.startswith('h'):\n",
    "            level = int(elem.name[1])\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "\n",
    "            current_section = ReadmeSection(\n",
    "                content='',\n",
    "                heading=elem.text.strip(),\n",
    "                level=level,\n",
    "                parent=section_stack[-1].heading if section_stack else None,\n",
    "                children=[],\n",
    "                metadata={}\n",
    "            )\n",
    "\n",
    "            # Append to parent only if there's a parent\n",
    "            if section_stack:\n",
    "                section_stack[-1].children.append(current_section.heading)\n",
    "\n",
    "            sections.append(current_section)\n",
    "            section_stack.append(current_section)\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section.content += \" \" + elem.get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "    #Strip any leading/trailing spaces in content for clean output\n",
    "    for section in sections:\n",
    "        section.content = section.content.strip()\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "# Function to process README and send sections to API\n",
    "def process_readme_and_send(readme_path: str):\n",
    "    with open(readme_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    sections = parse_readme(content)\n",
    "    \n",
    "    for section in sections:\n",
    "        # Prepare title and metadata without vectorization\n",
    "        title = section.heading\n",
    "        metadata = {\n",
    "            \"content\": section.content,\n",
    "            \"level\": section.level,\n",
    "            \"parent\": section.parent,\n",
    "            \"children\": section.children,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"access_count\": 0,\n",
    "            \"relevance_score\": 1.0\n",
    "        }\n",
    "\n",
    "        # Send to the API endpoint\n",
    "        response = send_request_with_metadata(title, metadata)\n",
    "        print(f\"Sent section '{title}' to API. Metadata: {metadata}\")\n",
    "        \n",
    "\n",
    "\n",
    "process_readme_and_send(\"README.MD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
