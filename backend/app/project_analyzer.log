2024-09-25 13:59:53,665 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-09-25 13:59:53,667 - DEBUG - load_verify_locations cafile='C:\\Users\\ADA\\miniconda3\\envs\\groqy\\Lib\\site-packages\\certifi\\cacert.pem'
2024-09-25 13:59:53,682 - INFO - Groq client initialized successfully.
2024-09-25 13:59:53,682 - INFO - Configuration saved successfully.
2024-09-25 13:59:53,683 - INFO - Starting project analysis...
2024-09-25 13:59:55,201 - INFO - Project structure analysis complete.
2024-09-25 13:59:55,215 - INFO - Generating AI analysis...
2024-09-25 13:59:55,219 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are an AI assistant tasked with analyzing a project structure.'}, {'role': 'user', 'content': 'You are an AI assistant analyzing a project structure.\n\nAnalyze the following project structure:\n\n### [.venv](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\.venv)\n\n**Type:**  file\n\n(Binary or unreadable file)\n\n### [Dockerfile](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\Dockerfile)\n\n**Type:**  file\n\n\n    FROM python:3.11.9-slim\n    \n    WORKDIR /app\n    \n    COPY requirements.txt .\n    RUN pip install --no-cache-dir -r requirements.txt\n    \n    COPY . .\n    \n    CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]\n    \n\n\n\n### [Testmain.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\Testmain.py)\n\n**Type:** PY file\n\n\n    from fastapi import FastAPI, HTTPException\n    from pydantic import BaseModel\n    from typing import List, Optional, Dict, Any\n    # from agentChef import DatasetKitchen, TemplateManager, FileHandler\n    from gravrag import Knowledge\n    import uvicorn\n    import asyncio\n    \n    app = FastAPI()\n    \n    # Configuration for AgentChef\n    config = {\n        \'templates_dir\': \'./templates\',\n        \'input_dir\': \'./input\',\n        \'output_dir\': \'./output\',\n        \'ollama_config\': {\n            \'model\': \'phi3\',\n            \'api_base\': \'http://localhost:11434\'\n        }\n    }\n    \n    # Initialize AgentChef components\n    # kitchen = DatasetKitchen(config)\n    # template_manager = TemplateManager(config[\'templates_dir\'])\n    # file_handler = FileHandler(config[\'input_dir\'], config[\'output_dir\'])\n    \n    # AgentChef API models\n    class PrepareDatasetRequest(BaseModel):\n        source: str\n        template: str\n        num_samples: int = 100\n        output_file: str\n    \n    class GenerateParaphrasesRequest(BaseModel):\n        seed_file: str\n        num_samples: int = 1\n        system_prompt: Optional[str] = None\n    \n    class AugmentDataRequest(BaseModel):\n        seed_parquet: str\n    \n    class ParseTextToParquetRequest(BaseModel):\n        text_content: str\n        template_name: str\n        filename: str\n    \n    class ConvertParquetRequest(BaseModel):\n        parquet_file: str\n        output_formats: List[str] = [\'csv\', \'jsonl\']\n    \n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [agent_chef_guide.md](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\agent_chef_guide.md)\n\n**Type:** MD file\n\n\n    # AgentChef Comprehensive Guide\n    \n    This guide demonstrates how to use the agentChef package to scrape data from various sources, process it into a structured format, and run it through a data processing pipeline.\n    \n    ## Initialization\n    \n    Import the necessary modules and initialize the DatasetKitchen:\n    \n    ```python\n    from chef import DatasetKitchen\n    from agentChef.cutlery import CustomAgentBase\n    import pandas as pd\n    \n    config = {\n        \'templates_dir\': \'./templates\',\n        \'input_dir\': \'./input\',\n        \'output_dir\': \'./output\',\n    }\n    \n    kitchen = DatasetKitchen(config)\n    ```\n    \n    ## Data Collection\n    \n    ### 1. Hugging Face Datasets\n    \n    ```python\n    hf_dataset_url = "https://huggingface.co/datasets/your_dataset"\n    hf_data = kitchen.document_loader.load_from_huggingface(hf_dataset_url)\n    ```\n    \n    ### 2. Wikipedia\n    \n    ```python\n    wiki_query = "Artificial Intelligence"\n    wiki_data = kitchen.document_loader.load_from_wikipedia(wiki_query)\n    ```\n    \n    ### 3. Reddit (Custom Agent)\n    \n    Create a custom Reddit agent:\n    \n    ```python\n    import praw\n    \n    class RedditAgent(CustomAgentBase):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.reddit = praw.Reddit(client_id=\'YOUR_CLIENT_ID\',\n                                      client_secret=\'YOUR_CLIENT_SECRET\',\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [agentchef_resources.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\agentchef_resources.py)\n\n**Type:** PY file\n\n\n    import os\n    import json\n    import logging\n    import requests\n    from typing import List, Dict, Any\n    from anthropic import Anthropic\n    import openai\n    from groq import Groq\n    \n    app.include_router(llm_router, prefix="/llm", tags=["llm"])\n    \n    # Load model data from JSON file\n    with open(\'neural_resources.json\', \'r\') as f:\n        model_data = json.load(f)\n    \n    class AIAsset:\n        def __init__(self, api_key: str):\n            self.api_key = api_key\n    \n        def create_message(self, model: str, message: str) -> Dict[str, Any]:\n            """Abstract method for creating a message"""\n            pass\n    \n        def get_output_tokens(self, response: Dict[str, Any]) -> int:\n            """Abstract method to get token usage"""\n            pass\n    \n        def execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n            """Abstract method for executing tool calls"""\n            pass\n    \n    class AnthropicLLM(AIAsset):\n        def __init__(self, api_key: str):\n            super().__init__(api_key)\n            self.client = Anthropic(api_key=api_key)\n    \n        def create_message(self, model: str, message: str) -> Dict[str, Any]:\n            response = self.client.messages.create(\n                model=model,\n                messages=[message],\n                max_tokens=model_data[\'models\'][model][\'max_tokens\']\n            )\n            return response.model_dump()\n    \n        def get_output_tokens(self, response: Dict[str, Any]) -> int:\n            return response.get(\'usage\', {}).get(\'output_tokens\', 0)\n    \n    class OpenAILLM(AIAsset):\n        def __init__(self, api_key: str):\n            super().__init__(api_key)\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n## Directory: api\n\n  ### [api\\__init__.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\__init__.py)\n\n  **Type:** PY file\n\n  \n    # __init__.py\n    \n    from .agentChef_api import app as agentchef_app, LLMConfig, CustomAgentConfig, PrepareDatasetRequest, GenerateParaphrasesRequest, AugmentDataRequest\n    from .gravrag_API import app as gravrag_app, MemoryInputModel\n    \n    __all__ = [\n        \'agentchef_app\',\n        \'LLMConfig\',\n        \'CustomAgentConfig\',\n        \'PrepareDatasetRequest\',\n        \'GenerateParaphrasesRequest\',\n        \'AugmentDataRequest\',\n        \'gravrag_app\',\n        \'MemoryInputModel\'\n    ]\n    \n\n\n\n  ### [api\\agentChef_api.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\agentChef_api.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import FastAPI, HTTPException\n    from pydantic import BaseModel\n    from typing import List, Optional, Dict, Any\n    import uvicorn\n    from cutlery import DatasetManager, TemplateManager, FileHandler, DocumentLoader\n    from chef import DatasetKitchen, DataCollectionAgent, DataDigestionAgent, DataGenerationAgent, DataCleaningAgent, DataAugmentationAgent\n    from cutlery import PromptManager\n    from ai_api_providers import LLMManager\n    from huggingface_hub import HfApi\n    import uvicorn\n    import pandas as pd\n    import json\n    import os\n    \n    app = FastAPI(title="AgentChef API", description="API for data processing and dataset creation")\n    \n    # Configuration for AgentChef\n    # config = {\n    #     \'templates_dir\': \'./templates\',\n    #     \'input_dir\': \'./input\',\n    #     \'output_dir\': \'./output\',\n    #     \'ollama_config\': {\n    #         \'model\': \'phi3\',\n    #         \'api_base\': \'http://localhost:11434\'\n    #     }\n    # }\n    \n    # Pydantic models for request bodies\n    class DataSourceRequest(BaseModel):\n        source_type: str\n        query: str\n        max_results: int = 10\n    \n    class StructureDataRequest(BaseModel):\n        data: List[Dict[str, Any]]\n        template_name: str\n    \n    class AugmentDataRequest(BaseModel):\n        input_file: str\n        num_samples: int = 5\n        agent_name: str\n    \n    class PushToHuggingFaceRequest(BaseModel):\n        file_path: str\n        repo_id: str\n        token: str\n    \n    # Helper function to get LLMManager instance\n    def get_llm_manager():\n        return llm_manager\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n  ### [api\\ai_api_providers.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\ai_api_providers.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import APIRouter, HTTPException, Depends\n    from pydantic import BaseModel\n    from typing import Dict, Any, List\n    from app.agentchef_resources import LLMManager, model_data\n    \n    router = APIRouter()\n    \n    # Initialize LLMManager\n    llm_manager = LLMManager()\n    \n    class Message(BaseModel):\n        content: str\n    \n    class APIKeyUpdate(BaseModel):\n        provider: str\n        api_key: str\n    \n    # Dependency to get the LLMManager instance\n    def get_llm_manager():\n        return llm_manager\n    \n    @router.post("/route_query")\n    async def route_query(message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        response = manager.route_query(message.content)\n        if "error" in response:\n            raise HTTPException(status_code=500, detail=response["error"])\n        return response\n    \n    @router.post("/set_api_key")\n    async def set_api_key(api_key_update: APIKeyUpdate, manager: LLMManager = Depends(get_llm_manager)):\n        manager.set_api_key(api_key_update.provider, api_key_update.api_key)\n        return {"message": f"API key updated for {api_key_update.provider}"}\n    \n    @router.get("/available_models")\n    async def get_available_models(manager: LLMManager = Depends(get_llm_manager)):\n        return {"available_models": list(manager.llm_models.keys())}\n    \n    @router.post("/create_message/{provider}/{model}")\n    async def create_message(provider: str, model: str, message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        if provider not in manager.llm_models:\n            raise HTTPException(status_code=404, detail=f"Provider {provider} not found")\n        try:\n            response = manager.llm_models[provider].create_message(model, message.content)\n            return response\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n    \n    @router.get("/model_info/{provider}/{model}")\n    async def get_model_info(provider: str, model: str):\n        if provider not in model_data[\'models\'] or model not in model_data[\'models\']:\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n  ### [api\\api_god_matrix_guide.md](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\api_god_matrix_guide.md)\n\n  **Type:** MD file\n\n  \n    # AgentChef and GravRAG API GOD MATRIX Usage Guide\n    \n    This guide explains how to use the combined AgentChef and GravRAG API GOD MATRIX system, which is set up using `main.py` and includes the functionality from `agentChef_api.py` & gravrag_API.py.\n    \n    ## Table of Contents\n    \n    1. [Setup and Installation](#setup-and-installation)\n    2. [Running the API](#running-the-api)\n    3. [AgentChef API Endpoints](#agentchef-api-endpoints)\n       - [Collect Data](#collect-data)\n       - [Structure Data](#structure-data)\n       - [Augment Data](#augment-data)\n       - [Push to Hugging Face](#push-to-hugging-face)\n       - [Get Templates](#get-templates)\n       - [Get Available Agents](#get-available-agents)\n    4. [GravRAG API Endpoints](#gravrag-api-endpoints)\n       - [Create Memory](#create-memory)\n       - [Recall Memory](#recall-memory)\n       - [Prune Memories](#prune-memories)\n    5. [Example Workflow](#example-workflow)\n    \n    ## Setup and Installation\n    \n    1. Ensure you have Python 3.7+ installed.\n    2. Install the required dependencies:\n       ```\n       pip install fastapi uvicorn agentchef gravrag pandas huggingface_hub\n       ```\n    3. Place `main.py`, `agentChef_api.py`, and `gravrag_API.py` in the same directory.\n    \n    ## Running the API\n    \n    1. Open a terminal and navigate to the directory containing `main.py`.\n    2. Run the following command:\n       ```\n       python main.py\n       ```\n    3. The AgentChef API will be available at `http://localhost:8888`.\n    4. The GravRAG API will be available at `http://localhost:6369`.\n    \n    ## AgentChef API Endpoints\n    \n    ### Collect Data\n    \n    - **Endpoint**: `POST http://localhost:8888/collect_data`\n    - **Description**: Collects data from arXiv, Wikipedia, or Hugging Face datasets.\n    - **Payload Example**:\n      ```json\n      {\n        "source_type": "arxiv",\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n  ### [api\\chef_api_providers_api.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\chef_api_providers_api.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import APIRouter, HTTPException, Depends\n    from pydantic import BaseModel\n    from typing import Dict, Any, List\n    from ..neural_resources import LLMManager, model_data  # Assuming the previous code is in llm_manager.py\n    \n    router = APIRouter()\n    \n    # Initialize LLMManager\n    llm_manager = LLMManager()\n    \n    class Message(BaseModel):\n        content: str\n    \n    class APIKeyUpdate(BaseModel):\n        provider: str\n        api_key: str\n    \n    # Dependency to get the LLMManager instance\n    def get_llm_manager():\n        return llm_manager\n    \n    @router.post("/route_query")\n    async def route_query(message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        response = manager.route_query(message.content)\n        if "error" in response:\n            raise HTTPException(status_code=500, detail=response["error"])\n        return response\n    \n    @router.post("/set_api_key")\n    async def set_api_key(api_key_update: APIKeyUpdate, manager: LLMManager = Depends(get_llm_manager)):\n        manager.set_api_key(api_key_update.provider, api_key_update.api_key)\n        return {"message": f"API key updated for {api_key_update.provider}"}\n    \n    @router.get("/available_models")\n    async def get_available_models(manager: LLMManager = Depends(get_llm_manager)):\n        return {"available_models": list(manager.llm_models.keys())}\n    \n    @router.post("/create_message/{provider}/{model}")\n    async def create_message(provider: str, model: str, message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        if provider not in manager.llm_models:\n            raise HTTPException(status_code=404, detail=f"Provider {provider} not found")\n        try:\n            response = manager.llm_models[provider].create_message(model, message.content)\n            return response\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n    \n    @router.get("/model_info/{provider}/{model}")\n    async def get_model_info(provider: str, model: str):\n        if provider not in model_data[\'models\'] or model not in model_data[\'models\']:\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n  ### [api\\comprehensive_file_insights.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\comprehensive_file_insights.py)\n\n  **Type:** PY file\n\n  (Binary or unreadable file)\n\n  ### [api\\file_contents.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\file_contents.py)\n\n  **Type:** PY file\n\n  (Binary or unreadable file)\n\n  ### [api\\file_insights.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\file_insights.py)\n\n  **Type:** PY file\n\n  (Binary or unreadable file)\n\n  ### [api\\file_structure.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\file_structure.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import APIRouter, HTTPException, Request\n    from pydantic import BaseModel\n    from typing import List, Dict, Any, Optional\n    import httpx\n    import requests\n    import json\n    \n    router = APIRouter()\n    \n    # URLs for both APIs\n    AGENT_CHEF_API_URL = "http://localhost:8888"\n    GRAV_RAG_API_URL = "http://localhost:6333"\n    \n    def prepare_dataset(source, template, num_samples, output_file):\n        response = requests.post(f"{AGENT_CHEF_API_URL}/prepare_dataset", json={\n            "source": source,\n            "template": template,\n            "num_samples": num_samples,\n            "output_file": output_file\n        })\n        return response.json()\n    \n    def generate_paraphrases(seed_file, num_samples, system_prompt=None):\n        response = requests.post(f"{AGENT_CHEF_API_URL}/generate_paraphrases", json={\n            "seed_file": seed_file,\n            "num_samples": num_samples,\n            "system_prompt": system_prompt\n        })\n        return response.json()\n    \n    def get_templates():\n        response = requests.get(f"{AGENT_CHEF_API_URL}/get_templates")\n        return response.json()\n    \n    # Example usage\n    if __name__ == "__main__":\n        # Prepare a dataset\n        result = prepare_dataset("example_source.txt", "chat", 100, "output_dataset.parquet")\n        print(result)\n    \n        # Generate paraphrases\n        result = generate_paraphrases("seed_file.txt", 5)\n        print(result)\n    \n        # Get available templates\n        templates = get_templates()\n        print(json.dumps(templates, indent=2))\n    \n\n\n\n  ### [api\\gravrag_API.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\gravrag_API.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import FastAPI, HTTPException\n    from pydantic import BaseModel\n    from typing import Dict, Any, Optional\n    from ..gravrag import Knowledge\n    \n    app = FastAPI()\n    \n    class MemoryInputModel(BaseModel):\n        content: str\n        metadata: Optional[Dict[str, Any]] = {}\n    \n    @app.post("/api/memory/create")\n    async def create_memory(memory_input: MemoryInputModel):\n        try:\n            await Knowledge.create_memory(memory_input.content, memory_input.metadata)\n            return {"message": "Memory created successfully"}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f"Error: {e}")\n    \n    @app.get("/api/memory/recall")\n    async def recall_memory(query: str, top_k: int = 5):\n        try:\n            results = await Knowledge.recall_memory(query, top_k)\n            return {"results": results}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f"Error: {e}")\n    \n    @app.post("/api/memory/prune")\n    async def prune_memories():\n        try:\n            await Knowledge.prune_memories()\n            return {"message": "Pruning complete"}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=f"Error: {e}")\n    \n    # Example ThunderClient payloads:\n    # 1. Create Memory\n    # {\n    #   "content": "This is a sample memory",\n    #   "metadata": {\n    #     "objective_id": "obj_1",\n    #     "task_id": "task_1"\n    #   }\n    # }\n    #\n    # 2. Recall Memory\n    # GET /api/memory/recall?query=How does GravRAG handle memory?\n    #\n    # 3. Prune Memories\n    # POST /api/memory/prune\n    \n\n\n\n  ### [api\\neural_resources_API.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\neural_resources_API.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import APIRouter, HTTPException, Depends\n    from pydantic import BaseModel\n    from typing import Dict, Any, List\n    from ..neural_resources import LLMManager, model_data  # Assuming the previous code is in llm_manager.py\n    \n    router = APIRouter()\n    \n    # Initialize LLMManager\n    llm_manager = LLMManager()\n    \n    class Message(BaseModel):\n        content: str\n    \n    class APIKeyUpdate(BaseModel):\n        provider: str\n        api_key: str\n    \n    # Dependency to get the LLMManager instance\n    def get_llm_manager():\n        return llm_manager\n    \n    @router.post("/route_query")\n    async def route_query(message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        response = manager.route_query(message.content)\n        if "error" in response:\n            raise HTTPException(status_code=500, detail=response["error"])\n        return response\n    \n    @router.post("/set_api_key")\n    async def set_api_key(api_key_update: APIKeyUpdate, manager: LLMManager = Depends(get_llm_manager)):\n        manager.set_api_key(api_key_update.provider, api_key_update.api_key)\n        return {"message": f"API key updated for {api_key_update.provider}"}\n    \n    @router.get("/available_models")\n    async def get_available_models(manager: LLMManager = Depends(get_llm_manager)):\n        return {"available_models": list(manager.llm_models.keys())}\n    \n    @router.post("/create_message/{provider}/{model}")\n    async def create_message(provider: str, model: str, message: Message, manager: LLMManager = Depends(get_llm_manager)):\n        if provider not in manager.llm_models:\n            raise HTTPException(status_code=404, detail=f"Provider {provider} not found")\n        try:\n            response = manager.llm_models[provider].create_message(model, message.content)\n            return response\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n    \n    @router.get("/model_info/{provider}/{model}")\n    async def get_model_info(provider: str, model: str):\n        if provider not in model_data[\'models\'] or model not in model_data[\'models\']:\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n  ### [api\\oarc_api.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\api\\oarc_api.py)\n\n  **Type:** PY file\n\n  \n    from fastapi import FastAPI, HTTPException\n    from pydantic import BaseModel\n    from typing import List, Optional\n    import uvicorn\n    from oarc import ollama_chatbot_base\n    \n    app = FastAPI()\n    \n    # Initialize DatasetKitchen and other components\n    chatbotbase = ollama_chatbot_base()\n    \n    class start_chatbot(BaseModel):\n        modelname: str  # Path to the source data file or URL\n        \n    @app.post("/start_chatbot")\n    async def start_chatbot(request: StartChatbotRequest):\n        """\n        """\n        try:\n            chatbotbase = start\n    \n            return {"message": f"Dataset prepared and saved to {request.output_file}"}\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=str(e))\n    \n\n\n\n### [chef.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\chef.py)\n\n**Type:** PY file\n\n\n    from typing import List, Dict, Any\n    import json\n    from colorama import Fore, Style\n    import click\n    # from .config import ConfigManager\n    from cutlery import DatasetManager, TemplateManager, PromptManager, FileHandler, DocumentLoader\n    from api.ai_api_providers import LLMManager\n    from api.agentChef_api import OllamaProvider\n    \n    # If ConfigManager is in a separate config.py file next to main.py\n    # from config import ConfigManager\n    \n    # If PromptManager is part of cutlery, update the import\n    from cutlery import PromptManager\n    \n    class DataCollectionAgent:\n        def __init__(self, template_manager: TemplateManager, file_handler: FileHandler, document_loader: DocumentLoader):\n            self.template_manager = template_manager\n            self.file_handler = file_handler\n            self.document_loader = document_loader\n    \n        def collect_and_structure_data(self, source: str, template_name: str) -> Dict[str, Any]:\n            template = self.template_manager.get_template(template_name)\n            \n            collected_data = self._collect_data(source)\n            structured_data = self._structure_data(collected_data, template)\n            \n            self.file_handler.save_to_parquet(structured_data, f"{template_name}_structured_data.parquet")\n            \n            return structured_data\n    \n        def _collect_data(self, source: str) -> List[Dict[str, Any]]:\n            if source.startswith("hf://"):\n                dataset_name = source[5:]\n                return self.document_loader.load_from_huggingface(dataset_name)\n            elif source.startswith("github://"):\n                repo_name = source[9:]\n                return self.document_loader.load_from_github(repo_name)\n            elif source.startswith("wiki://"):\n                query = source[7:]\n                return [self.document_loader.load_from_wikipedia(query)]\n            elif source.startswith("arxiv://"):\n                query = source[8:]\n                return self.document_loader.load_from_arxiv(query)\n            elif self.document_loader.is_url(source):\n                return self.document_loader.load_web_page(source)\n            else:\n                return self.file_handler.load_seed_data(source)\n    \n        def _structure_data(self, data: List[Dict[str, Any]], template: List[str]) -> Dict[str, List[Any]]:\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [config.json](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\config.json)\n\n**Type:** JSON file\n\n\n    {\n        "api_key": "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",\n        "project_dir": "D:/CodingGit_StorageHDD/Ollama_Custom_Mods/nextjs/cee_cee_mystery/quantum-nexus/groqy/backend/app",\n        "selected_files": [],\n        "system_prompt": "You are an AI assistant analyzing a project structure.",\n        "user_prompt": "Analyze the following project structure:",\n        "include_context": true,\n        "excluded_dirs": [\n            "node_modules",\n            ".git",\n            "__pycache__"\n        ],\n        "excluded_extensions": [\n            ".pyc",\n            ".log",\n            ".pyo"\n        ]\n    }\n    \n\n\n\n### [custom_agent_example.md](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\custom_agent_example.md)\n\n**Type:** MD file\n\n\n    # Custom Agents in agentChef\n    \n    Users can create custom agents to extend the functionality of the agentChef package. Here\'s how to define and use custom agents:\n    \n    ## Defining a Custom Agent\n    \n    To create a custom agent, inherit from the `CustomAgentBase` class and implement the required methods:\n    \n    ```python\n    from agentChef.cutlery import CustomAgentBase\n    import pandas as pd\n    \n    class MyCustomAgent(CustomAgentBase):\n        def __init__(self, llm_manager, template_manager, file_handler, prompt_manager, document_loader):\n            super().__init__(llm_manager, template_manager, file_handler, prompt_manager, document_loader)\n            # Add any custom initialization here\n    \n        def process_data(self, data: Any) -> pd.DataFrame:\n            # Implement your custom data processing logic here\n            # This method should return a pandas DataFrame\n            processed_data = ...  # Your processing logic\n            return pd.DataFrame(processed_data)\n    \n        def scrape_data(self, source: str) -> Any:\n            # Implement your custom data scraping logic here\n            # This method can return data in any format that your process_data method can handle\n            scraped_data = ...  # Your scraping logic\n            return scraped_data\n    \n        # Optionally, add any custom methods you need\n        def custom_method(self, ...):\n            # Custom functionality\n            pass\n    \n    # The `run` method is inherited from CustomAgentBase and typically doesn\'t need to be overridden\n    ```\n    \n    ## Example: Reddit Comment Scraper Agent\n    \n    Here\'s an example of a custom agent that scrapes comments from a Reddit post:\n    \n    ```python\n    import praw\n    from agentChef.cutlery import CustomAgentBase\n    import pandas as pd\n    \n    class RedditCommentAgent(CustomAgentBase):\n        def __init__(self, llm_manager, template_manager, file_handler, prompt_manager, document_loader):\n            super().__init__(llm_manager, template_manager, file_handler, prompt_manager, document_loader)\n            self.reddit = praw.Reddit(client_id=\'YOUR_CLIENT_ID\',\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [cutlery.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\cutlery.py)\n\n**Type:** PY file\n\n\n    # Import necessary libraries (assuming they\'re all needed)\n    import pandas as pd\n    from tqdm import tqdm\n    from datasets import load_dataset\n    import numpy as np\n    from colorama import Fore, Style, init\n    import os, json, urllib.parse, tarfile, gzip, shutil, requests, random, re, time, logging, glob\n    from typing import List, Dict, Any, Optional\n    import logging\n    \n    init(autoreset=True)\n    \n    from langchain_community.document_loaders import (\n        WebBaseLoader, PyPDFLoader, TextLoader, Docx2txtLoader,\n        UnstructuredPowerPointLoader, WikipediaLoader, ArxivLoader,\n        UnstructuredEPubLoader, JSONLoader, CSVLoader\n    )\n    \n    from api.ai_api_providers import LLMManager\n    \n    from huggingface_hub import snapshot_download, HfApi, hf_hub_download\n    from github import Github\n    init(autoreset=True)\n    \n    import requests\n    from abc import ABC, abstractmethod\n    from typing import List, Dict, Any\n    \n    import os\n    import json\n    from typing import Dict, Any\n    \n    class DataAgent(ABC):\n        @abstractmethod\n        def fetch_data(self, query: str) -> List[Dict[str, Any]]:\n            pass\n    \n        @abstractmethod\n        def get_source_name(self) -> str:\n            pass\n    \n    class ArxivAgent(DataAgent):\n        def fetch_data(self, query: str) -> List[Dict[str, Any]]:\n            # Implement arXiv API logic here\n            # Return a list of dictionaries containing paper information\n            pass\n    \n        def get_source_name(self) -> str:\n            return "arXiv"\n    \n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [example.venv](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\example.venv)\n\n**Type:** VENV file\n\n(Binary or unreadable file)\n\n### [gravrag.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\gravrag.py)\n\n**Type:** PY file\n\n\n    import time\n    import math\n    import uuid\n    import logging\n    from typing import List, Dict, Any, Optional\n    from sentence_transformers import SentenceTransformer\n    from qdrant_client import QdrantClient\n    from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition\n    \n    # Set up logging\n    logging.basicConfig(level=logging.INFO)\n    logger = logging.getLogger(__name__)\n    \n    # Gravitational constants and thresholds\n    GRAVITATIONAL_THRESHOLD = 1e-5\n    \n    class MemoryPacket:\n        def __init__(self, vector: List[float], metadata: Dict[str, Any]):\n            """\n            Memory packet to handle all gravity-based functions and metadata tracking.\n            """\n            self.vector = vector  # Semantic vector from SentenceTransformer\n            self.metadata = metadata or {}\n            \n            # Default metadata if not provided\n            self.metadata.setdefault("timestamp", time.time())\n            self.metadata.setdefault("recall_count", 0)\n            self.metadata.setdefault("memetic_similarity", 1.0)  # Could be refined over time\n            self.metadata.setdefault("semantic_relativity", 1.0)  # Set after query similarity\n            self.metadata.setdefault("gravitational_pull", self.calculate_gravitational_pull())\n            self.metadata.setdefault("spacetime_coordinate", self.calculate_spacetime_coordinate())\n    \n        def calculate_gravitational_pull(self) -> float:\n            """\n            Gravitational pull incorporates vector magnitude, recall count, memetic similarity, and semantic relativity.\n            """\n            vector_magnitude = math.sqrt(sum(x ** 2 for x in self.vector))\n            recall_count = self.metadata["recall_count"]\n            memetic_similarity = self.metadata["memetic_similarity"]\n            semantic_relativity = self.metadata["semantic_relativity"]\n            \n            # Calculate gravitational pull\n            gravitational_pull = vector_magnitude * (1 + math.log1p(recall_count)) * memetic_similarity * semantic_relativity\n            \n            self.metadata["gravitational_pull"] = gravitational_pull\n            return gravitational_pull\n    \n        def calculate_spacetime_coordinate(self) -> float:\n            """\n            Spacetime coordinate is a decaying function of gravitational pull and time.\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [gravrag_readme.md](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\gravrag_readme.md)\n\n**Type:** MD file\n\n\n    I am unable to access the uploaded file directly. However, I will provide an updated **README** based on the current state of the **GravRAG** system as you have specified, integrating the functionality we\'ve discussed.\n    \n    ---\n    \n    ## **GravRAG Memory System**\n    GravRAG (Gravitational Relativity Augmented Generation) is a memory system that dynamically calculates the relevance of memories based on **semantic relativity**, **memetic similarity**, and **recall frequency**. The system assigns **gravitational pull** and **spacetime coordinates** to memories to influence their significance over time.\n    \n    ---\n    \n    ### **Key Concepts**\n    \n    1. **Gravitational Pull**:\n       - **Gravitational pull** is calculated as a combination of the **semantic similarity** between memories, the **recall count**, and the **memetic similarity** of memories.\n       - Higher gravitational pull makes memories more likely to be recalled in future queries.\n       \n    2. **Spacetime Coordinates**:\n       - **Spacetime coordinates** measure the relevance of memories over time.\n       - Memories decay over time unless frequently recalled or semantically similar to new queries, ensuring that **important or frequently accessed memories** remain relevant.\n    \n    3. **Memetic Similarity**:\n       - Measures the **contextual relationship** between memories that are frequently co-recalled.\n       - As memories are accessed together, their **memetic similarity** increases, dynamically altering their relevance in the system.\n    \n    4. **Semantic Relativity**:\n       - Measured using **cosine similarity** between the semantic vectors of two pieces of content.\n       - **SentenceTransformer** is used to generate vectors that represent the semantic meaning of memories.\n    \n    5. **Recall Count**:\n       - Every time a memory is recalled, its **recall count** increases, boosting its **gravitational pull** and ensuring its relevance within the system grows.\n    \n    ---\n    \n    ### **System Features**\n    \n    - **Automatic Vectorization**:\n       - The system automatically generates **semantic vectors** for new memories using the **SentenceTransformer** model.\n       \n    - **Gravitational Pull Calculation**:\n       - Gravitational pull is calculated dynamically, incorporating **vector magnitude**, **recall frequency**, **memetic similarity**, and **semantic relativity**.\n    \n    - **Spacetime Coordinate Decay**:\n       - Memories\' spacetime coordinates decay over time unless frequently accessed, simulating a real-world decay of relevance for old, unused memories.\n    \n    - **Memory Pruning**:\n       - Low relevance memories are pruned from the system based on their **gravitational pull** falling below a set threshold, ensuring efficient memory management.\n    \n    ---\n    \n    ### **How It Works**\n    \n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [main.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\main.py)\n\n**Type:** PY file\n\n\n    from fastapi import FastAPI, HTTPException\n    from pydantic import BaseModel\n    from typing import List, Optional, Dict, Any\n    # from agentChef import DatasetKitchen, TemplateManager, FileHandler\n    # from gravrag import Knowledge\n    import uvicorn\n    import asyncio\n    import sys\n    import os\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    \n    # Import the AgentChef API\n    # We\'re importing the entire app and specific models that we might need\n    from api.agentChef_api import app as agentchef_app, LLMConfig, CustomAgentConfig, PrepareDatasetRequest, GenerateParaphrasesRequest, AugmentDataRequest\n    \n    # Import the GravRAG API\n    # Similarly, we import the GravRAG app and its MemoryInputModel\n    from api.gravrag_API import app as gravrag_app, MemoryInputModel\n    \n    from api.ai_api_providers import router as llm_router\n    app.include_router(llm_router, prefix="/llm", tags=["llm"])\n    \n    # Create separate FastAPI instances for AgentChef and GravRAG\n    # This allows us to run two separate APIs on different ports\n    agentchef_fastapi = FastAPI()\n    gravrag_fastapi = FastAPI()\n    \n    # Configuration for AgentChef\n    # This dictionary contains all the necessary settings for AgentChef\n    config = {\n        \'templates_dir\': \'./templates\',  # Directory for templates\n        \'input_dir\': \'./input\',          # Directory for input files\n        \'output_dir\': \'./output\',        # Directory for output files\n        \'ollama_config\': {\n            \'model\': \'phi3\',             # The model to use\n            \'api_base\': \'http://localhost:11434\'  # Ollama API endpoint\n        }\n    }\n    \n    # Initialize AgentChef components\n    # These objects will be used to interact with various parts of AgentChef\n    # Initialize AgentChef components\n    kitchen = agentchef_app.DatasetKitchen(config)\n    template_manager = agentchef_app.TemplateManager(config[\'templates_dir\'])\n    file_handler = agentchef_app.FileHandler(config[\'input_dir\'], config[\'output_dir\'])\n    chef_llm_manager = agentchef_app.LLMManager()\n    \n    grav_llm_manager = gravrag_app.LLMManager()\n    chef_llm_manager = agentchef_app.LLMManager()\n    \n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [neural_resources.json](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\neural_resources.json)\n\n**Type:** JSON file\n\n\n    {\n      "models": {\n        "llama3-groq-70b-8192-tool-use-preview": {\n          "provider": "groq",\n          "type": "text-generation",\n          "capabilities": [\n            "text-generation",\n            "conversation",\n            "code-generation",\n            "function-calling"\n          ],\n          "temperature": 1.0,\n          "max_tokens": 8192,\n          "context_window": 8192,\n          "description": "Large model optimized for tool use and function calling tasks."\n        },\n        "llama-3.1-70b-versatile": {\n          "provider": "groq",\n          "type": "text-generation",\n          "capabilities": [\n            "text-generation",\n            "conversation",\n            "code-generation",\n            "function-calling"\n          ],\n          "temperature": 1.0,\n          "max_tokens": 131072,\n          "context_window": 131072,\n          "description": "Very large model for advanced reasoning and diverse capabilities."\n        },\n        "mixtral-8x7b-32768": {\n          "provider": "groq",\n          "type": "text-generation",\n          "capabilities": [\n            "text-generation",\n            "conversation",\n            "code-generation"\n          ],\n          "temperature": 1.0,\n          "max_tokens": 32768,\n          "context_window": 32768,\n          "description": "Balanced model for general-purpose text generation and coding tasks."\n        }\n      }\n    }\n    \n\n\n\n### [neural_resources.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\neural_resources.py)\n\n**Type:** PY file\n\n\n    import os\n    import json\n    import logging\n    import requests\n    from typing import List, Dict, Any\n    from anthropic import Anthropic\n    import openai\n    from groq import Groq\n    from fastapi import FastAPI\n    #TODO POSSIBLE CIRCULAR IMPORT\n    from api.neural_resources_API import router as llm_router\n    \n    app = FastAPI()\n    \n    app.include_router(llm_router, prefix="/llm", tags=["llm"])\n    \n    # Load model data from JSON file\n    with open(\'neural_resources.json\', \'r\') as f:\n        model_data = json.load(f)\n    \n    class AIAsset:\n        def __init__(self, api_key: str):\n            self.api_key = api_key\n    \n        def create_message(self, model: str, message: str) -> Dict[str, Any]:\n            """Abstract method for creating a message"""\n            pass\n    \n        def get_output_tokens(self, response: Dict[str, Any]) -> int:\n            """Abstract method to get token usage"""\n            pass\n    \n        def execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n            """Abstract method for executing tool calls"""\n            pass\n    \n    class AnthropicLLM(AIAsset):\n        def __init__(self, api_key: str):\n            super().__init__(api_key)\n            self.client = Anthropic(api_key=api_key)\n    \n        def create_message(self, model: str, message: str) -> Dict[str, Any]:\n            response = self.client.messages.create(\n                model=model,\n                messages=[message],\n                max_tokens=model_data[\'models\'][model][\'max_tokens\']\n            )\n            return response.model_dump()\n    \n        def get_output_tokens(self, response: Dict[str, Any]) -> int:\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n### [requirements.txt](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\requirements.txt)\n\n**Type:** TXT file\n\n\n    fastapi\n    uvicorn\n    qdrant-client\n    sentence-transformers\n    sentence_transformers\n    cachetools\n    Flask==2.0.1\n    gunicorn==20.1.0\n    \n\n\n\n### [runthisIFYOUAREBORCH.py](file://D:\\CodingGit_StorageHDD\\Ollama_Custom_Mods\\nextjs\\cee_cee_mystery\\quantum-nexus\\groqy\\backend\\app\\runthisIFYOUAREBORCH.py)\n\n**Type:** PY file\n\n\n    import os\n    import re\n    import yaml\n    import json\n    import time\n    import logging\n    import threading\n    import tkinter as tk\n    from tkinter import filedialog, messagebox, simpledialog, ttk\n    from groq import Groq\n    from pygments import highlight\n    from pygments.lexers import get_lexer_for_filename\n    from pygments.formatters import HtmlFormatter\n    import html2text\n    \n    # Constants\n    CONFIG_FILE = \'config.json\'\n    LOG_FILE = \'project_analyzer.log\'\n    RATE_LIMIT = 2  # seconds between API calls\n    \n    # Set up logging\n    logging.basicConfig(\n        filename=LOG_FILE,\n        level=logging.DEBUG,\n        format=\'%(asctime)s - %(levelname)s - %(message)s\',\n    )\n    \n    class ProjectAnalyzerGUI:\n        def __init__(self, root):\n            self.root = root\n            self.root.title("Project Analyzer")\n            self.root.geometry("1200x800")\n            self.root.protocol("WM_DELETE_WINDOW", self.on_close)\n    \n            self.config = self.load_config()\n            self.api_key = self.config.get(\'api_key\', \'\')\n            self.client = None\n            self.project_dir = self.config.get(\'project_dir\', \'\')\n            self.selected_files = self.config.get(\'selected_files\', [])\n            self.system_prompt = self.config.get(\'system_prompt\', "You are an AI assistant analyzing a project structure.")\n            self.user_prompt = self.config.get(\'user_prompt\', "Analyze the following project structure:")\n            self.include_context = tk.BooleanVar(value=self.config.get(\'include_context\', True))\n            self.excluded_dirs = set(self.config.get(\'excluded_dirs\', [\'.git\', \'__pycache__\', \'node_modules\']))\n            self.excluded_extensions = set(self.config.get(\'excluded_extensions\', [\'.log\', \'.pyc\', \'.pyo\']))\n    \n            self.create_widgets()\n            self.populate_fields()\n    \n        def create_widgets(self):\n            # Main frame\n    \n    ... (truncated, showing first 50 lines)\n    \n\n\n\n'}], 'model': 'llama-3.1-70b-versatile', 'max_tokens': 4096}}
2024-09-25 13:59:55,250 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2024-09-25 13:59:55,250 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2024-09-25 13:59:55,407 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000225723A4C90>
2024-09-25 13:59:55,407 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022573A6D400> server_hostname='api.groq.com' timeout=5.0
2024-09-25 13:59:55,441 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000225003AF410>
2024-09-25 13:59:55,441 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-09-25 13:59:55,441 - DEBUG - send_request_headers.complete
2024-09-25 13:59:55,442 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-09-25 13:59:55,442 - DEBUG - send_request_body.complete
2024-09-25 13:59:55,442 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-09-25 14:00:00,471 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 25 Sep 2024 19:00:01 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'20000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'8786'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'33.642s'), (b'x-request-id', b'req_01j8n8vtptezj8vbdnybn2tp7d'), (b'via', b'1.1 google'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=tkjlHGi4n41Ry8HPVoqVngVusrwM6sY_Pgjv0W0fVMY-1727290801-1.0.1.1-a2IDY4m17qzdOYbE.AxNIX6P29DCZcKRT9DDUOAsqGX5KT6JiY50AXP6qwZnMa_ZthPH7xq8adZRClzyTXNlQA; path=/; expires=Wed, 25-Sep-24 19:30:01 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8c8d411738b560b2-ORD'), (b'Content-Encoding', b'gzip')])
2024-09-25 14:00:00,472 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2024-09-25 14:00:00,472 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-09-25 14:00:00,472 - DEBUG - receive_response_body.complete
2024-09-25 14:00:00,472 - DEBUG - response_closed.started
2024-09-25 14:00:00,472 - DEBUG - response_closed.complete
2024-09-25 14:00:00,472 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Wed, 25 Sep 2024 19:00:01 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '20000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '8786', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '33.642s', 'x-request-id': 'req_01j8n8vtptezj8vbdnybn2tp7d', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=tkjlHGi4n41Ry8HPVoqVngVusrwM6sY_Pgjv0W0fVMY-1727290801-1.0.1.1-a2IDY4m17qzdOYbE.AxNIX6P29DCZcKRT9DDUOAsqGX5KT6JiY50AXP6qwZnMa_ZthPH7xq8adZRClzyTXNlQA; path=/; expires=Wed, 25-Sep-24 19:30:01 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '8c8d411738b560b2-ORD', 'content-encoding': 'gzip'})
2024-09-25 14:00:00,475 - INFO - API call duration: 5.26 seconds
2024-09-25 14:00:00,475 - INFO - Rate limit delay complete
2024-09-25 14:00:00,475 - INFO - Analysis completed in 6.79 seconds.
2024-09-25 14:26:44,329 - INFO - Configuration saved successfully.
2024-09-25 14:26:44,431 - DEBUG - close.started
2024-09-25 14:26:44,431 - DEBUG - close.complete
