from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Dict, Any
from neural_resources import LLMManager  # Import LLMManager, not model_data

router = APIRouter()

# Initialize LLMManager
llm_manager = LLMManager()

# Payload structure for input message
class Message(BaseModel):
    content: str  # The message content to send to the language model

# Payload structure for updating API keys
class APIKeyUpdate(BaseModel):
    provider: str  # The provider name (e.g., "openai", "groq")
    api_key: str   # The new API key to set for the specified provider

# Dependency to get the LLMManager instance
def get_llm_manager():
    return llm_manager

@router.post("/route_query")
async def route_query(message: Message, manager: LLMManager = Depends(get_llm_manager)):
    """
    Route a query to the available models in the LLMManager.

    Payload (Input):
    {
        "content": "Your message here"
    }

    Payload (Output) - Successful Response:
    {
        "response": {
            // Response generated by the language model
        }
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        response = manager.route_query(message.content)  # Direct call to route_query method
        if "error" in response:
            raise HTTPException(status_code=500, detail=response["error"])
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/set_api_key")
async def set_api_key(api_key_update: APIKeyUpdate, manager: LLMManager = Depends(get_llm_manager)):
    """
    Update or override the API key for a specific provider.

    Payload (Input):
    {
        "provider": "provider_name",  # Name of the provider (e.g., "openai")
        "api_key": "new_api_key"      # The new API key for the provider
    }

    Payload (Output) - Successful Response:
    {
        "message": "API key updated for provider_name"
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        manager.set_api_key(api_key_update.provider, api_key_update.api_key)  # Use set_api_key method from LLMManager
        return {"message": f"API key updated for {api_key_update.provider}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/available_models")
async def get_available_models(manager: LLMManager = Depends(get_llm_manager)):
    """
    Get a list of all available models across all providers.

    Payload (Output):
    {
        "available_models": [
            "provider1", "provider2", "provider3"  # List of available models
        ]
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        return {"available_models": list(manager.llm_models.keys())}  # Use llm_models in LLMManager to get model providers
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/create_message/{provider}/{model}")
async def create_message(provider: str, model: str, message: Message, manager: LLMManager = Depends(get_llm_manager)):
    """
    Create a message using the specified provider and model.

    Payload (Input):
    {
        "content": "Your message here"  # The message content to send to the language model
    }

    Payload (Output) - Successful Response:
    {
        "response": {
            // Response generated by the specific provider's model
        }
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    if provider not in manager.llm_models:
        raise HTTPException(status_code=404, detail=f"Provider {provider} not found")

    try:
        response = manager.llm_models[provider].create_message(model, message.content)  # Correct call to create_message method
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/model_info/{provider}/{model}")
async def get_model_info(provider: str, model: str):
    """
    Retrieve model information from the model_data loaded in neural_resources.py.
    """
    # Example implementation based on what you want the info to return. If not supported, omit or correct.
    return {"detail": "This API is not supported."}
