from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Dict, Any
from neural_resources.neural_resources import LLMManager  # Correct import for LLMManager
import logging

router = APIRouter()

# Initialize LLMManager
llm_manager = LLMManager()

# Payload structure for input message
class Message(BaseModel):
    content: str  # The message content to send to the language model

# Payload structure for updating API keys
class APIKeyUpdate(BaseModel):
    provider: str  # The provider name (e.g., "openai", "groq")
    api_key: str   # The new API key to set for the specified provider

# Dependency to get the LLMManager instance
def get_llm_manager():
    return llm_manager

@router.post("/route_query")
async def route_query(message: Message, manager: LLMManager = Depends(get_llm_manager)):
    """
    Route a query to the available models in the LLMManager.

    Payload (Input):
    {
        "content": "Your message here"
    }

    Payload (Output) - Successful Response:
    {
        "response": {
            // Response generated by the language model
        }
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        response = manager.route_query(message.content)
        
        if "error" in response:
            # More specific error handling
            if "No available models" in response["error"]:
                raise HTTPException(status_code=503, detail="No models are currently available")
            else:
                raise HTTPException(status_code=500, detail=response["error"])

        return response
    except Exception as e:
        logging.error(f"Error in route_query: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal Server Error")

@router.post("/set_api_key")
async def set_api_key(api_key_update: APIKeyUpdate, manager: LLMManager = Depends(get_llm_manager)):
    """
    Update or override the API key for a specific provider.

    Payload (Input):
    {
        "provider": "provider_name",  # Name of the provider (e.g., "openai")
        "api_key": "new_api_key"      # The new API key for the provider
    }

    Payload (Output) - Successful Response:
    {
        "message": "API key updated for provider_name"
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        manager.set_api_key(api_key_update.provider, api_key_update.api_key)
        return {"message": f"API key updated for {api_key_update.provider}"}
    except Exception as e:
        logging.error(f"Error in set_api_key: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to update API key")

@router.get("/available_models")
async def get_available_models(manager: LLMManager = Depends(get_llm_manager)):
    """
    Get a list of all available models across all providers.

    Payload (Output):
    {
        "available_models": [
            "provider1", "provider2", "provider3"  # List of available models
        ]
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        return {"available_models": list(manager.llm_models.keys())}
    except Exception as e:
        logging.error(f"Error in get_available_models: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to retrieve available models")

@router.post("/create_message/{provider}/{model}")
async def create_message(provider: str, model: str, message: Message, manager: LLMManager = Depends(get_llm_manager)):
    """
    Create a message using the specified provider and model.

    Payload (Input):
    {
        "content": "Your message here"  # The message content to send to the language model
    }

    Payload (Output) - Successful Response:
    {
        "response": {
            // Response generated by the specific provider's model
        }
    }

    Error Response:
    {
        "detail": "Error message explaining what went wrong"
    }
    """
    try:
        if provider not in manager.llm_models:
            raise HTTPException(status_code=404, detail=f"Provider {provider} not found")
        response = manager.llm_models[provider].create_message(model, message.content)
        return response
    except Exception as e:
        logging.error(f"Error in create_message for {provider} using {model}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to create message")

@router.get("/model_info/{provider}/{model}")
async def get_model_info(provider: str, model: str):
    """
    Retrieve model information from the model_data loaded in neural_resources.py.
    """
    try:
        # Return mock model info based on what you'd like to display
        return {"provider": provider, "model": model, "info": "No detailed info available"}
    except Exception as e:
        logging.error(f"Error in get_model_info for {provider}/{model}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to retrieve model info")
